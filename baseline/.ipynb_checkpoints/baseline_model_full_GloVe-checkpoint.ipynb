{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, json, time, unittest, datetime, shutil\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from pandas_ml import ConfusionMatrix\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The data below is only on the 10k datasets for now. This will be updated to leverage the full datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All types of reviews - 10K dataset\n",
    "# reviews_df = pd.read_json(\"../dataset/review_10k.json\", lines=True)\n",
    "\n",
    "# Just restaurant reviews - 10K dataset\n",
    "reviews_df = pd.read_json(\"../dataset/restaurant_reviews_10k.json\", lines=True)\n",
    "\n",
    "# All types of reviews\n",
    "# reviews_df = pd.read_json(\"../../../final_project/full_dataset/review.json\", lines=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--6MefnULPED_I942VcFNA</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>This is one of my top 3 places to get BBQ pork...</td>\n",
       "      <td>2</td>\n",
       "      <td>FEg8v92qx3kK4Hu4TF28Fg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--6MefnULPED_I942VcFNA</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-31</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>This restaurant is famous for their BBQ dishes...</td>\n",
       "      <td>0</td>\n",
       "      <td>HPtjvIrhzAUkKsiVkeT4MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--6MefnULPED_I942VcFNA</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-10-23</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Roasted pork is one of my favorite things... A...</td>\n",
       "      <td>1</td>\n",
       "      <td>MpvqV7lQcl15rflTBEUhXA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--6MefnULPED_I942VcFNA</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-30</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>I walked by the restaurant more than 5 years a...</td>\n",
       "      <td>1</td>\n",
       "      <td>x-Gbs8sVid3yhJIoHD6Gfw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--6MefnULPED_I942VcFNA</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-02-07</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>I came here to order a roast duck over rice to...</td>\n",
       "      <td>0</td>\n",
       "      <td>7Dykd1HolQx8mKPYhYDYSg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool       date  funny  stars  \\\n",
       "0  --6MefnULPED_I942VcFNA     0 2017-08-17      0      4   \n",
       "1  --6MefnULPED_I942VcFNA     0 2017-05-31      0      3   \n",
       "2  --6MefnULPED_I942VcFNA     0 2016-10-23      0      2   \n",
       "3  --6MefnULPED_I942VcFNA     0 2017-07-30      0      2   \n",
       "4  --6MefnULPED_I942VcFNA     0 2017-02-07      1      2   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  This is one of my top 3 places to get BBQ pork...       2   \n",
       "1  This restaurant is famous for their BBQ dishes...       0   \n",
       "2  Roasted pork is one of my favorite things... A...       1   \n",
       "3  I walked by the restaurant more than 5 years a...       1   \n",
       "4  I came here to order a roast duck over rice to...       0   \n",
       "\n",
       "                  user_id  \n",
       "0  FEg8v92qx3kK4Hu4TF28Fg  \n",
       "1  HPtjvIrhzAUkKsiVkeT4MA  \n",
       "2  MpvqV7lQcl15rflTBEUhXA  \n",
       "3  x-Gbs8sVid3yhJIoHD6Gfw  \n",
       "4  7Dykd1HolQx8mKPYhYDYSg  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reviews_train_df = pd.read_json(\"../../full_dataset/restaurant_reviews_final_train.json\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-11-11</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>So when you go to a restaurant like this pleas...</td>\n",
       "      <td>0</td>\n",
       "      <td>---1lKK3aKOuomHnwAkAow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1JgaRBX0oiRsvEhHF3ZMjw</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-02-16</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Food was 30 mins late and the pizza guy thinks...</td>\n",
       "      <td>5</td>\n",
       "      <td>---1lKK3aKOuomHnwAkAow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2BbFeotL85cIaBjSq1SWiA</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-10-17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>When you say your a vegetarian don't recomend ...</td>\n",
       "      <td>2</td>\n",
       "      <td>---1lKK3aKOuomHnwAkAow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5aeR9KcboZmhDZlFscnYRA</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-16</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>So Fresh Mama let us host a Homeschool Board G...</td>\n",
       "      <td>1</td>\n",
       "      <td>---1lKK3aKOuomHnwAkAow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5cbsjFtrntUAeUx51FaFTg</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-11-05</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>So I was coming here once a month or so maybe ...</td>\n",
       "      <td>1</td>\n",
       "      <td>---1lKK3aKOuomHnwAkAow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool       date  funny  stars  \\\n",
       "0  --9e1ONYQuAa-CB_Rrw7Tw     0 2008-11-11      0      4   \n",
       "1  1JgaRBX0oiRsvEhHF3ZMjw     1 2011-02-16      7      1   \n",
       "2  2BbFeotL85cIaBjSq1SWiA     1 2010-10-17      1      1   \n",
       "3  5aeR9KcboZmhDZlFscnYRA     0 2013-01-16      0      5   \n",
       "4  5cbsjFtrntUAeUx51FaFTg     0 2010-11-05      0      1   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  So when you go to a restaurant like this pleas...       0   \n",
       "1  Food was 30 mins late and the pizza guy thinks...       5   \n",
       "2  When you say your a vegetarian don't recomend ...       2   \n",
       "3  So Fresh Mama let us host a Homeschool Board G...       1   \n",
       "4  So I was coming here once a month or so maybe ...       1   \n",
       "\n",
       "                  user_id  \n",
       "0  ---1lKK3aKOuomHnwAkAow  \n",
       "1  ---1lKK3aKOuomHnwAkAow  \n",
       "2  ---1lKK3aKOuomHnwAkAow  \n",
       "3  ---1lKK3aKOuomHnwAkAow  \n",
       "4  ---1lKK3aKOuomHnwAkAow  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reviews_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500464, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reviews_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reviews_test_df = pd.read_json(\"../../full_dataset/restaurant_reviews_final_test.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7dHYudt6OOIjiaxkSvv3lQ</td>\n",
       "      <td>197</td>\n",
       "      <td>2017-11-08</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "      <td>Auf unserer Rundreise haben wir häufig die Res...</td>\n",
       "      <td>203</td>\n",
       "      <td>--2vR0DIsmQ6WfcSzKWigw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9edPSkfXKsJmkZYIaOmA7Q</td>\n",
       "      <td>194</td>\n",
       "      <td>2017-11-06</td>\n",
       "      <td>176</td>\n",
       "      <td>4</td>\n",
       "      <td>Nachdem wir die Las Vegas North Premium Outlet...</td>\n",
       "      <td>199</td>\n",
       "      <td>--2vR0DIsmQ6WfcSzKWigw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El4FC8jcawUVgw_0EIcbaQ</td>\n",
       "      <td>122</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>109</td>\n",
       "      <td>4</td>\n",
       "      <td>Das MGM Grand ist mit seinen 6.853 Zimmern ein...</td>\n",
       "      <td>130</td>\n",
       "      <td>--2vR0DIsmQ6WfcSzKWigw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GHS1rVjO-RMcRB6WJLpCDQ</td>\n",
       "      <td>108</td>\n",
       "      <td>2015-09-16</td>\n",
       "      <td>94</td>\n",
       "      <td>3</td>\n",
       "      <td>In Las Vegas kann man zwischen zwei verschiede...</td>\n",
       "      <td>112</td>\n",
       "      <td>--2vR0DIsmQ6WfcSzKWigw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IhlKa2x5J4vr47hjDY8Jnw</td>\n",
       "      <td>11</td>\n",
       "      <td>2013-06-02</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>Bei einer meiner letzten Besuche in Stuttgart ...</td>\n",
       "      <td>15</td>\n",
       "      <td>--2vR0DIsmQ6WfcSzKWigw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool       date  funny  stars  \\\n",
       "0  7dHYudt6OOIjiaxkSvv3lQ   197 2017-11-08    179      5   \n",
       "1  9edPSkfXKsJmkZYIaOmA7Q   194 2017-11-06    176      4   \n",
       "2  El4FC8jcawUVgw_0EIcbaQ   122 2015-10-13    109      4   \n",
       "3  GHS1rVjO-RMcRB6WJLpCDQ   108 2015-09-16     94      3   \n",
       "4  IhlKa2x5J4vr47hjDY8Jnw    11 2013-06-02      7      5   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  Auf unserer Rundreise haben wir häufig die Res...     203   \n",
       "1  Nachdem wir die Las Vegas North Premium Outlet...     199   \n",
       "2  Das MGM Grand ist mit seinen 6.853 Zimmern ein...     130   \n",
       "3  In Las Vegas kann man zwischen zwei verschiede...     112   \n",
       "4  Bei einer meiner letzten Besuche in Stuttgart ...      15   \n",
       "\n",
       "                  user_id  \n",
       "0  --2vR0DIsmQ6WfcSzKWigw  \n",
       "1  --2vR0DIsmQ6WfcSzKWigw  \n",
       "2  --2vR0DIsmQ6WfcSzKWigw  \n",
       "3  --2vR0DIsmQ6WfcSzKWigw  \n",
       "4  --2vR0DIsmQ6WfcSzKWigw  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reviews_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(373155, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reviews_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_features = 10000\n",
    "\n",
    "# # text = reviews_df[\"text\"]\n",
    "# train_count_vect = final_reviews_train_df[\"text\"]\n",
    "# test_count_vect = final_reviews_test_df[\"text\"]\n",
    "\n",
    "# print(\"Fitting Count Vectorizer\")\n",
    "# # vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "# #                                 max_features=n_features,\n",
    "# #                                 stop_words='english')\n",
    "# # word_vector = vectorizer.fit_transform(text)\n",
    "\n",
    "# # No setting of hyper-parameters\n",
    "# vectorizer = CountVectorizer()\n",
    "# word_vector = vectorizer.fit_transform(text)\n",
    "\n",
    "# print(np.shape(word_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Print example text, stars, and embeddings\n",
    "\n",
    "# print(reviews_df[\"text\"][102])\n",
    "# print(reviews_df[\"stars\"][102])\n",
    "# print(word_vector[102])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1160it [00:00, 11589.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  2018-08-10 20:06:57.432425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:34, 11590.62it/s]\n",
      "  0%|          | 0/1500464 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500464/1500464 [44:09<00:00, 566.35it/s]\n",
      "100%|██████████| 373155/373155 [11:05<00:00, 560.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End time:  2018-08-10 21:02:51.808614\n",
      "Time taken:  0:55:54.376189\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle\n",
    "\n",
    "# Primary GloVe file\n",
    "gloveFile = \"../../glove/glove.6B.300d.txt\"\n",
    "\n",
    "# Large GloVe file\n",
    "# gloveFile = \"../../glove/glove.42B.300d.txt\"\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "print(\"Start time: \", start_time)\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(gloveFile)\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "\n",
    "# This function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "#     words = str(s).lower().decode('utf-8')\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "# create sentence vectors using the above function for training and validation set\n",
    "x_train_glove = [sent2vec(x) for x in tqdm(final_reviews_train_df[\"text\"])]\n",
    "x_test_glove = [sent2vec(x) for x in tqdm(final_reviews_test_df[\"text\"])]\n",
    "\n",
    "\n",
    "x_train_glove = np.array(x_train_glove)\n",
    "x_test_glove = np.array(x_test_glove)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"End time: \", end_time)\n",
    "\n",
    "time_taken = end_time - start_time\n",
    "print(\"Time taken: \", time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB Training and Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_user_reviews = reviews_df[\"text\"][0:6000]\n",
    "# x_dev_user_reviews = reviews_df[\"text\"][6001:8000]\n",
    "# x_test_user_reviews = reviews_df[\"text\"][8001:10000]\n",
    "\n",
    "\n",
    "# x_train_user_reviews = word_vector[0:6000]\n",
    "# # x_dev_user_reviews = word_vector[6001:8000]\n",
    "# x_train_user_reviews = word_vector[0:8000]\n",
    "# x_test_user_reviews = word_vector[8001:10000]\n",
    "\n",
    "# # print(\"x_train_user_reviews\", x_train_user_reviews)\n",
    "# # print(\"shape x_train_user_reviews\", np.shape(x_train_user_reviews))\n",
    "\n",
    "\n",
    "\n",
    "# # y_train_user_stars = reviews_df[\"stars\"][0:6000]\n",
    "# # y_dev_user_stars = reviews_df[\"stars\"][6001:8000]\n",
    "# y_train_user_stars = reviews_df[\"stars\"][0:8000]\n",
    "# y_test_user_stars = reviews_df[\"stars\"][8001:10000]\n",
    "\n",
    "y_train_user_stars = final_reviews_train_df[\"stars\"]\n",
    "y_test_user_stars = final_reviews_test_df[\"stars\"]\n",
    "\n",
    "\n",
    "# self._X_train, self._X_test, self._y_train, self._y_test = train_test_split(padded_seq, target, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  2018-08-10 21:02:51.882970\n",
      "End time:  2018-08-10 21:03:09.652436\n",
      "Time taken:  0:00:17.769466\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print(\"Start time: \", start_time)\n",
    "\n",
    "# nb = MultinomialNB()\n",
    "# Using Gaussian NB as it works witht the continuous data from the GloVe implementation\n",
    "nb = GaussianNB()\n",
    "\n",
    "# nb.fit(x_train_user_reviews, y_train_user_stars)\n",
    "nb.fit(x_train_glove, y_train_user_stars)\n",
    "\n",
    "# y_pred = nb.predict(x_test_user_reviews)\n",
    "y_pred = nb.predict(x_test_glove)\n",
    "\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"End time: \", end_time)\n",
    "\n",
    "time_taken = end_time - start_time\n",
    "print(\"Time taken: \", time_taken)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 35.14%\n",
      "F1 Score on test set: 0.36\n"
     ]
    }
   ],
   "source": [
    "# acc = accuracy_score(y_pred, y_test_user_stars)\n",
    "acc = accuracy_score(y_pred, y_test_user_stars)\n",
    "print(\"Accuracy on test set: {:.02%}\".format(acc))\n",
    "\n",
    "# log_loss_metric = log_loss(y_test_user_stars, y_pred)\n",
    "# print(\"Log-loss on test set: {:.02%}\".format(log_loss_metric))\n",
    "\n",
    "f1 = f1_score(y_test_user_stars, y_pred, average='weighted') \n",
    "# print(f1)\n",
    "print(\"F1 Score on test set: {:.02}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.27      0.58      0.37     29410\n",
      "          2       0.17      0.33      0.23     37665\n",
      "          3       0.26      0.23      0.24     66213\n",
      "          4       0.43      0.30      0.35    121614\n",
      "          5       0.53      0.42      0.47    118253\n",
      "\n",
      "avg / total       0.39      0.35      0.36    373155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['1', '2', '3', '4', '5']\n",
    "print(classification_report(y_test_user_stars, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(ConfusionMatrix(self._eval_actual, self._eval_predicted_classes))\n",
    "# nb.confusion_matrix.plot(normalized=True)\n",
    "\n",
    "confus_mat = ConfusionMatrix(y_test_user_stars, y_pred)\n",
    "confus_mat.plot(normalized=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr_coeff: 0.176601907082\n"
     ]
    }
   ],
   "source": [
    "corr_coeff = matthews_corrcoef(y_true=y_test_user_stars, y_pred=y_pred)\n",
    "print(\"corr_coeff:\", corr_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print example prediction\n",
    "\n",
    "print()\n",
    "print(y_pred[400])\n",
    "y_test_user_stars[400]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

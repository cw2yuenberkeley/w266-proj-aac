{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexanderherring/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding nltk.tree.Tree pretty-printing to use custom GraphViz.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, re, json, time, unittest, datetime, shutil\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.8\"))\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Helper libraries for this notebook\n",
    "# from common import utils, vocabulary\n",
    "from w266_common import utils, vocabulary, tf_embed_viz, treeviz\n",
    "from w266_common import patched_numpy_io\n",
    "import sst\n",
    "# import ngram_lm, ngram_lm_test\n",
    "\n",
    "# Monkey-patch NLTK with better Tree display that works on Cloud or other display-less server.\n",
    "print(\"Overriding nltk.tree.Tree pretty-printing to use custom GraphViz.\")\n",
    "treeviz.monkey_patch(nltk.tree.Tree, node_style_fn=sst.sst_node_style, format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/alexanderherring/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "Loaded 57,340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45,872 sentences (979,646 tokens)\n",
      "Test set: 11,468 sentences (181,546 tokens)\n",
      "Train set vocabulary: 30,000 words\n"
     ]
    }
   ],
   "source": [
    "assert(nltk.download('brown'))  # Make sure we have the data.\n",
    "corpus = nltk.corpus.brown\n",
    "V = 30000\n",
    "train_sents, test_sents = utils.get_train_test_sents(corpus, split=0.8, shuffle=False)\n",
    "# Build vocabulary only on the training set.\n",
    "vocab = vocabulary.Vocabulary((utils.canonicalize_word(w) for w in utils.flatten(train_sents)), size=V)\n",
    "print(\"Train set vocabulary: {:,} words\".format(vocab.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The data below is only on the 10k datasets for now. This will be updated to leverage the full datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_json(\"../dataset/user_10k.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_stars</th>\n",
       "      <th>compliment_cool</th>\n",
       "      <th>compliment_cute</th>\n",
       "      <th>compliment_funny</th>\n",
       "      <th>compliment_hot</th>\n",
       "      <th>compliment_list</th>\n",
       "      <th>compliment_more</th>\n",
       "      <th>compliment_note</th>\n",
       "      <th>compliment_photos</th>\n",
       "      <th>compliment_plain</th>\n",
       "      <th>...</th>\n",
       "      <th>cool</th>\n",
       "      <th>elite</th>\n",
       "      <th>fans</th>\n",
       "      <th>friends</th>\n",
       "      <th>funny</th>\n",
       "      <th>name</th>\n",
       "      <th>review_count</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>yelping_since</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[cvVMmlU1ouS3I5fhutaryQ, nj6UZ8tdGo8YJ9lUMTVWN...</td>\n",
       "      <td>0</td>\n",
       "      <td>Johnny</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>oMy_rEb0UBEmMlu-zcxnoQ</td>\n",
       "      <td>2014-11-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0njfJmB-7n84DlIgUByCNw, rFn3Xe3RqHxRSxWOU19Gp...</td>\n",
       "      <td>0</td>\n",
       "      <td>Chris</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>JJ-aSuM4pCFPdkfoZ34q0Q</td>\n",
       "      <td>2013-09-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>Tiffy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>uUzsFQn_6cXDh6rPNGbIFA</td>\n",
       "      <td>2017-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>mBneaEEH5EMyxaVyqS-72A</td>\n",
       "      <td>2015-03-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>Evelyn</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>W5mJGs-dcDWRGEhAzUYtoA</td>\n",
       "      <td>2016-09-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   average_stars  compliment_cool  compliment_cute  compliment_funny  \\\n",
       "0           4.67                0                0                 0   \n",
       "1           3.70                0                0                 0   \n",
       "2           2.00                0                0                 0   \n",
       "3           4.67                0                0                 0   \n",
       "4           4.67                0                0                 0   \n",
       "\n",
       "   compliment_hot  compliment_list  compliment_more  compliment_note  \\\n",
       "0               0                0                0                0   \n",
       "1               0                0                0                0   \n",
       "2               0                0                0                0   \n",
       "3               0                0                0                0   \n",
       "4               0                0                0                0   \n",
       "\n",
       "   compliment_photos  compliment_plain      ...        cool  elite  fans  \\\n",
       "0                  0                 1      ...           0     []     0   \n",
       "1                  0                 0      ...           0     []     0   \n",
       "2                  0                 0      ...           0     []     0   \n",
       "3                  0                 0      ...           0     []     0   \n",
       "4                  0                 0      ...           0     []     0   \n",
       "\n",
       "                                             friends  funny    name  \\\n",
       "0  [cvVMmlU1ouS3I5fhutaryQ, nj6UZ8tdGo8YJ9lUMTVWN...      0  Johnny   \n",
       "1  [0njfJmB-7n84DlIgUByCNw, rFn3Xe3RqHxRSxWOU19Gp...      0   Chris   \n",
       "2                                                 []      0   Tiffy   \n",
       "3                                                 []      0    Mark   \n",
       "4                                                 []      0  Evelyn   \n",
       "\n",
       "   review_count useful                 user_id  yelping_since  \n",
       "0             8      0  oMy_rEb0UBEmMlu-zcxnoQ     2014-11-03  \n",
       "1            10      0  JJ-aSuM4pCFPdkfoZ34q0Q     2013-09-24  \n",
       "2             1      0  uUzsFQn_6cXDh6rPNGbIFA     2017-03-02  \n",
       "3             6      0  mBneaEEH5EMyxaVyqS-72A     2015-03-13  \n",
       "4             3      0  W5mJGs-dcDWRGEhAzUYtoA     2016-09-08  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_json(\"../dataset/review_10k.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0W4lkclzZThpx3V65bVgig</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>0</td>\n",
       "      <td>v0i_UHJMo_hPBq9bxWvW4w</td>\n",
       "      <td>5</td>\n",
       "      <td>Love the staff, love the meat, love the place....</td>\n",
       "      <td>0</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AEx2SYEUJmTxVVB18LlCwA</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>0</td>\n",
       "      <td>vkVSCC7xljjrAI4UGfnKEQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Super simple place but amazing nonetheless. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VR6GpWIda3SfvPC-lg9H3w</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>0</td>\n",
       "      <td>n6QzIUObkYshz4dz2QRJTw</td>\n",
       "      <td>5</td>\n",
       "      <td>Small unassuming place that changes their menu...</td>\n",
       "      <td>0</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CKC0-MOWMqoeWf6s-szl8g</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>0</td>\n",
       "      <td>MV3CcKScW05u5LVfF6ok0g</td>\n",
       "      <td>5</td>\n",
       "      <td>Lester's is located in a beautiful neighborhoo...</td>\n",
       "      <td>0</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACFtxLv8pGrrxMm6EgjreA</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>0</td>\n",
       "      <td>IXvOzsEMYtiJI0CARmj77Q</td>\n",
       "      <td>4</td>\n",
       "      <td>Love coming here. Yes the place always needs t...</td>\n",
       "      <td>0</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool       date  funny               review_id  \\\n",
       "0  0W4lkclzZThpx3V65bVgig     0 2016-05-28      0  v0i_UHJMo_hPBq9bxWvW4w   \n",
       "1  AEx2SYEUJmTxVVB18LlCwA     0 2016-05-28      0  vkVSCC7xljjrAI4UGfnKEQ   \n",
       "2  VR6GpWIda3SfvPC-lg9H3w     0 2016-05-28      0  n6QzIUObkYshz4dz2QRJTw   \n",
       "3  CKC0-MOWMqoeWf6s-szl8g     0 2016-05-28      0  MV3CcKScW05u5LVfF6ok0g   \n",
       "4  ACFtxLv8pGrrxMm6EgjreA     0 2016-05-28      0  IXvOzsEMYtiJI0CARmj77Q   \n",
       "\n",
       "   stars                                               text  useful  \\\n",
       "0      5  Love the staff, love the meat, love the place....       0   \n",
       "1      5  Super simple place but amazing nonetheless. It...       0   \n",
       "2      5  Small unassuming place that changes their menu...       0   \n",
       "3      5  Lester's is located in a beautiful neighborhoo...       0   \n",
       "4      4  Love coming here. Yes the place always needs t...       0   \n",
       "\n",
       "                  user_id  \n",
       "0  bv2nCi5Qv5vroFiqKGopiw  \n",
       "1  bv2nCi5Qv5vroFiqKGopiw  \n",
       "2  bv2nCi5Qv5vroFiqKGopiw  \n",
       "3  bv2nCi5Qv5vroFiqKGopiw  \n",
       "4  bv2nCi5Qv5vroFiqKGopiw  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = pd.read_json(\"../dataset/business_10k.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>attributes</th>\n",
       "      <th>business_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>city</th>\n",
       "      <th>hours</th>\n",
       "      <th>is_open</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>stars</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4855 E Warner Rd, Ste B9</td>\n",
       "      <td>{'AcceptsInsurance': True, 'ByAppointmentOnly'...</td>\n",
       "      <td>FYWN1wneV18bWNgQjJ2GNg</td>\n",
       "      <td>[Dentists, General Dentistry, Health &amp; Medical...</td>\n",
       "      <td>Ahwatukee</td>\n",
       "      <td>{'Friday': '7:30-17:00', 'Tuesday': '7:30-17:0...</td>\n",
       "      <td>1</td>\n",
       "      <td>33.330690</td>\n",
       "      <td>-111.978599</td>\n",
       "      <td>Dental by Design</td>\n",
       "      <td></td>\n",
       "      <td>85044</td>\n",
       "      <td>22</td>\n",
       "      <td>4.0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3101 Washington Rd</td>\n",
       "      <td>{'BusinessParking': {'garage': False, 'street'...</td>\n",
       "      <td>He-G7vWjzVUysIKrfNbPUQ</td>\n",
       "      <td>[Hair Stylists, Hair Salons, Men's Hair Salons...</td>\n",
       "      <td>McMurray</td>\n",
       "      <td>{'Monday': '9:00-20:00', 'Tuesday': '9:00-20:0...</td>\n",
       "      <td>1</td>\n",
       "      <td>40.291685</td>\n",
       "      <td>-80.104900</td>\n",
       "      <td>Stephen Szabo Salon</td>\n",
       "      <td></td>\n",
       "      <td>15317</td>\n",
       "      <td>11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6025 N 27th Ave, Ste 1</td>\n",
       "      <td>{}</td>\n",
       "      <td>KQPW8lFf1y5BT2MxiSZ3QA</td>\n",
       "      <td>[Departments of Motor Vehicles, Public Service...</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "      <td>33.524903</td>\n",
       "      <td>-112.115310</td>\n",
       "      <td>Western Motor Vehicle</td>\n",
       "      <td></td>\n",
       "      <td>85017</td>\n",
       "      <td>18</td>\n",
       "      <td>1.5</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5000 Arizona Mills Cr, Ste 435</td>\n",
       "      <td>{'BusinessAcceptsCreditCards': True, 'Restaura...</td>\n",
       "      <td>8DShNS-LuFqpEWIp0HxijA</td>\n",
       "      <td>[Sporting Goods, Shopping]</td>\n",
       "      <td>Tempe</td>\n",
       "      <td>{'Monday': '10:00-21:00', 'Tuesday': '10:00-21...</td>\n",
       "      <td>0</td>\n",
       "      <td>33.383147</td>\n",
       "      <td>-111.964725</td>\n",
       "      <td>Sports Authority</td>\n",
       "      <td></td>\n",
       "      <td>85282</td>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>581 Howe Ave</td>\n",
       "      <td>{'Alcohol': 'full_bar', 'HasTV': True, 'NoiseL...</td>\n",
       "      <td>PfOCPjBrlQAnz__NXj9h_w</td>\n",
       "      <td>[American (New), Nightlife, Bars, Sandwiches, ...</td>\n",
       "      <td>Cuyahoga Falls</td>\n",
       "      <td>{'Monday': '11:00-1:00', 'Tuesday': '11:00-1:0...</td>\n",
       "      <td>1</td>\n",
       "      <td>41.119535</td>\n",
       "      <td>-81.475690</td>\n",
       "      <td>Brick House Tavern + Tap</td>\n",
       "      <td></td>\n",
       "      <td>44221</td>\n",
       "      <td>116</td>\n",
       "      <td>3.5</td>\n",
       "      <td>OH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          address  \\\n",
       "0        4855 E Warner Rd, Ste B9   \n",
       "1              3101 Washington Rd   \n",
       "2          6025 N 27th Ave, Ste 1   \n",
       "3  5000 Arizona Mills Cr, Ste 435   \n",
       "4                    581 Howe Ave   \n",
       "\n",
       "                                          attributes             business_id  \\\n",
       "0  {'AcceptsInsurance': True, 'ByAppointmentOnly'...  FYWN1wneV18bWNgQjJ2GNg   \n",
       "1  {'BusinessParking': {'garage': False, 'street'...  He-G7vWjzVUysIKrfNbPUQ   \n",
       "2                                                 {}  KQPW8lFf1y5BT2MxiSZ3QA   \n",
       "3  {'BusinessAcceptsCreditCards': True, 'Restaura...  8DShNS-LuFqpEWIp0HxijA   \n",
       "4  {'Alcohol': 'full_bar', 'HasTV': True, 'NoiseL...  PfOCPjBrlQAnz__NXj9h_w   \n",
       "\n",
       "                                          categories            city  \\\n",
       "0  [Dentists, General Dentistry, Health & Medical...       Ahwatukee   \n",
       "1  [Hair Stylists, Hair Salons, Men's Hair Salons...        McMurray   \n",
       "2  [Departments of Motor Vehicles, Public Service...         Phoenix   \n",
       "3                         [Sporting Goods, Shopping]           Tempe   \n",
       "4  [American (New), Nightlife, Bars, Sandwiches, ...  Cuyahoga Falls   \n",
       "\n",
       "                                               hours  is_open   latitude  \\\n",
       "0  {'Friday': '7:30-17:00', 'Tuesday': '7:30-17:0...        1  33.330690   \n",
       "1  {'Monday': '9:00-20:00', 'Tuesday': '9:00-20:0...        1  40.291685   \n",
       "2                                                 {}        1  33.524903   \n",
       "3  {'Monday': '10:00-21:00', 'Tuesday': '10:00-21...        0  33.383147   \n",
       "4  {'Monday': '11:00-1:00', 'Tuesday': '11:00-1:0...        1  41.119535   \n",
       "\n",
       "    longitude                      name neighborhood postal_code  \\\n",
       "0 -111.978599          Dental by Design                    85044   \n",
       "1  -80.104900       Stephen Szabo Salon                    15317   \n",
       "2 -112.115310     Western Motor Vehicle                    85017   \n",
       "3 -111.964725          Sports Authority                    85282   \n",
       "4  -81.475690  Brick House Tavern + Tap                    44221   \n",
       "\n",
       "   review_count  stars state  \n",
       "0            22    4.0    AZ  \n",
       "1            11    3.0    PA  \n",
       "2            18    1.5    AZ  \n",
       "3             9    3.0    AZ  \n",
       "4           116    3.5    OH  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df = pd.read_json(\"../dataset/checkin_10k.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7KPBkxAOEtb3QeIL9PEErg</td>\n",
       "      <td>{'Thursday': {'21:00': 4, '1:00': 1, '4:00': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kREVIrSBbtqBhIYkTccQUg</td>\n",
       "      <td>{'Monday': {'13:00': 1}, 'Thursday': {'20:00':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tJRDll5yqpZwehenzE2cSg</td>\n",
       "      <td>{'Monday': {'12:00': 1, '1:00': 1}, 'Friday': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r1p7RAMzCV_6NPF0dNoR3g</td>\n",
       "      <td>{'Thursday': {'23:00': 1}, 'Saturday': {'21:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mDdqgfrvROGAumcQdZ3HIg</td>\n",
       "      <td>{'Monday': {'12:00': 1, '21:00': 1}, 'Wednesda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                               time\n",
       "0  7KPBkxAOEtb3QeIL9PEErg  {'Thursday': {'21:00': 4, '1:00': 1, '4:00': 1...\n",
       "1  kREVIrSBbtqBhIYkTccQUg  {'Monday': {'13:00': 1}, 'Thursday': {'20:00':...\n",
       "2  tJRDll5yqpZwehenzE2cSg  {'Monday': {'12:00': 1, '1:00': 1}, 'Friday': ...\n",
       "3  r1p7RAMzCV_6NPF0dNoR3g  {'Thursday': {'23:00': 1}, 'Saturday': {'21:00...\n",
       "4  mDdqgfrvROGAumcQdZ3HIg  {'Monday': {'12:00': 1, '21:00': 1}, 'Wednesda..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_df = pd.read_json(\"../dataset/photos_10k.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>caption</th>\n",
       "      <th>label</th>\n",
       "      <th>photo_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OnAzbTDn79W6CFZIriqLrA</td>\n",
       "      <td></td>\n",
       "      <td>inside</td>\n",
       "      <td>soK1szeyan202jnsGhUDmA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OnAzbTDn79W6CFZIriqLrA</td>\n",
       "      <td></td>\n",
       "      <td>inside</td>\n",
       "      <td>dU7AyRB_fHOZkflodEyN5A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OnAzbTDn79W6CFZIriqLrA</td>\n",
       "      <td></td>\n",
       "      <td>outside</td>\n",
       "      <td>6T1qlbBdKkXA1cDNqMjg2g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OnAzbTDn79W6CFZIriqLrA</td>\n",
       "      <td>Bakery area</td>\n",
       "      <td>inside</td>\n",
       "      <td>lHhMNhCA7rAZmi-MMfF3ZA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XaeCGHZzsMwvFcHYq3q9sA</td>\n",
       "      <td></td>\n",
       "      <td>food</td>\n",
       "      <td>oHSCeyoK9oLIGaCZq-wRJw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id      caption    label                photo_id\n",
       "0  OnAzbTDn79W6CFZIriqLrA                inside  soK1szeyan202jnsGhUDmA\n",
       "1  OnAzbTDn79W6CFZIriqLrA                inside  dU7AyRB_fHOZkflodEyN5A\n",
       "2  OnAzbTDn79W6CFZIriqLrA               outside  6T1qlbBdKkXA1cDNqMjg2g\n",
       "3  OnAzbTDn79W6CFZIriqLrA  Bakery area   inside  lHhMNhCA7rAZmi-MMfF3ZA\n",
       "4  XaeCGHZzsMwvFcHYq3q9sA                  food  oHSCeyoK9oLIGaCZq-wRJw"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_df = pd.read_json(\"../dataset/tip_10k.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>likes</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tJRDll5yqpZwehenzE2cSg</td>\n",
       "      <td>2012-07-15</td>\n",
       "      <td>0</td>\n",
       "      <td>Get here early enough to have dinner.</td>\n",
       "      <td>zcTZk7OG8ovAmh_fenH21g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jH19V2I9fIslnNhDzPmdkA</td>\n",
       "      <td>2015-08-12</td>\n",
       "      <td>0</td>\n",
       "      <td>Great breakfast large portions and friendly wa...</td>\n",
       "      <td>ZcLKXikTHYOnYt5VYRO5sg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dAa0hB2yrnHzVmsCkN4YvQ</td>\n",
       "      <td>2014-06-20</td>\n",
       "      <td>0</td>\n",
       "      <td>Nice place. Great staff.  A fixture in the tow...</td>\n",
       "      <td>oaYhjqBbh18ZhU0bpyzSuw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dAa0hB2yrnHzVmsCkN4YvQ</td>\n",
       "      <td>2016-10-12</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy hour 5-7 Monday - Friday</td>\n",
       "      <td>ulQ8Nyj7jCUR8M83SUMoRQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ESzO3Av0b1_TzKOiqzbQYQ</td>\n",
       "      <td>2017-01-28</td>\n",
       "      <td>0</td>\n",
       "      <td>Parking is a premium, keep circling, you will ...</td>\n",
       "      <td>ulQ8Nyj7jCUR8M83SUMoRQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id       date  likes  \\\n",
       "0  tJRDll5yqpZwehenzE2cSg 2012-07-15      0   \n",
       "1  jH19V2I9fIslnNhDzPmdkA 2015-08-12      0   \n",
       "2  dAa0hB2yrnHzVmsCkN4YvQ 2014-06-20      0   \n",
       "3  dAa0hB2yrnHzVmsCkN4YvQ 2016-10-12      0   \n",
       "4  ESzO3Av0b1_TzKOiqzbQYQ 2017-01-28      0   \n",
       "\n",
       "                                                text                 user_id  \n",
       "0              Get here early enough to have dinner.  zcTZk7OG8ovAmh_fenH21g  \n",
       "1  Great breakfast large portions and friendly wa...  ZcLKXikTHYOnYt5VYRO5sg  \n",
       "2  Nice place. Great staff.  A fixture in the tow...  oaYhjqBbh18ZhU0bpyzSuw  \n",
       "3                     Happy hour 5-7 Monday - Friday  ulQ8Nyj7jCUR8M83SUMoRQ  \n",
       "4  Parking is a premium, keep circling, you will ...  ulQ8Nyj7jCUR8M83SUMoRQ  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tip_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efeb821c400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "users_df.review_count.hist()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SST from data/sst/trainDevTestTrees_PTB.zip\n",
      "Training set:     8,544 trees\n",
      "Development set:  1,101 trees\n",
      "Test set:         2,210 trees\n",
      "Building vocabulary - 16,474 words\n",
      "Processing to phrases...  Done!\n",
      "Splits: train / dev / test : 98,794 / 13,142 / 26,052\n"
     ]
    }
   ],
   "source": [
    "import sst\n",
    "ds = sst.SSTDataset(V=20000).process(label_scheme=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yelp_processed = sst.SSTDataset(V=20000).process(label_scheme=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.datascience.com/resources/notebooks/word-embeddings-in-python\n",
    "\n",
    "# imports\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "# from gensim import corpora\n",
    "# from gensim.models import Phrases\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# from ds_voc.text_processing import TextProcessing\n",
    "# # sample for speed\n",
    "# raw_df = raw_df.sample(frac=0.1,  replace=False)\n",
    "# print raw_df.shape\n",
    "# # grab review text\n",
    "# raw = list(raw_df['Text'])\n",
    "# print len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.nltk.org/book/ch03.html\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/alexanderherring/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-07-20 01:46:10--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2018-07-20 01:46:10--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: â€˜glove.6B.zip.2â€™\n",
      "\n",
      "glove.6B.zip.2      100%[===================>] 822.24M  97.9MB/s    in 8.8s    \n",
      "\n",
      "2018-07-20 01:46:20 (93.8 MB/s) - â€˜glove.6B.zip.2â€™ saved [862182613/862182613]\n",
      "\n",
      "/bin/sh: 1: unzip: not found\n"
     ]
    }
   ],
   "source": [
    "#http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'itervalues'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-2a943907060e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m etree_w2v = Pipeline([\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"word2vec vectorizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMeanEmbeddingVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n\u001b[1;32m      8\u001b[0m etree_w2v_tfidf = Pipeline([\n",
      "\u001b[0;32m<ipython-input-51-073d10bd9224>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, word2vec)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# if a text is empty we should return a vector of zeros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# with the same dimensionality as all the other vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitervalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3614\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3616\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'itervalues'"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etree_w2v = Pipeline([\n",
    "#     (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(reviews_df[\"text\"])),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_w2v_tfidf = Pipeline([\n",
    "#     (\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)),\n",
    "    (\"word2vec vectorizer\", TfidfEmbeddingVectorizer(reviews_df[\"text\"])),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "# text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "text = reviews_df[\"text\"]\n",
    "\n",
    "\n",
    "\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "# print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "word_vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "# print(vector.shape)\n",
    "# print(type(vector))\n",
    "# print(vector.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-36cfb6f5a346>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \"\"\"\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m   1315\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# tokens = word_tokenize(reviews_df[\"text\"])\n",
    "# type(tokens)\n",
    "# len(tokens)\n",
    "# tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextProcessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a6c6a41367f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# word2vec expexts a list of list: each document is a list of tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextProcessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_and_stem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcleaned\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TextProcessing' is not defined"
     ]
    }
   ],
   "source": [
    "# word2vec expexts a list of list: each document is a list of tokens\n",
    "te = TextProcessing()\n",
    "\n",
    "cleaned = [te.default_clean(d) for d in raw]\n",
    "sentences = [te.stop_and_stem(c) for c in cleaned]\n",
    "#Now we are ready to fit a model. Here we are using the gensim implementation:\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=sentences, # tokenized senteces, list of list of strings\n",
    "                 size=300,  # size of embedding vectors\n",
    "                 workers=4, # how many threads?\n",
    "                 min_count=20, # minimum frequency per token, filtering rare words\n",
    "                 sample=0.05, # weight of downsampling common words\n",
    "                 sg = 0, # should we use skip-gram? if 0, then cbow\n",
    "                 iter=5,\n",
    "                 hs = 0\n",
    "        )\n",
    "\n",
    "X = model[model.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB Training and Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_user_reviews   (0, 1609)\t1\n",
      "  (0, 2014)\t1\n",
      "  (0, 2113)\t1\n",
      "  (0, 4656)\t1\n",
      "  (0, 7537)\t1\n",
      "  (0, 8418)\t1\n",
      "  (0, 8814)\t1\n",
      "  (0, 10629)\t1\n",
      "  (0, 11323)\t1\n",
      "  (0, 11531)\t1\n",
      "  (0, 11584)\t1\n",
      "  (0, 12076)\t1\n",
      "  (0, 12953)\t1\n",
      "  (0, 12993)\t1\n",
      "  (0, 13782)\t1\n",
      "  (0, 13812)\t1\n",
      "  (0, 13829)\t1\n",
      "  (0, 14874)\t1\n",
      "  (0, 15313)\t1\n",
      "  (0, 16072)\t1\n",
      "  (0, 16374)\t1\n",
      "  (0, 16558)\t1\n",
      "  (0, 16651)\t3\n",
      "  (0, 16747)\t1\n",
      "  (0, 17386)\t1\n",
      "  :\t:\n",
      "  (5999, 21103)\t1\n",
      "  (5999, 21510)\t1\n",
      "  (5999, 22408)\t1\n",
      "  (5999, 22706)\t1\n",
      "  (5999, 23684)\t1\n",
      "  (5999, 24363)\t2\n",
      "  (5999, 24365)\t1\n",
      "  (5999, 26151)\t1\n",
      "  (5999, 27006)\t1\n",
      "  (5999, 28303)\t1\n",
      "  (5999, 28308)\t6\n",
      "  (5999, 28323)\t1\n",
      "  (5999, 28333)\t1\n",
      "  (5999, 28354)\t1\n",
      "  (5999, 28378)\t2\n",
      "  (5999, 28646)\t2\n",
      "  (5999, 29604)\t1\n",
      "  (5999, 29962)\t1\n",
      "  (5999, 30117)\t1\n",
      "  (5999, 30357)\t2\n",
      "  (5999, 30591)\t2\n",
      "  (5999, 31161)\t2\n",
      "  (5999, 31311)\t1\n",
      "  (5999, 31548)\t3\n",
      "  (5999, 31555)\t1\n",
      "shape x_train_user_reviews (6000, 31803)\n"
     ]
    }
   ],
   "source": [
    "# x_train_user_reviews = reviews_df[\"text\"][0:6000]\n",
    "# x_dev_user_reviews = reviews_df[\"text\"][6001:8000]\n",
    "# x_test_user_reviews = reviews_df[\"text\"][8001:10000]\n",
    "\n",
    "\n",
    "x_train_user_reviews = word_vector[0:6000]\n",
    "x_dev_user_reviews = word_vector[6001:8000]\n",
    "x_test_user_reviews = word_vector[8001:10000]\n",
    "\n",
    "# print(\"x_train_user_reviews\", x_train_user_reviews)\n",
    "# print(\"shape x_train_user_reviews\", np.shape(x_train_user_reviews))\n",
    "\n",
    "\n",
    "\n",
    "y_train_user_stars = reviews_df[\"stars\"][0:6000]\n",
    "y_dev_user_stars = reviews_df[\"stars\"][6001:8000]\n",
    "y_test_user_stars = reviews_df[\"stars\"][8001:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test_user_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 'csr' stands for \"Compressed Sparse Row\", which is one format\n",
    "# # for representing sparse matricies.\n",
    "# x_train_user_reviews, y_train_user_stars = reviews_df.as_sparse_bow(\"train\", root_only=True)\n",
    "# x_dev_user_reviews,  y_dev_user_stars  = reviews_df.as_sparse_bow(\"dev\", root_only=True)\n",
    "# x_test_user_reviews,  y_test_user_stars  = reviews_df.as_sparse_bow(\"test\", root_only=True)\n",
    "# print(\"Training set: x = {:s} sparse, y = {:s}\".format(str(x_train_user_reviews.shape), \n",
    "#                                                 str(y_train_user_stars.shape)))\n",
    "# print(\"Dev set: x = {:s} sparse, y = {:s}\".format(str(x_dev_user_reviews.shape), \n",
    "#                                                 str(y_dev_user_stars.shape)))\n",
    "# print(\"Test set:     x = {:s} sparse, y = {:s}\".format(str(x_test_user_reviews.shape), \n",
    "#                                                 str(y_test_user_stars.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-0dbdb528a8ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_user_reviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_user_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_user_stars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# print(\"Accuracy on test set: {:.02%}\".format(acc))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m     71\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# Invalid inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     if y.ndim > 2 or (y.dtype == object and len(y) and\n\u001b[0m\u001b[1;32m    272\u001b[0m                       not isinstance(y.flat[0], string_types)):\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m'unknown'\u001b[0m  \u001b[0;31m# [[[1, 2]]] or [obj_1] and not [\"label_1\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: len() of unsized object"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(x_train_user_reviews, y_train_user_stars)\n",
    "\n",
    "y_pred = nb.predict(x_test_user_reviews)\n",
    "\n",
    "acc = accuracy_score(x_test_user_reviews, y_test_user_stars)\n",
    "# print(\"Accuracy on test set: {:.02%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

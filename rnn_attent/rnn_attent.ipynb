{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, json, time, unittest, datetime, shutil\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The data below is only on the 10k datasets for now. This will be updated to leverage the full datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_json(\"../dataset/user_10k.json\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_stars</th>\n",
       "      <th>compliment_cool</th>\n",
       "      <th>compliment_cute</th>\n",
       "      <th>compliment_funny</th>\n",
       "      <th>compliment_hot</th>\n",
       "      <th>compliment_list</th>\n",
       "      <th>compliment_more</th>\n",
       "      <th>compliment_note</th>\n",
       "      <th>compliment_photos</th>\n",
       "      <th>compliment_plain</th>\n",
       "      <th>...</th>\n",
       "      <th>cool</th>\n",
       "      <th>elite</th>\n",
       "      <th>fans</th>\n",
       "      <th>friends</th>\n",
       "      <th>funny</th>\n",
       "      <th>name</th>\n",
       "      <th>review_count</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>yelping_since</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[cvVMmlU1ouS3I5fhutaryQ, nj6UZ8tdGo8YJ9lUMTVWN...</td>\n",
       "      <td>0</td>\n",
       "      <td>Johnny</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>oMy_rEb0UBEmMlu-zcxnoQ</td>\n",
       "      <td>2014-11-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0njfJmB-7n84DlIgUByCNw, rFn3Xe3RqHxRSxWOU19Gp...</td>\n",
       "      <td>0</td>\n",
       "      <td>Chris</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>JJ-aSuM4pCFPdkfoZ34q0Q</td>\n",
       "      <td>2013-09-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>Tiffy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>uUzsFQn_6cXDh6rPNGbIFA</td>\n",
       "      <td>2017-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>mBneaEEH5EMyxaVyqS-72A</td>\n",
       "      <td>2015-03-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>Evelyn</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>W5mJGs-dcDWRGEhAzUYtoA</td>\n",
       "      <td>2016-09-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   average_stars  compliment_cool  compliment_cute  compliment_funny  \\\n",
       "0           4.67                0                0                 0   \n",
       "1           3.70                0                0                 0   \n",
       "2           2.00                0                0                 0   \n",
       "3           4.67                0                0                 0   \n",
       "4           4.67                0                0                 0   \n",
       "\n",
       "   compliment_hot  compliment_list  compliment_more  compliment_note  \\\n",
       "0               0                0                0                0   \n",
       "1               0                0                0                0   \n",
       "2               0                0                0                0   \n",
       "3               0                0                0                0   \n",
       "4               0                0                0                0   \n",
       "\n",
       "   compliment_photos  compliment_plain      ...        cool  elite  fans  \\\n",
       "0                  0                 1      ...           0     []     0   \n",
       "1                  0                 0      ...           0     []     0   \n",
       "2                  0                 0      ...           0     []     0   \n",
       "3                  0                 0      ...           0     []     0   \n",
       "4                  0                 0      ...           0     []     0   \n",
       "\n",
       "                                             friends  funny    name  \\\n",
       "0  [cvVMmlU1ouS3I5fhutaryQ, nj6UZ8tdGo8YJ9lUMTVWN...      0  Johnny   \n",
       "1  [0njfJmB-7n84DlIgUByCNw, rFn3Xe3RqHxRSxWOU19Gp...      0   Chris   \n",
       "2                                                 []      0   Tiffy   \n",
       "3                                                 []      0    Mark   \n",
       "4                                                 []      0  Evelyn   \n",
       "\n",
       "   review_count useful                 user_id  yelping_since  \n",
       "0             8      0  oMy_rEb0UBEmMlu-zcxnoQ     2014-11-03  \n",
       "1            10      0  JJ-aSuM4pCFPdkfoZ34q0Q     2013-09-24  \n",
       "2             1      0  uUzsFQn_6cXDh6rPNGbIFA     2017-03-02  \n",
       "3             6      0  mBneaEEH5EMyxaVyqS-72A     2015-03-13  \n",
       "4             3      0  W5mJGs-dcDWRGEhAzUYtoA     2016-09-08  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All types of reviews - 10K dataset\n",
    "# reviews_df = pd.read_json(\"../dataset/review_10k.json\", lines=True)\n",
    "\n",
    "# Just restaurant reviews - 10K dataset\n",
    "reviews_df = pd.read_json(\"../dataset/restaurant_reviews_10k.json\", lines=True)\n",
    "\n",
    "# All types of reviews\n",
    "# reviews_df = pd.read_json(\"../../../final_project/full_dataset/review.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--6MefnULPED_I942VcFNA</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>This is one of my top 3 places to get BBQ pork...</td>\n",
       "      <td>2</td>\n",
       "      <td>FEg8v92qx3kK4Hu4TF28Fg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--6MefnULPED_I942VcFNA</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-31</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>This restaurant is famous for their BBQ dishes...</td>\n",
       "      <td>0</td>\n",
       "      <td>HPtjvIrhzAUkKsiVkeT4MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--6MefnULPED_I942VcFNA</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-10-23</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Roasted pork is one of my favorite things... A...</td>\n",
       "      <td>1</td>\n",
       "      <td>MpvqV7lQcl15rflTBEUhXA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--6MefnULPED_I942VcFNA</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-30</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>I walked by the restaurant more than 5 years a...</td>\n",
       "      <td>1</td>\n",
       "      <td>x-Gbs8sVid3yhJIoHD6Gfw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--6MefnULPED_I942VcFNA</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-02-07</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>I came here to order a roast duck over rice to...</td>\n",
       "      <td>0</td>\n",
       "      <td>7Dykd1HolQx8mKPYhYDYSg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool       date  funny  stars  \\\n",
       "0  --6MefnULPED_I942VcFNA     0 2017-08-17      0      4   \n",
       "1  --6MefnULPED_I942VcFNA     0 2017-05-31      0      3   \n",
       "2  --6MefnULPED_I942VcFNA     0 2016-10-23      0      2   \n",
       "3  --6MefnULPED_I942VcFNA     0 2017-07-30      0      2   \n",
       "4  --6MefnULPED_I942VcFNA     0 2017-02-07      1      2   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  This is one of my top 3 places to get BBQ pork...       2   \n",
       "1  This restaurant is famous for their BBQ dishes...       0   \n",
       "2  Roasted pork is one of my favorite things... A...       1   \n",
       "3  I walked by the restaurant more than 5 years a...       1   \n",
       "4  I came here to order a roast duck over rice to...       0   \n",
       "\n",
       "                  user_id  \n",
       "0  FEg8v92qx3kK4Hu4TF28Fg  \n",
       "1  HPtjvIrhzAUkKsiVkeT4MA  \n",
       "2  MpvqV7lQcl15rflTBEUhXA  \n",
       "3  x-Gbs8sVid3yhJIoHD6Gfw  \n",
       "4  7Dykd1HolQx8mKPYhYDYSg  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = pd.read_json(\"../dataset/business_10k.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>attributes</th>\n",
       "      <th>business_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>city</th>\n",
       "      <th>hours</th>\n",
       "      <th>is_open</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>stars</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4855 E Warner Rd, Ste B9</td>\n",
       "      <td>{'AcceptsInsurance': True, 'ByAppointmentOnly'...</td>\n",
       "      <td>FYWN1wneV18bWNgQjJ2GNg</td>\n",
       "      <td>[Dentists, General Dentistry, Health &amp; Medical...</td>\n",
       "      <td>Ahwatukee</td>\n",
       "      <td>{'Friday': '7:30-17:00', 'Tuesday': '7:30-17:0...</td>\n",
       "      <td>1</td>\n",
       "      <td>33.330690</td>\n",
       "      <td>-111.978599</td>\n",
       "      <td>Dental by Design</td>\n",
       "      <td></td>\n",
       "      <td>85044</td>\n",
       "      <td>22</td>\n",
       "      <td>4.0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3101 Washington Rd</td>\n",
       "      <td>{'BusinessParking': {'garage': False, 'street'...</td>\n",
       "      <td>He-G7vWjzVUysIKrfNbPUQ</td>\n",
       "      <td>[Hair Stylists, Hair Salons, Men's Hair Salons...</td>\n",
       "      <td>McMurray</td>\n",
       "      <td>{'Monday': '9:00-20:00', 'Tuesday': '9:00-20:0...</td>\n",
       "      <td>1</td>\n",
       "      <td>40.291685</td>\n",
       "      <td>-80.104900</td>\n",
       "      <td>Stephen Szabo Salon</td>\n",
       "      <td></td>\n",
       "      <td>15317</td>\n",
       "      <td>11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6025 N 27th Ave, Ste 1</td>\n",
       "      <td>{}</td>\n",
       "      <td>KQPW8lFf1y5BT2MxiSZ3QA</td>\n",
       "      <td>[Departments of Motor Vehicles, Public Service...</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "      <td>33.524903</td>\n",
       "      <td>-112.115310</td>\n",
       "      <td>Western Motor Vehicle</td>\n",
       "      <td></td>\n",
       "      <td>85017</td>\n",
       "      <td>18</td>\n",
       "      <td>1.5</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5000 Arizona Mills Cr, Ste 435</td>\n",
       "      <td>{'BusinessAcceptsCreditCards': True, 'Restaura...</td>\n",
       "      <td>8DShNS-LuFqpEWIp0HxijA</td>\n",
       "      <td>[Sporting Goods, Shopping]</td>\n",
       "      <td>Tempe</td>\n",
       "      <td>{'Monday': '10:00-21:00', 'Tuesday': '10:00-21...</td>\n",
       "      <td>0</td>\n",
       "      <td>33.383147</td>\n",
       "      <td>-111.964725</td>\n",
       "      <td>Sports Authority</td>\n",
       "      <td></td>\n",
       "      <td>85282</td>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>581 Howe Ave</td>\n",
       "      <td>{'Alcohol': 'full_bar', 'HasTV': True, 'NoiseL...</td>\n",
       "      <td>PfOCPjBrlQAnz__NXj9h_w</td>\n",
       "      <td>[American (New), Nightlife, Bars, Sandwiches, ...</td>\n",
       "      <td>Cuyahoga Falls</td>\n",
       "      <td>{'Monday': '11:00-1:00', 'Tuesday': '11:00-1:0...</td>\n",
       "      <td>1</td>\n",
       "      <td>41.119535</td>\n",
       "      <td>-81.475690</td>\n",
       "      <td>Brick House Tavern + Tap</td>\n",
       "      <td></td>\n",
       "      <td>44221</td>\n",
       "      <td>116</td>\n",
       "      <td>3.5</td>\n",
       "      <td>OH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          address  \\\n",
       "0        4855 E Warner Rd, Ste B9   \n",
       "1              3101 Washington Rd   \n",
       "2          6025 N 27th Ave, Ste 1   \n",
       "3  5000 Arizona Mills Cr, Ste 435   \n",
       "4                    581 Howe Ave   \n",
       "\n",
       "                                          attributes             business_id  \\\n",
       "0  {'AcceptsInsurance': True, 'ByAppointmentOnly'...  FYWN1wneV18bWNgQjJ2GNg   \n",
       "1  {'BusinessParking': {'garage': False, 'street'...  He-G7vWjzVUysIKrfNbPUQ   \n",
       "2                                                 {}  KQPW8lFf1y5BT2MxiSZ3QA   \n",
       "3  {'BusinessAcceptsCreditCards': True, 'Restaura...  8DShNS-LuFqpEWIp0HxijA   \n",
       "4  {'Alcohol': 'full_bar', 'HasTV': True, 'NoiseL...  PfOCPjBrlQAnz__NXj9h_w   \n",
       "\n",
       "                                          categories            city  \\\n",
       "0  [Dentists, General Dentistry, Health & Medical...       Ahwatukee   \n",
       "1  [Hair Stylists, Hair Salons, Men's Hair Salons...        McMurray   \n",
       "2  [Departments of Motor Vehicles, Public Service...         Phoenix   \n",
       "3                         [Sporting Goods, Shopping]           Tempe   \n",
       "4  [American (New), Nightlife, Bars, Sandwiches, ...  Cuyahoga Falls   \n",
       "\n",
       "                                               hours  is_open   latitude  \\\n",
       "0  {'Friday': '7:30-17:00', 'Tuesday': '7:30-17:0...        1  33.330690   \n",
       "1  {'Monday': '9:00-20:00', 'Tuesday': '9:00-20:0...        1  40.291685   \n",
       "2                                                 {}        1  33.524903   \n",
       "3  {'Monday': '10:00-21:00', 'Tuesday': '10:00-21...        0  33.383147   \n",
       "4  {'Monday': '11:00-1:00', 'Tuesday': '11:00-1:0...        1  41.119535   \n",
       "\n",
       "    longitude                      name neighborhood postal_code  \\\n",
       "0 -111.978599          Dental by Design                    85044   \n",
       "1  -80.104900       Stephen Szabo Salon                    15317   \n",
       "2 -112.115310     Western Motor Vehicle                    85017   \n",
       "3 -111.964725          Sports Authority                    85282   \n",
       "4  -81.475690  Brick House Tavern + Tap                    44221   \n",
       "\n",
       "   review_count  stars state  \n",
       "0            22    4.0    AZ  \n",
       "1            11    3.0    PA  \n",
       "2            18    1.5    AZ  \n",
       "3             9    3.0    AZ  \n",
       "4           116    3.5    OH  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df = pd.read_json(\"../dataset/checkin_10k.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7KPBkxAOEtb3QeIL9PEErg</td>\n",
       "      <td>{'Thursday': {'21:00': 4, '1:00': 1, '4:00': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kREVIrSBbtqBhIYkTccQUg</td>\n",
       "      <td>{'Monday': {'13:00': 1}, 'Thursday': {'20:00':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tJRDll5yqpZwehenzE2cSg</td>\n",
       "      <td>{'Monday': {'12:00': 1, '1:00': 1}, 'Friday': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r1p7RAMzCV_6NPF0dNoR3g</td>\n",
       "      <td>{'Thursday': {'23:00': 1}, 'Saturday': {'21:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mDdqgfrvROGAumcQdZ3HIg</td>\n",
       "      <td>{'Monday': {'12:00': 1, '21:00': 1}, 'Wednesda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                               time\n",
       "0  7KPBkxAOEtb3QeIL9PEErg  {'Thursday': {'21:00': 4, '1:00': 1, '4:00': 1...\n",
       "1  kREVIrSBbtqBhIYkTccQUg  {'Monday': {'13:00': 1}, 'Thursday': {'20:00':...\n",
       "2  tJRDll5yqpZwehenzE2cSg  {'Monday': {'12:00': 1, '1:00': 1}, 'Friday': ...\n",
       "3  r1p7RAMzCV_6NPF0dNoR3g  {'Thursday': {'23:00': 1}, 'Saturday': {'21:00...\n",
       "4  mDdqgfrvROGAumcQdZ3HIg  {'Monday': {'12:00': 1, '21:00': 1}, 'Wednesda..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_df = pd.read_json(\"../dataset/photos_10k.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>caption</th>\n",
       "      <th>label</th>\n",
       "      <th>photo_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OnAzbTDn79W6CFZIriqLrA</td>\n",
       "      <td></td>\n",
       "      <td>inside</td>\n",
       "      <td>soK1szeyan202jnsGhUDmA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OnAzbTDn79W6CFZIriqLrA</td>\n",
       "      <td></td>\n",
       "      <td>inside</td>\n",
       "      <td>dU7AyRB_fHOZkflodEyN5A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OnAzbTDn79W6CFZIriqLrA</td>\n",
       "      <td></td>\n",
       "      <td>outside</td>\n",
       "      <td>6T1qlbBdKkXA1cDNqMjg2g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OnAzbTDn79W6CFZIriqLrA</td>\n",
       "      <td>Bakery area</td>\n",
       "      <td>inside</td>\n",
       "      <td>lHhMNhCA7rAZmi-MMfF3ZA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XaeCGHZzsMwvFcHYq3q9sA</td>\n",
       "      <td></td>\n",
       "      <td>food</td>\n",
       "      <td>oHSCeyoK9oLIGaCZq-wRJw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id      caption    label                photo_id\n",
       "0  OnAzbTDn79W6CFZIriqLrA                inside  soK1szeyan202jnsGhUDmA\n",
       "1  OnAzbTDn79W6CFZIriqLrA                inside  dU7AyRB_fHOZkflodEyN5A\n",
       "2  OnAzbTDn79W6CFZIriqLrA               outside  6T1qlbBdKkXA1cDNqMjg2g\n",
       "3  OnAzbTDn79W6CFZIriqLrA  Bakery area   inside  lHhMNhCA7rAZmi-MMfF3ZA\n",
       "4  XaeCGHZzsMwvFcHYq3q9sA                  food  oHSCeyoK9oLIGaCZq-wRJw"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_df = pd.read_json(\"../dataset/tip_10k.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>likes</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tJRDll5yqpZwehenzE2cSg</td>\n",
       "      <td>2012-07-15</td>\n",
       "      <td>0</td>\n",
       "      <td>Get here early enough to have dinner.</td>\n",
       "      <td>zcTZk7OG8ovAmh_fenH21g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jH19V2I9fIslnNhDzPmdkA</td>\n",
       "      <td>2015-08-12</td>\n",
       "      <td>0</td>\n",
       "      <td>Great breakfast large portions and friendly wa...</td>\n",
       "      <td>ZcLKXikTHYOnYt5VYRO5sg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dAa0hB2yrnHzVmsCkN4YvQ</td>\n",
       "      <td>2014-06-20</td>\n",
       "      <td>0</td>\n",
       "      <td>Nice place. Great staff.  A fixture in the tow...</td>\n",
       "      <td>oaYhjqBbh18ZhU0bpyzSuw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dAa0hB2yrnHzVmsCkN4YvQ</td>\n",
       "      <td>2016-10-12</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy hour 5-7 Monday - Friday</td>\n",
       "      <td>ulQ8Nyj7jCUR8M83SUMoRQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ESzO3Av0b1_TzKOiqzbQYQ</td>\n",
       "      <td>2017-01-28</td>\n",
       "      <td>0</td>\n",
       "      <td>Parking is a premium, keep circling, you will ...</td>\n",
       "      <td>ulQ8Nyj7jCUR8M83SUMoRQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id       date  likes  \\\n",
       "0  tJRDll5yqpZwehenzE2cSg 2012-07-15      0   \n",
       "1  jH19V2I9fIslnNhDzPmdkA 2015-08-12      0   \n",
       "2  dAa0hB2yrnHzVmsCkN4YvQ 2014-06-20      0   \n",
       "3  dAa0hB2yrnHzVmsCkN4YvQ 2016-10-12      0   \n",
       "4  ESzO3Av0b1_TzKOiqzbQYQ 2017-01-28      0   \n",
       "\n",
       "                                                text                 user_id  \n",
       "0              Get here early enough to have dinner.  zcTZk7OG8ovAmh_fenH21g  \n",
       "1  Great breakfast large portions and friendly wa...  ZcLKXikTHYOnYt5VYRO5sg  \n",
       "2  Nice place. Great staff.  A fixture in the tow...  oaYhjqBbh18ZhU0bpyzSuw  \n",
       "3                     Happy hour 5-7 Monday - Friday  ulQ8Nyj7jCUR8M83SUMoRQ  \n",
       "4  Parking is a premium, keep circling, you will ...  ulQ8Nyj7jCUR8M83SUMoRQ  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Count Vectorizer\n",
      "(10000, 24872)\n"
     ]
    }
   ],
   "source": [
    "n_features = 100000\n",
    "\n",
    "text = reviews_df[\"text\"]\n",
    "\n",
    "print(\"Fitting Count Vectorizer\")\n",
    "# vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "#                                 max_features=n_features,\n",
    "#                                 stop_words='english')\n",
    "# word_vector = vectorizer.fit_transform(text)\n",
    "\n",
    "# No setting of hyper-parameters\n",
    "vectorizer = CountVectorizer()\n",
    "word_vector = vectorizer.fit_transform(text)\n",
    "\n",
    "print(np.shape(word_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At ces trade show and looking for lunch. I show up at 2:03 and the host jokingly says we are closed. We laughed. But he meant it. Last year my burger ordered medium came out almost raw. I am never going back\n",
      "1\n",
      "  (0, 17650)\t1\n",
      "  (0, 3376)\t1\n",
      "  (0, 13684)\t1\n",
      "  (0, 12582)\t1\n",
      "  (0, 4549)\t1\n",
      "  (0, 19037)\t1\n",
      "  (0, 11962)\t1\n",
      "  (0, 22483)\t1\n",
      "  (0, 3953)\t1\n",
      "  (0, 10897)\t1\n",
      "  (0, 13729)\t1\n",
      "  (0, 24528)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 12556)\t1\n",
      "  (0, 13164)\t1\n",
      "  (0, 15363)\t1\n",
      "  (0, 13056)\t1\n",
      "  (0, 19747)\t2\n",
      "  (0, 1101)\t1\n",
      "  (0, 10472)\t1\n",
      "  (0, 1133)\t1\n",
      "  (0, 3582)\t1\n",
      "  (0, 15453)\t1\n",
      "  (0, 14751)\t1\n",
      "  (0, 2016)\t1\n",
      "  (0, 1762)\t2\n",
      "  (0, 9793)\t1\n",
      "  (0, 23190)\t1\n",
      "  (0, 23929)\t2\n",
      "  (0, 1555)\t1\n",
      "  (0, 1239)\t2\n",
      "  (0, 8885)\t1\n",
      "  (0, 3440)\t1\n",
      "  (0, 22022)\t1\n",
      "  (0, 11748)\t1\n",
      "  (0, 14510)\t1\n"
     ]
    }
   ],
   "source": [
    "#Print example text, stars, and embeddings\n",
    "\n",
    "print(reviews_df[\"text\"][102])\n",
    "print(reviews_df[\"stars\"][102])\n",
    "print(word_vector[102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB Training and Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_user_reviews = reviews_df[\"text\"][0:6000]\n",
    "# x_dev_user_reviews = reviews_df[\"text\"][6001:8000]\n",
    "# x_test_user_reviews = reviews_df[\"text\"][8001:10000]\n",
    "\n",
    "\n",
    "# x_train_user_reviews = word_vector[0:6000]\n",
    "# x_dev_user_reviews = word_vector[6001:8000]\n",
    "x_train_user_reviews = word_vector[0:8000]\n",
    "x_test_user_reviews = word_vector[8001:10000]\n",
    "\n",
    "# print(\"x_train_user_reviews\", x_train_user_reviews)\n",
    "# print(\"shape x_train_user_reviews\", np.shape(x_train_user_reviews))\n",
    "\n",
    "\n",
    "\n",
    "# y_train_user_stars = reviews_df[\"stars\"][0:6000]\n",
    "# y_dev_user_stars = reviews_df[\"stars\"][6001:8000]\n",
    "y_train_user_stars = reviews_df[\"stars\"][0:8000]\n",
    "y_test_user_stars = reviews_df[\"stars\"][8001:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 55.93%\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(x_train_user_reviews, y_train_user_stars)\n",
    "\n",
    "y_pred = nb.predict(x_test_user_reviews)\n",
    "\n",
    "acc = accuracy_score(y_pred, y_test_user_stars)\n",
    "print(\"Accuracy on test set: {:.02%}\".format(acc))\n",
    "# pred_proba = nb.predict_proba(y_pred)\n",
    "# log_loss_metric = log_loss(y_test_user_stars, pred_proba)\n",
    "# print(\"Log-loss on test set: {:.02%}\".format(log_loss_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Print example prediction\n",
    "\n",
    "print(y_pred[400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"cPickle\"\r\n"
     ]
    }
   ],
   "source": [
    "#!pip cPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN with Attention (old) code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encoding=utf8\n",
    "\n",
    "# import sys\n",
    "# import re\n",
    "# # import cPickle\n",
    "# import _pickle as cPickle\n",
    "# import numpy as np\n",
    "\n",
    "# # For special characters\n",
    "# reload(sys)\n",
    "\n",
    "# # sys.setdefaultencoding('utf8')\n",
    "\n",
    "# _PAD = b\"_PAD\"\n",
    "# _GO = b\"_GO\"\n",
    "# _EOS = b\"_EOS\"\n",
    "# _UNK = b\"_UNK\"\n",
    "# _START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "# PAD_ID = 0\n",
    "# GO_ID = 1\n",
    "# EOS_ID = 2\n",
    "# UNK_ID = 3\n",
    "\n",
    "# _WORD_SPLIT = re.compile(b\"([.,!?\\\"':;)(])\")\n",
    "# _DIGIT_RE = re.compile(BR\"\\d\")\n",
    "\n",
    "# def basic_tokenizer(sentence):\n",
    "#     \"\"\" Split sentence into list of tokens \"\"\"\n",
    "#     words = []\n",
    "#     for space_separated_item in sentence.strip().split():\n",
    "# #         words.extend(_WORD_SPLIT.split(space_separated_item))\n",
    "#         words.extend(space_separated_item)\n",
    "#     return [w for w in words if w] # if w removes the \"\"\n",
    "\n",
    "# def get_vocab(tokenized, max_vocab_size):\n",
    "#     \"\"\"\n",
    "#     Get vocab_list, vocab_dict and rev_vocab_dict given the\n",
    "#     tokenized sentences.\n",
    "#     \"\"\"\n",
    "#     # Replace word count\n",
    "#     vocab = {}\n",
    "# #     for sentence in tokenized:\n",
    "# #         for word in sentence:\n",
    "# #             if word in vocab:\n",
    "# #                 vocab[word] += 1\n",
    "# #             else:\n",
    "# #                 vocab[word] = 1\n",
    "\n",
    "\n",
    "# ### Minimizing looops\n",
    "\n",
    "# #     print(\"tokenized\", tokenized)\n",
    "#     for word in tokenized:\n",
    "# #         print(\"word\", word)\n",
    "#         if word in vocab:\n",
    "#             vocab[word] += 1\n",
    "#         else:\n",
    "#             vocab[word] = 1\n",
    "\n",
    "\n",
    "#     vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "#     if len(vocab_list) > max_vocab_size:\n",
    "#         vocab_list = vocab_list[:max_vocab_size]\n",
    "\n",
    "#     # Get vocab dict (word -> token) and rev dict (token -> word)\n",
    "#     vocab_dict = dict([(x,y) for (y,x) in enumerate(vocab_list)])\n",
    "    \n",
    "# #     print(\"vocab_dict\", vocab_dict)\n",
    "    \n",
    "# #     rev_vocab_dict = {v: k for k, v in vocab_dict.iteritems()}\n",
    "#     rev_vocab_dict = {v: k for k, v in vocab_dict.items()}\n",
    "\n",
    "    \n",
    "#     print(\"vocab_list\", vocab_list)\n",
    "    \n",
    "#     return vocab_list, vocab_dict, rev_vocab_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # def sentence_to_token_ids(sentence, vocab_dict, target_lang,\n",
    "# #     normalize_digits=True):\n",
    "    \n",
    "    \n",
    "# def sentence_to_token_ids(sentence, vocab_dict, normalize_digits=True):\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Convert a single sentence of words to token ids.\n",
    "    \n",
    "#     ############### If it is the target   \n",
    "#     ############### language, we will append an EOS token to the end.\n",
    "#     \"\"\"\n",
    "#     if not normalize_digits:\n",
    "#         # replace words not in vocab_dict with UNK_ID\n",
    "#         tokens = [vocab_dict.get(w, UNK_ID) for w in sentence]\n",
    "#     else:\n",
    "# #         tokens = [vocab_dict.get(_DIGIT_RE.sub(b\"0\", w), UNK_ID)\n",
    "# #             for w in sentence]\n",
    "# #         tokens = [vocab_dict.get(_DIGIT_RE.sub(b\"0\", w), UNK_ID)\n",
    "# #             for w in sentence]\n",
    "#         tokens = [vocab_dict.get(w, UNK_ID) for w in sentence]\n",
    "\n",
    "# #     # Append EOS token if target langauge sentence\n",
    "# #     if target_lang:\n",
    "# #         tokens.append(EOS_ID)\n",
    "\n",
    "#     tokens.append(EOS_ID)\n",
    "\n",
    "    \n",
    "    \n",
    "#     print(\"tokens\", tokens)\n",
    "    \n",
    "    \n",
    "#     return tokens\n",
    "\n",
    "\n",
    "# # def data_to_token_ids(tokenized, vocab_dict, target_lang,\n",
    "# #     normalize_digits=True):\n",
    "    \n",
    "# def data_to_token_ids(tokenized, vocab_dict, normalize_digits=True):\n",
    "#     \"\"\"\n",
    "#     Convert tokens into ids used vocab_dict and normalize all digits\n",
    "#     to 0.\n",
    "#     \"\"\"\n",
    "#     data_as_tokens = []\n",
    "#     seq_lens = []\n",
    "#     max_len = max(len(sentence) for sentence in tokenized) + 1 # +1 for EOS\n",
    "\n",
    "#     for sentence in tokenized:\n",
    "# #         token_ids = sentence_to_token_ids(sentence, vocab_dict, target_lang,\n",
    "# #             normalize_digits)\n",
    "#         token_ids = sentence_to_token_ids(sentence, vocab_dict, normalize_digits)\n",
    "#         # Padding\n",
    "#         data_as_tokens.append(token_ids + [PAD_ID]*(max_len - len(token_ids)))\n",
    "#         # Store original sequence length\n",
    "#         seq_lens.append(len(token_ids))\n",
    "\n",
    "#     return np.array(data_as_tokens), np.array(seq_lens)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # def process_data(datafile, max_vocab_size, target_lang):\n",
    "\n",
    "# def process_data(datafile, max_vocab_size):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     \"\"\"\n",
    "#     ############### Read the sentences from our datafiles.\n",
    "    \n",
    "#     text = reviews_df[\"text\"]\n",
    "    \n",
    "    \n",
    "#     \"\"\"\n",
    "# #     with open(datafile, 'rb') as f:\n",
    "# #         sentences = cPickle.load(f)\n",
    "\n",
    "#     ##############\n",
    "#     sentences = datafile\n",
    "\n",
    "#     # Split into tokens\n",
    "#     tokenized = []\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # From baseline\n",
    "# #     vectorizer = CountVectorizer()\n",
    "# #     word_vector = vectorizer.fit_transform(text)\n",
    "    \n",
    "# #     vectorizer = CountVectorizer()\n",
    "# #     tokenized = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# #######     for i in xrange(len(sentences)):\n",
    "#     for i in range(len(sentences)):\n",
    "#         tokenized.append(basic_tokenizer(sentences[i]))\n",
    "    \n",
    "#     print(\"tokenized type\", type(tokenized))\n",
    "\n",
    "#     print(\"tokenized[200]\", tokenized[200])\n",
    "# #     print(\"tokenized\", tokenized)\n",
    "    \n",
    "\n",
    "#     # Get vocab information\n",
    "#     vocab_list, vocab_dict, rev_vocab_dict = get_vocab(tokenized,\n",
    "#         max_vocab_size)\n",
    "\n",
    "#     # Convert data to token ids\n",
    "# #     data_as_tokens, seq_lens = data_to_token_ids(tokenized, vocab_dict,\n",
    "# #         target_lang, normalize_digits=True)\n",
    "    \n",
    "    \n",
    "#     data_as_tokens, seq_lens = data_to_token_ids(tokenized, vocab_dict, normalize_digits=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     return data_as_tokens, seq_lens, vocab_dict, rev_vocab_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ################### Don't need\n",
    "\n",
    "# ################### Don't need################### Don't need################### Don't need\n",
    "# ################### Don't need\n",
    "# def split_data(en_token_ids, sp_token_ids,\n",
    "#     en_seq_lens, sp_seq_len, train_ratio=0.8):\n",
    "#     \"\"\"\n",
    "#     Split the into train and validation sets.\n",
    "#     \"\"\"\n",
    "\n",
    "#     decoder_inputs = []\n",
    "#     targets = []\n",
    "#     # Add go token to decoder inputs and create targets\n",
    "#     for sentence in sp_token_ids:\n",
    "#         decoder_inputs.append(np.array([GO_ID] + list(sentence)))\n",
    "#         targets.append(np.array(([GO_ID] + list(sentence))[1:] + [0]))\n",
    "\n",
    "#     sp_token_ids = np.array(decoder_inputs)\n",
    "#     targets = np.array(targets)\n",
    "\n",
    "#     # Splitting index\n",
    "#     last_train_index = int(0.8*len(en_token_ids))\n",
    "\n",
    "#     train_encoder_inputs = en_token_ids[:last_train_index]\n",
    "#     train_decoder_inputs = sp_token_ids[:last_train_index]\n",
    "#     train_targets = targets[:last_train_index]\n",
    "#     train_en_seq_lens = en_seq_lens[:last_train_index]\n",
    "#     train_sp_seq_len = sp_seq_len[:last_train_index]\n",
    "\n",
    "#     valid_encoder_inputs = en_token_ids[last_train_index:]\n",
    "#     valid_decoder_inputs = sp_token_ids[last_train_index:]\n",
    "#     valid_targets = targets[last_train_index:]\n",
    "#     valid_en_seq_lens = en_seq_lens[last_train_index:]\n",
    "#     valid_sp_seq_len = sp_seq_len[last_train_index:]\n",
    "\n",
    "# #     print \"%i training samples and %i validations samples\" % (\n",
    "# #         len(train_encoder_inputs), len(valid_encoder_inputs))\n",
    "\n",
    "#     return train_encoder_inputs, train_decoder_inputs, train_targets, \\\n",
    "#         train_en_seq_lens, train_sp_seq_len, \\\n",
    "#         valid_encoder_inputs, valid_decoder_inputs, valid_targets, \\\n",
    "#         valid_en_seq_lens, valid_sp_seq_len\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "# def generate_epoch(encoder_inputs, decoder_inputs, targets, en_seq_lens, sp_seq_lens,\n",
    "#     num_epochs, batch_size):\n",
    "\n",
    "#     for epoch_num in range(num_epochs):\n",
    "#         yield generate_batch(encoder_inputs, decoder_inputs, targets,\n",
    "#             en_seq_lens, sp_seq_lens, batch_size)\n",
    "\n",
    "# def generate_batch(encoder_inputs, decoder_inputs, targets,\n",
    "#     en_seq_lens, sp_seq_lens, batch_size):\n",
    "\n",
    "#     data_size = len(encoder_inputs)\n",
    "\n",
    "#     num_batches = (data_size // batch_size)\n",
    "#     for batch_num in range(num_batches):\n",
    "#         start_index = batch_num * batch_size\n",
    "#         end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "\n",
    "#         yield encoder_inputs[start_index:end_index], \\\n",
    "#             decoder_inputs[start_index:end_index], \\\n",
    "#             targets[start_index:end_index], \\\n",
    "#             en_seq_lens[start_index:end_index], \\\n",
    "#             sp_seq_lens[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ff793a8826ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### (old)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'process_data' is not defined"
     ]
    }
   ],
   "source": [
    "# ### (old)\n",
    "\n",
    "# test = process_data(text, 10000)\n",
    "\n",
    "# print('test', test)\n",
    "# # test = process_data(\"At ces trade show and looking for lunch. I show up at 2:03 and the host jokingly says we are closed. We laughed. But he meant it.\", 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN with Attention (new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'util'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-66932bcfbd05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxavier_weight_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# from tensorflow.models.rnn import rnn, rnn_cell\n",
    "\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "\n",
    "rnn_cell = tf.nn.rnn_cell\n",
    "\n",
    "\n",
    "#rnn= tf.nn.rnn\n",
    "rnn= tf.nn.dynamic_rnn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "import collections\n",
    "import util\n",
    "from random import shuffle\n",
    "from util import xavier_weight_init\n",
    "import sys\n",
    "\n",
    "class Config(object):\n",
    "      \"\"\"Holds model hyperparams and data information.\n",
    "      The config class is used to store various hyperparameters and dataset\n",
    "      information parameters. Model objects are passed a Config() object at\n",
    "      instantiation.\n",
    "      \"\"\"\n",
    "      batch_size =32\n",
    "      batches_per_epoch =  15\n",
    "      step_size= 128 # number of words in a review\n",
    "      input_dim= 128 # this is the word vector size\n",
    "      hidden_dim = 100 # number of nerons per hidden layer\n",
    "      label_dim = 5 # we have a total of classes (like or not like)\n",
    "      max_epochs = 500\n",
    "      early_stopping = 3\n",
    "      dropout =1\n",
    "      learning_rate = 0.001\n",
    "      forget_bias = 1.0\n",
    "      #model = 'RNN' #'BiRNN'\n",
    "      model = 'BiRNN'\n",
    "      cell_type = 'LSTM'\n",
    "      #cell_type = 'GRU'\n",
    "      stack = 1\n",
    "      use_peepholes = False\n",
    "      cell_clip = 1.0\n",
    "      train_file = \"\"\n",
    "      label_file = \"\"\n",
    "      run_type = \"regression\"\n",
    "      multi_learn = False\n",
    "      train_data_dir = \"Data/train\"\n",
    "      val_data_dir = \"Data/val\"\n",
    "      attention=True\n",
    "      test_data_dir = \"Data/test\"\n",
    "      # train_num_reviews = 1\n",
    "      val_num_reviews = 1\n",
    "      marker_list = []\n",
    "      cur_marker = 0\n",
    "      epoch_per_val=4\n",
    "      init='norm'\n",
    "      weight_dir='default'\n",
    "      grad_clip_threshold=5\n",
    "      residual=False\n",
    "\n",
    "\n",
    "\n",
    "class Models(object):\n",
    "\n",
    "    def read_markers(self, data_dir):\n",
    "        for f in os.listdir(data_dir):\n",
    "            if f[0:8] == 'compress':\n",
    "                self.config.marker_list.append(f)\n",
    "    def read_train_file(self, data_dir):\n",
    "            '''\n",
    "            Read the data and label file.\n",
    "            assumed file name conventions:\n",
    "                -file starts with x indicates data file, starts with y indicates label file\n",
    "                -file name x_(# words in a review)_(size of the word vector)_(#of reviews in the file)_(corresponding label marker).data\n",
    "                -file name y_(type of label)_(bucket or regression)_(# words in a review)_(size of the word vector)_(#of reviews in the file)_(corresponding label marker).data\n",
    "            input: data file directory\n",
    "            output:\n",
    "                it outputs a 3 hyper-dimensional structrue as data and a 2 hyper-dimensional structrue as label:\n",
    "                data : [number of reviews [number of words in the review x dimension of word vector]]\n",
    "                label: [number of reviews, [one hot vector if classification, number if regression]]\n",
    "            '''\n",
    "\n",
    "            loaded=np.load(os.path.join(data_dir, self.config.marker_list[int(self.config.cur_marker)]))\n",
    "            self.training_data = loaded['training_data']\n",
    "            self.training_label = loaded['training_label']\n",
    "            self.config.input_dim = self.training_data.shape[2]\n",
    "            self.config.step_size = self.training_data.shape[1]\n",
    "            self.config.label_dim = self.training_label.shape[1]\n",
    "            return\n",
    "\n",
    "    def read_val_file(self, data_dir):\n",
    "\n",
    "        loaded=np.load(os.path.join(data_dir, 'compress_val.npz'))\n",
    "        self.val_data = loaded['training_data']\n",
    "        self.val_label = loaded['training_label']\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def print_model_params(self):\n",
    "        print('*'*99)\n",
    "        print( 'Run Type:', str(self.config.run_type))\n",
    "        print( 'Model:', self.config.model)\n",
    "\n",
    "        print( 'Cell type:', self.config.cell_type)\n",
    "        print( 'Hidden Units:', str(self.config.hidden_dim))\n",
    "\n",
    "        print( \"\\n\")\n",
    "        print( 'Learning rate:', str(self.config.learning_rate))\n",
    "        print( 'init:', str(self.config.init))\n",
    "        print( 'Dropout:', str(self.config.dropout))\n",
    "        print( 'graident threshold', str(self.config.grad_clip_threshold))\n",
    "        print( \"\\n\")\n",
    "\n",
    "        print( 'attention:', str(self.config.attention))\n",
    "        print( 'residual:', str(self.config.residual))\n",
    "        print( 'Stack:', str(self.config.stack))\n",
    "        print( 'step size:', self.config.step_size)\n",
    "        print( 'input dim:', self.config.input_dim)\n",
    "        print( 'batch isze', self.config.batch_size)\n",
    "        print( \"\\n\")\n",
    "        # print() 'review per training file', self.config.train_num_reviews)\n",
    "        print( 'marker list', self.config.marker_list)\n",
    "        print( 'Forget Bias:', str(self.config.forget_bias))\n",
    "        print( 'Peehole:', str(self.config.use_peepholes))\n",
    "        print( '*'*99)\n",
    "\n",
    "    def init_variables(self):\n",
    "            '''\n",
    "            initialize model parameters, note LSTM and BiRNN requires twice the hidden dimenssion due their design\n",
    "            '''\n",
    "            weight_size=self.config.hidden_dim\n",
    "            if self.config.model=='BiRNN':\n",
    "                weight_size_out=2*weight_size\n",
    "                attention_weight = 2*self.config.hidden_dim\n",
    "            else:\n",
    "                weight_size_out=weight_size\n",
    "                attention_weight = self.config.hidden_dim\n",
    "\n",
    "            if self.config.attention:\n",
    "                weight_size_out = self.config.step_size\n",
    "            elif self.config.model!='BiRNN':\n",
    "                weight_size_out = weight_size\n",
    "\n",
    "            xavier_initializer = xavier_weight_init()\n",
    "            # Define weights and bias\n",
    "            with tf.variable_scope(str('test')):\n",
    "                if self.config.init=='norm':\n",
    "                      weights_hidden = tf.Variable(tf.random_normal([self.config.input_dim, weight_size])) # Hidden layer weights\n",
    "                      weights_out = tf.Variable(tf.random_normal([weight_size_out, self.config.label_dim]))\n",
    "                      biases_hidden = tf.Variable(tf.random_normal([weight_size]))\n",
    "                      biases_out = tf.Variable(tf.random_normal([self.config.label_dim]))\n",
    "                      wegiths_attention=tf.Variable(tf.random_normal([attention_weight]))\n",
    "                elif self.config.init=='xaiver':\n",
    "                      weights_hidden = tf.Variable(xavier_initializer((self.config.input_dim, weight_size)))\n",
    "                      weights_out = tf.Variable(xavier_initializer((weight_size_out, self.config.label_dim)))\n",
    "                      biases_hidden =tf.Variable(xavier_initializer((weight_size,)))\n",
    "                      biases_out = tf.Variable(xavier_initializer((self.config.label_dim,)))\n",
    "                      wegiths_attention =tf.Variable(xavier_initializer((attention_weight,)))\n",
    "\n",
    "                self.weights = {\n",
    "                    'hidden': weights_hidden,\n",
    "                    'out1': weights_out\n",
    "                }\n",
    "                self.biases = {\n",
    "                   'hidden': biases_hidden,\n",
    "                    'out1': biases_out\n",
    "                }\n",
    "                for i in range(self.config.step_size):\n",
    "                    self.weights[i]=wegiths_attention#tf.Variable(tf.random_normal([weight_size_out]))\n",
    "                    self.biases[i]=tf.Variable(tf.random_normal([self.config.batch_size]))\n",
    "\n",
    "    def BiRNN(self, scope):\n",
    "            '''\n",
    "            bidirection rnn model\n",
    "            Note: bidirectional model is most useful when tacking RNNs, in single stack case it just averaging two outputs\n",
    "            input: information needed to construct a model. F_bias is only relevant when cell type is LSTM\n",
    "            output:\n",
    "                linear combination of the rnn results and output weights\n",
    "            '''\n",
    "            # input shape: (batch_size, step_size, input_dim)\n",
    "            # we need to permute step_size and batch_size(change the position of step and batch size)\n",
    "            data = tf.transpose(self.input_data, [1, 0, 2])\n",
    "\n",
    "            # Reshape to prepare input to hidden activation\n",
    "            # (step_size*batch_size, n_input), flattens the batch and step\n",
    "            #after the above transformation, data is now (step_size*batch_size, input_dim)\n",
    "            data = tf.reshape(data, [-1, self.config.input_dim])\n",
    "\n",
    "            # Define lstm cells with tensorflow\n",
    "            with tf.variable_scope(str(scope)):\n",
    "                  # Linear activation\n",
    "                  data = tf.matmul(data, self.weights['hidden']) + self.biases['hidden']\n",
    "                  data = tf.nn.dropout(data, self.config.dropout)\n",
    "                  # Define a cell\n",
    "                  if self.config.cell_type == 'GRU':\n",
    "                      lstm_fw_cell = rnn_cell.GRUCell(self.config.hidden_dim)\n",
    "                      lstm_bw_cell = rnn_cell.GRUCell(self.config.hidden_dim)\n",
    "                  else:\n",
    "                      lstm_fw_cell = rnn_cell.LSTMCell(self.config.hidden_dim, forget_bias=self.config.forget_bias,\n",
    "                                                       use_peepholes=self.config.use_peepholes, cell_clip=self.config.cell_clip, state_is_tuple=True)\n",
    "                      lstm_bw_cell = rnn_cell.LSTMCell(self.config.hidden_dim, forget_bias=self.config.forget_bias,\n",
    "                                                       use_peepholes=self.config.use_peepholes, cell_clip=self.config.cell_clip, state_is_tuple=True)\n",
    "\n",
    "                  self.init_state_bw = lstm_bw_cell.zero_state(self.config.batch_size, dtype=tf.float32)\n",
    "                  self.init_state_fw = lstm_fw_cell.zero_state(self.config.batch_size, dtype=tf.float32)\n",
    "\n",
    "                  # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "                  data = tf.split(0, self.config.step_size, data) # step_size * (batch_size, hidden_dim)\n",
    "\n",
    "                  if self.config.stack == 2:\n",
    "                      print('running stack 2.......')\n",
    "                      outputs1, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, data,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN1\")\n",
    "                      outputs, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs1,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN2\")\n",
    "                  elif self.config.stack == 3:\n",
    "                      print('running stack 3.......')\n",
    "                      outputs1, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, data,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN1\")\n",
    "                      outputs2, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs1,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN2\")\n",
    "\n",
    "                      outputs, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs2,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN3\")\n",
    "                  elif self.config.stack == 4:\n",
    "                      print('running stack 4.......')\n",
    "                      outputs1, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, data,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN1\")\n",
    "                      outputs2, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs1,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN2\")\n",
    "                      outputs3, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs2,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN3\")\n",
    "                      outputs, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs3,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN4\")\n",
    "                  elif self.config.stack == 5:\n",
    "                      print('running stack 5.......')\n",
    "                      outputs1, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, data,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN1\")\n",
    "                      outputs2, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs1,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN2\")\n",
    "                      outputs3, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs2,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN3\")\n",
    "                      outputs4, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs3,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN4\")\n",
    "                      outputs, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs4,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN5\")\n",
    "                  elif self.config.stack == 6:\n",
    "                      print('running stack 6.......')\n",
    "                      outputs1, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, data,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN1\")\n",
    "                      outputs2, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs1,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN2\")\n",
    "                      outputs3, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs2,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN3\")\n",
    "                      outputs4, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs3,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN4\")\n",
    "                      outputs5, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs4,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN5\")\n",
    "                      outputs, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs5,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN6\")\n",
    "                  elif self.config.stack == 7:\n",
    "                      print('running stack 7.......')\n",
    "                      outputs1, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, data,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN1\")\n",
    "                      outputs2, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs1,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN2\")\n",
    "                      outputs3, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs2,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN3\")\n",
    "                      outputs4, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs3,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN4\")\n",
    "                      outputs5, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs4,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN5\")\n",
    "                      outputs6, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs5,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN6\")\n",
    "                      outputs, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs6,\n",
    "                                                              initial_state_fw=self.init_state_fw,\n",
    "                                                              initial_state_bw=self.init_state_bw, scope=\"RNN7\")\n",
    "                  else:\n",
    "                      print('running single stack Bi-directional RNN.......')\n",
    "                      outputs, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, data,\n",
    "                                                                    initial_state_fw=self.init_state_fw,\n",
    "                                                                    initial_state_bw=self.init_state_bw, scope=\"RNN1\")\n",
    "\n",
    "                  if self.config.attention:\n",
    "                        pred = self.compute_output(outputs, data)\n",
    "                  else:\n",
    "                        pred = self.compute_output(outputs[-1], data)\n",
    "                  return pred\n",
    "\n",
    "\n",
    "    def RNN(self, scope):\n",
    "            '''\n",
    "            standard rnn model\n",
    "            input: information needed to construct a model. F_bias is only relevant when cell type is LSTM\n",
    "            output:\n",
    "                linear combination of the rnn results and output weights\n",
    "            '''\n",
    "            # input shape: (batch_size, step_size, input_dim)\n",
    "            # we need to permute step_size and batch_size(change the position of step and batch size)\n",
    "            data = tf.transpose(self.input_data, [1, 0, 2])\n",
    "            # Reshape to prepare input to hidden activation\n",
    "            # (step_size*batch_size, n_input), flattens the batch and step\n",
    "            #after the above transformation, data is now (step_size*batch_size, input_dim)\n",
    "            data = tf.reshape(data, [-1, self.config.input_dim])\n",
    "\n",
    "            with tf.variable_scope(str(scope)):\n",
    "                  data = tf.nn.dropout(tf.matmul(data, self.weights['hidden']) + self.biases['hidden'], self.config.dropout)\n",
    "\n",
    "                  # Define a lstm cell with tensorflow\n",
    "                  if self.config.cell_type == 'GRU':\n",
    "                      lstm_cell = rnn_cell.GRUCell(self.config.hidden_dim)\n",
    "                  else:\n",
    "                      lstm_cell = rnn_cell.LSTMCell(self.config.hidden_dim, forget_bias=self.config.forget_bias, state_is_tuple=True)\n",
    "                  self.init_state = lstm_cell.zero_state(self.config.batch_size, dtype=tf.float32)\n",
    "                  # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "                  data = tf.split(0, self.config.step_size, data) # step_size * (batch_size, hidden_dim)\n",
    "\n",
    "                  if self.config.stack == 2:\n",
    "                      print('running stack 2.......')\n",
    "                      output1, states = tf.nn.rnn(lstm_cell, data, initial_state=self.init_state, scope=\"RNN1\")\n",
    "                      outputs, states = tf.nn.rnn(lstm_cell, output1, initial_state=self.init_state, scope=\"RNN2\")\n",
    "                  elif self.config.stack == 3:\n",
    "                      print('running stack 3.......')\n",
    "                      output1, states = tf.nn.rnn(lstm_cell, data, initial_state=self.init_state, scope=\"RNN1\")\n",
    "                      output2, states = tf.nn.rnn(lstm_cell, output1, initial_state=self.init_state, scope=\"RNN2\")\n",
    "                      outputs, states = tf.nn.rnn(lstm_cell, output2, initial_state=self.init_state, scope=\"RNN3\")\n",
    "                  elif self.config.stack == 4:\n",
    "                      print('running stack 4.......')\n",
    "                      output1, states = tf.nn.rnn(lstm_cell, data, initial_state=self.init_state, scope=\"RNN1\")\n",
    "                      output2, states = tf.nn.rnn(lstm_cell, output1, initial_state=self.init_state, scope=\"RNN2\")\n",
    "                      output3, states = tf.nn.rnn(lstm_cell, output2, initial_state=self.init_state, scope=\"RNN3\")\n",
    "                      outputs, states = tf.nn.rnn(lstm_cell, output3, initial_state=self.init_state, scope=\"RNN4\")\n",
    "                  else:\n",
    "                      print('running single stack RNN.......')\n",
    "                      outputs, states = tf.nn.rnn(lstm_cell, data, initial_state=self.init_state, scope=\"RNN1\")\n",
    "\n",
    "                  # Get lstm cell output\n",
    "                  outputs, states = tf.nn.rnn(lstm_cell, data, initial_state=self.init_state)\n",
    "\n",
    "                  # we really just interested in the last state's output\n",
    "                  # return [tf.matmul(outputs[-1], self.weights['out1']) + self.biases['out1']]\n",
    "                  if self.config.attention:\n",
    "                        pred=self.compute_output(outputs)\n",
    "                  else:\n",
    "                        pred =self.compute_output(outputs[-1])\n",
    "                  return pred\n",
    "\n",
    "    def compute_output(self, outputs, data):\n",
    "            if not self.config.attention:\n",
    "                print('running none attention mode.......')\n",
    "                # Linear activation\n",
    "                # for basic rnn prediction we really just interested in the last state's output, we need to average them in this case\n",
    "                return [tf.nn.dropout(tf.matmul(outputs, self.weights['out1']) + self.biases['out1'], self.config.dropout)]\n",
    "            else:\n",
    "                print('running attention mode.......')\n",
    "                # print total_outputs.get_shape()\n",
    "                # print outputs[-1].get_shape()\n",
    "                # we now need to do apply the attention model, the output of each layer comes out from outputs[0], total layer = step_size\n",
    "                # I will first iterate through each layer and multiply the output to its weights\n",
    "                # I will follow the example below, which essentially produces a matrix vector product\n",
    "                # x = tf.constant(5.0, shape=[5, 6])\n",
    "                # w = tf.constant([0.0, 1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "                # xw = tf.mul(x, w)\n",
    "                # max_in_rows = tf.reduce_max(xw, 1), i need to ues reduce sum here\n",
    "                #\n",
    "                # sess = tf.Session()\n",
    "                # print sess.run(xw)\n",
    "                # # ==> [[0.0, 5.0, 10.0, 15.0, 20.0, 25.0],\n",
    "                # #      [0.0, 5.0, 10.0, 15.0, 20.0, 25.0],\n",
    "                # #      [0.0, 5.0, 10.0, 15.0, 20.0, 25.0],\n",
    "                # #      [0.0, 5.0, 10.0, 15.0, 20.0, 25.0],\n",
    "                # #      [0.0, 5.0, 10.0, 15.0, 20.0, 25.0]]\n",
    "                #\n",
    "                # print sess.run(max_in_rows)\n",
    "                # # ==> [25.0, 25.0, 25.0, 25.0, 25.0]\n",
    "                # print self.weights[1].get_shape() #(256,)\n",
    "                # print outputs[27].get_shape() #(?,256)\n",
    "                # attention_list = [tf.reduce_sum(tf.mul(outputs[i], weights[i]),1)+bias[i] for i in range(len(outputs))]\n",
    "                if self.config.residual:\n",
    "                    print('running residual mode.......')\n",
    "                    data = tf.transpose(self.input_data, [1, 0, 2])\n",
    "                    for i in range(self.config.step_size):\n",
    "                        data1 = tf.concat(1, [data[i], data[i]])\n",
    "                        outputs[i]+=data1\n",
    "                else:\n",
    "                    print('running non-residual mode.......')\n",
    "                attention_list = [tf.reduce_sum(tf.mul(outputs[i], self.weights[i]),1)+self.biases[i] for i in range(self.config.step_size)]\n",
    "                #after obtaining the attention list I need to make a vector out of it\n",
    "                attention_vec = tf.transpose(tf.pack(attention_list))\n",
    "                #attention_vec = tf.add(attention_vec,data)\n",
    "                # print self.weights['out1'].get_shape()\n",
    "                pred=[tf.nn.dropout(tf.matmul(attention_vec, self.weights['out1']) + self.biases['out1'], self.config.dropout)]\n",
    "                return pred\n",
    "\n",
    "    def add_placeholders(self):\n",
    "            '''\n",
    "            feeding information to the input placeholders\n",
    "            this function is call as the init process, data are feed in by tensor flow graph\n",
    "            '''\n",
    "            # define graph input place holders\n",
    "            self.input_data = tf.placeholder(\"float\", [None, self.config.step_size, self.config.input_dim])\n",
    "            self.input_label = tf.placeholder(\"float\", [None, self.config.label_dim])\n",
    "\n",
    "    def get_feed_dict(self, data, label):\n",
    "        if (self.config.model == 'BiRNN'):\n",
    "            feed_dict = {self.input_data: data,\n",
    "                         self.input_label: label}\n",
    "        else:\n",
    "            feed_dict = {self.input_data: data,\n",
    "                         self.input_label: label}\n",
    "        return feed_dict\n",
    "\n",
    "    def run_model(self, scope=None, debug=False):\n",
    "            '''\n",
    "            this is the core function that launches the model, it initializes the weights and call the model specified in the config\n",
    "            after model execution it records the test and training loss.\n",
    "            input: model, training data, label, test data/label, and all other paramters needed to run the model\n",
    "            output:\n",
    "                the best learning rate found through cross vaildation.\n",
    "            '''\n",
    "            self.print_model_params()\n",
    "            #making predictions, this actives the rnn model\n",
    "            if (self.config.model ==\"BiRNN\"): pred = self.BiRNN(scope)\n",
    "            elif (self.config.model==\"RNN\"): pred = self.RNN(scope)\n",
    "\n",
    "             # Define loss and optimizer\n",
    "            label1 = tf.split(1, self.config.label_dim, self.input_label)\n",
    "\n",
    "            if self.config.run_type=='regression':\n",
    "                cost = tf.sqrt(tf.reduce_mean(tf.square(tf.sub(pred[0], self.input_label))))\n",
    "\n",
    "            if self.config.run_type=='classification':\n",
    "                cost = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(pred[0], self.input_label))\n",
    "\n",
    "\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "            #opt_func=tf.train.AdamOptimizer(learning_rate=self.config.learning_rate)\n",
    "            #tvars=tf.trainable_variables()\n",
    "\n",
    "            #clip the graident\n",
    "            # tf.gradients(ys, xs, grad_ys=None, name='gradients', colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None)\n",
    "            # Constructs symbolic partial derivatives of sum of ys w.r.t. x in xs.\n",
    "            #grads, _=tf.clip_by_global_norm(tf.gradients(cost, tvars), self.config.grad_clip_threshold)\n",
    "            #optimizer=opt_func.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "            #compute accuracy for classification\n",
    "            class_one_hot_prediction = tf.argmax(self.input_label, 1)\n",
    "            classification_prediction=tf.argmax(tf.nn.softmax(pred[0]),1)\n",
    "            classification_acc =tf.reduce_sum(tf.cast(tf.equal(classification_prediction, class_one_hot_prediction), 'int32'))\n",
    "\n",
    "             # Initializing the variables\n",
    "            init = tf.global_variables_initializer()\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            def ValidationError(_type):\n",
    "                #-------------------------validation starts here-------------------------------------------\n",
    "                    val_loss=[]\n",
    "                    val_epoch = 0\n",
    "                    if _type == 'val':\n",
    "                          print('running validation loss')\n",
    "                          self.read_val_file(self.config.val_data_dir)\n",
    "                    elif _type == 'test':\n",
    "                          print('running test loss')\n",
    "                          self.read_val_file(self.config.test_data_dir)\n",
    "\n",
    "                    train_dropout=self.config.dropout\n",
    "                    self.config.dropout = train_dropout\n",
    "                    val_i=1\n",
    "                    val_last_index = 0\n",
    "                    while val_i*self.config.batch_size <= len(self.val_data):\n",
    "                        samples=[i for i in range(val_last_index, val_i*self.config.batch_size)]\n",
    "                        val_last_index = val_i*self.config.batch_size\n",
    "                        val_i+=1\n",
    "                        sample = np.array(samples)\n",
    "                        input_training_data=self.val_data[sample, :]\n",
    "                        input_training_label=self.val_label[sample, :]\n",
    "                        feed_dict = self.get_feed_dict(input_training_data, input_training_label)\n",
    "                        if self.config.run_type == 'classification':\n",
    "                            loss, match = sess.run([cost, classification_acc], feed_dict)\n",
    "                            acc= 1.0*match/len(input_training_label)\n",
    "                        elif self.config.run_type == 'regression':\n",
    "                            acc = sess.run(cost, feed_dict)\n",
    "                        val_loss.append(acc)\n",
    "                    self.config.dropout=train_dropout\n",
    "                    return 1.0*sum(val_loss)/(len(val_loss))\n",
    "\n",
    "            def SaveWeights():\n",
    "                  # if not os.path.exists(\"./weights\"):\n",
    "                  if not os.path.exists(\"./\"+self.config.weight_dir):\n",
    "                        os.makedirs(\"./\"+self.config.weight_dir)\n",
    "                  path=saver.save(sess, './'+self.config.weight_dir+'/', global_step=None, latest_filename=None, meta_graph_suffix='meta', write_meta_graph=True, write_state=True)\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            #Launch the graph\n",
    "            with tf.Session() as sess:\n",
    "                #saver.restore(sess, './'+self.config.weight_dir+'/')\n",
    "                #print 'weights restored...'\n",
    "                #test_accuracy = ValidationError('test')\n",
    "                #print 'test accuracy', test_accuracy\n",
    "                #return test_accuracy\n",
    "\n",
    "                sess.run(init)\n",
    "                best_val_epoch = 0\n",
    "                if self.config.run_type=='classification':\n",
    "                    best_val_accuracy= float('-inf')\n",
    "                if self.config.run_type=='regression':\n",
    "                    best_val_accuracy= float('inf')\n",
    "                #-------------------------training starts here-------------------------------------------\n",
    "                # I have batches per epoch and epoch per validation check out which is my max_epoch\n",
    "                # I will read one file per time and taking batches out of the file, once the file is exhausted I will move\n",
    "                # on to the next file without interupting the epoch run\n",
    "                # note, number batchs per epoch * batch size must be less than the numver of reviews in a file\n",
    "                index = 1\n",
    "                last_index = 0\n",
    "                total_epoch = 0\n",
    "                val_epoch = 0\n",
    "                val_loss = []\n",
    "                for epoch in xrange(self.config.max_epochs):\n",
    "                    total_epoch +=1\n",
    "                    val_epoch +=1\n",
    "                    train_accuarcy = []\n",
    "                    test_accuracy = 0\n",
    "                    train_loss = []\n",
    "                    counter = 0\n",
    "                    # Training\n",
    "                    total_traing_data = self.training_label.shape[0]\n",
    "                    while counter  < self.config.batches_per_epoch:\n",
    "                        current_index=index*self.config.batch_size\n",
    "                        if current_index >= total_traing_data:\n",
    "                            samples=[i for i in range(total_traing_data-self.config.batch_size, total_traing_data)]\n",
    "                            sample = np.array(samples)\n",
    "                            #samples=np.random.randint(total_traing_data, size=self.config.batch_size)\n",
    "                            input_training_data=self.training_data[sample, :]\n",
    "                            input_training_label=self.training_label[sample, :]\n",
    "                            index = 1\n",
    "                            last_index=0\n",
    "                            self.config.cur_marker+=1\n",
    "                            if self.config.cur_marker == len(self.config.marker_list): self.config.cur_marker = 0\n",
    "                            self.read_train_file(self.config.train_data_dir)\n",
    "                        else:\n",
    "                            samples=[i for i in range(last_index, current_index)]\n",
    "                            last_index = current_index\n",
    "                            index +=1\n",
    "                            sample = np.array(samples)\n",
    "                            input_training_data=self.training_data[sample, :]\n",
    "                            input_training_label=self.training_label[sample, :]\n",
    "\n",
    "                        feed_dict = self.get_feed_dict(input_training_data, input_training_label)\n",
    "                        sess.run(optimizer, feed_dict)\n",
    "                        if self.config.run_type == 'classification':\n",
    "                            loss, match = sess.run([cost, classification_acc], feed_dict)\n",
    "                            acc= 1.0*match/len(input_training_label)\n",
    "                        elif self.config.run_type == 'regression':\n",
    "                            acc = sess.run(cost, feed_dict)\n",
    "                            loss = acc\n",
    "                        train_accuarcy.append(acc)\n",
    "                        train_loss.append(loss)\n",
    "                        counter += 1\n",
    "\n",
    "                    epoch_loss=sum(train_loss)/counter\n",
    "                    epoch_acc=sum(train_accuarcy)/counter\n",
    "                    print(\"Epoch \" + str(epoch) + \", Loss= \" + \"{:.6f}\".format(epoch_loss) + \", Accuracy= \" + \"{:.6f}\".format(epoch_acc))\n",
    "                    sys.stdout.flush()\n",
    "                    if val_epoch == self.config.epoch_per_val:\n",
    "                        val_epoch = 0\n",
    "                        val_accuracy = ValidationError('val')\n",
    "                        if self.config.run_type=='classification':\n",
    "                            if best_val_accuracy<val_accuracy:\n",
    "                                best_val_epoch=total_epoch\n",
    "                                best_val_accuracy= val_accuracy\n",
    "                                SaveWeights()\n",
    "                        if self.config.run_type == 'regression':\n",
    "                            if best_val_accuracy>val_accuracy:\n",
    "                                best_val_epoch=total_epoch\n",
    "                                best_val_accuracy= val_accuracy\n",
    "                                SaveWeights()\n",
    "\n",
    "                        print('*'*30)\n",
    "                        print(str(self.config.run_type)+' validation accuracy at epoch %d: %f'%(total_epoch, val_accuracy))\n",
    "                        print('best validation accuracy so far at epoch %d: %f'%(total_epoch, best_val_accuracy))\n",
    "                        print('*'*30)\n",
    "                print(\"Optimization Finished!\")\n",
    "\n",
    "                saver.restore(sess, './'+self.config.weight_dir+'/')\n",
    "                print('weights restored...')\n",
    "                test_accuracy = ValidationError('test')\n",
    "                print('test accuracy', test_accuracy)\n",
    "                return test_accuracy\n",
    "\n",
    "\n",
    "    def __init__(self, config):\n",
    "      self.config = config\n",
    "      if len(self.config.marker_list) == 0: self.read_markers(self.config.train_data_dir)\n",
    "      self.config_cur_marker=self.config.marker_list[0]\n",
    "      self.read_train_file(self.config.train_data_dir)\n",
    "      self.add_placeholders()\n",
    "      self.init_variables()\n",
    "      self.val_data=[]\n",
    "\n",
    "def run_regression(config=None, stack=1, attention=False, res=False):\n",
    "      for i in range(stack):\n",
    "            ts = int(time.time())\n",
    "            if config is None:\n",
    "                config = Config()\n",
    "            config.run_type='regression'\n",
    "            config.train_data_dir='Data/train/regression'\n",
    "            config.val_data_dir='Data/val/regression'\n",
    "            config.test_data_dir='Data/test/regression'\n",
    "\n",
    "            config.cell_type='LSTM'\n",
    "            #config.cell_type='GRU'\n",
    "            config.model=\"BiRNN\"\n",
    "            #config.model=\"RNN\"\n",
    "            config.learning_rate=0.001\n",
    "            config.batch_size=16\n",
    "            config.batches_per_epoch=80\n",
    "            config.max_epochs=40\n",
    "            config.dropout=1\n",
    "            config.hidden_dim=300\n",
    "            config.epoch_per_val=5\n",
    "            config.stack=i+1\n",
    "            config.attention=attention\n",
    "            #config.init='norm'\n",
    "            config.init='xaiver'\n",
    "            config.grad_clip_threshold = 10000\n",
    "            config.residual=res\n",
    "\n",
    "            config.weight_dir=\"regression_\"+str(config.model)+\"_\"+str(config.cell_type)+\"_\"+\"stack\"+str(config.stack)+\"_\"+str(ts)\n",
    "            if not os.path.exists(\"./\"+config.weight_dir):\n",
    "              os.makedirs(\"./\"+config.weight_dir)\n",
    "            f=open(\"./\"+config.weight_dir+'/run_file.txt', 'a')\n",
    "            sys.stdout = f\n",
    "            model = Models(config)\n",
    "            loss_val = model.run_model(scope=str(i))\n",
    "\n",
    "def run_2classification(config=None,stack=1, attention=False, res=False):\n",
    "    for i in range(5,8):\n",
    "            ts = int(time.time())\n",
    "            if config is None:\n",
    "              config = Config()\n",
    "            config.run_type='classification'\n",
    "            config.train_data_dir='Data/train/2_classification'\n",
    "            config.val_data_dir='Data/val/2_classification'\n",
    "            config.test_data_dir='Data/test/2_classification'\n",
    "            config.cell_type='LSTM'\n",
    "            #config.cell_type='GRU'\n",
    "            config.model=\"BiRNN\"\n",
    "            #config.model=\"RNN\"\n",
    "            config.learning_rate=0.001\n",
    "            config.batch_size=16\n",
    "            config.batches_per_epoch=80\n",
    "            config.max_epochs=40\n",
    "            config.dropout=1\n",
    "            config.hidden_dim=300\n",
    "            config.epoch_per_val=5\n",
    "            config.stack=i+1\n",
    "            config.attention=attention\n",
    "            #config.init='norm'\n",
    "            config.init='xaiver'\n",
    "            config.grad_clip_threshold = 10000\n",
    "            config.residual=res\n",
    "\n",
    "            config.weight_dir=\"attention_2_classification_\"+str(config.model)+\"_\"+str(config.cell_type)+\"_\"+\"stack\"+str(config.stack)+\"_\"+str(ts)\n",
    "            if not os.path.exists(\"./\"+config.weight_dir):\n",
    "              os.makedirs(\"./\"+config.weight_dir)\n",
    "            f=open(\"./\"+config.weight_dir+'/run_file.txt', 'a')\n",
    "            sys.stdout = f\n",
    "            model = Models(config)\n",
    "            loss_val = model.run_model(scope=str(i))\n",
    "\n",
    "#def run_3classification(config=None,stack=1, attention=False):\n",
    "#      for i in range(stack):\n",
    "#            ts = int(time.time())\n",
    "#            ts = int(time.time())\n",
    "#            if config is None:\n",
    "#              config = Config()\n",
    "#\n",
    "#            config.run_type='classification'\n",
    "#            config.train_data_dir='Data/train/3_classification'\n",
    "#            config.val_data_dir='Data/val/3_classification'\n",
    "#            config.test_data_dir='Data/test/3_classification'\n",
    "#            config.weight_dir=\"3_classification_\"+str(ts)\n",
    "#            config.cell_type='LSTM'\n",
    "#            #config.cell_type='GRU'\n",
    "#            config.model=\"BiRNN\"\n",
    "#            #config.model=\"RNN\"\n",
    "#            config.learning_rate=0.001\n",
    "#            config.batch_size=128\n",
    "#            config.batches_per_epoch=5\n",
    "#            config.max_epochs=30\n",
    "#            config.dropout=0.8\n",
    "#            config.hidden_dim=300\n",
    "#            config.epoch_per_val=5\n",
    "#            config.stack=i+1\n",
    "#            config.attention=attention\n",
    "#            #config.init='norm'\n",
    "#            config.init='xaiver'\n",
    "#            config.grad_clip_threshold = 10000\n",
    "#\n",
    "#\n",
    "#            config.weight_dir=\"3_classification_\"+str(config.model)+\"_\"+str(config.cell_type)+\"_\"+\"stack\"+str(config.stack)+\"_\"+str(ts)\n",
    "#            if not os.path.exists(\"./\"+config.weight_dir):\n",
    "#              os.makedirs(\"./\"+config.weight_dir)\n",
    "#            f=open(\"./\"+config.weight_dir+'/run_file.txt', 'a')\n",
    "#            sys.stdout = f\n",
    "#            model = Models(config)\n",
    "#            loss_val = model.run_model(scope=str(i))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(31415)\n",
    "    print(sys.argv[1])\n",
    "    if sys.argv[1] == '2_classification':\n",
    "        run_2classification(stack=1, attention=False)\n",
    "    elif sys.argv[1] == '3_classification':\n",
    "        run_3classification(stack=4, attention=True)\n",
    "    elif sys.argv[1] == 'regression':\n",
    "        run_regression(stack=1, attention=False)\n",
    "    elif sys.argv[1]=='stack_regression':\n",
    "        run_regression(stack=7, attention=True, res=True)\n",
    "    elif sys.argv[1]=='stack_classification':\n",
    "        run_2classification(stack=7, attention=True, res=True)\n",
    "    else:\n",
    "        print('you must select a task to run')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

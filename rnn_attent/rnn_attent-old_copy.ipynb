{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, json, time, unittest, datetime, shutil\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The data below is only on the 10k datasets for now. This will be updated to leverage the full datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_json(\"../dataset/user_10k.json\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_stars</th>\n",
       "      <th>compliment_cool</th>\n",
       "      <th>compliment_cute</th>\n",
       "      <th>compliment_funny</th>\n",
       "      <th>compliment_hot</th>\n",
       "      <th>compliment_list</th>\n",
       "      <th>compliment_more</th>\n",
       "      <th>compliment_note</th>\n",
       "      <th>compliment_photos</th>\n",
       "      <th>compliment_plain</th>\n",
       "      <th>...</th>\n",
       "      <th>cool</th>\n",
       "      <th>elite</th>\n",
       "      <th>fans</th>\n",
       "      <th>friends</th>\n",
       "      <th>funny</th>\n",
       "      <th>name</th>\n",
       "      <th>review_count</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>yelping_since</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[cvVMmlU1ouS3I5fhutaryQ, nj6UZ8tdGo8YJ9lUMTVWN...</td>\n",
       "      <td>0</td>\n",
       "      <td>Johnny</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>oMy_rEb0UBEmMlu-zcxnoQ</td>\n",
       "      <td>2014-11-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0njfJmB-7n84DlIgUByCNw, rFn3Xe3RqHxRSxWOU19Gp...</td>\n",
       "      <td>0</td>\n",
       "      <td>Chris</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>JJ-aSuM4pCFPdkfoZ34q0Q</td>\n",
       "      <td>2013-09-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>Tiffy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>uUzsFQn_6cXDh6rPNGbIFA</td>\n",
       "      <td>2017-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>mBneaEEH5EMyxaVyqS-72A</td>\n",
       "      <td>2015-03-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>Evelyn</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>W5mJGs-dcDWRGEhAzUYtoA</td>\n",
       "      <td>2016-09-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   average_stars  compliment_cool  compliment_cute  compliment_funny  \\\n",
       "0           4.67                0                0                 0   \n",
       "1           3.70                0                0                 0   \n",
       "2           2.00                0                0                 0   \n",
       "3           4.67                0                0                 0   \n",
       "4           4.67                0                0                 0   \n",
       "\n",
       "   compliment_hot  compliment_list  compliment_more  compliment_note  \\\n",
       "0               0                0                0                0   \n",
       "1               0                0                0                0   \n",
       "2               0                0                0                0   \n",
       "3               0                0                0                0   \n",
       "4               0                0                0                0   \n",
       "\n",
       "   compliment_photos  compliment_plain      ...        cool  elite  fans  \\\n",
       "0                  0                 1      ...           0     []     0   \n",
       "1                  0                 0      ...           0     []     0   \n",
       "2                  0                 0      ...           0     []     0   \n",
       "3                  0                 0      ...           0     []     0   \n",
       "4                  0                 0      ...           0     []     0   \n",
       "\n",
       "                                             friends  funny    name  \\\n",
       "0  [cvVMmlU1ouS3I5fhutaryQ, nj6UZ8tdGo8YJ9lUMTVWN...      0  Johnny   \n",
       "1  [0njfJmB-7n84DlIgUByCNw, rFn3Xe3RqHxRSxWOU19Gp...      0   Chris   \n",
       "2                                                 []      0   Tiffy   \n",
       "3                                                 []      0    Mark   \n",
       "4                                                 []      0  Evelyn   \n",
       "\n",
       "   review_count useful                 user_id  yelping_since  \n",
       "0             8      0  oMy_rEb0UBEmMlu-zcxnoQ     2014-11-03  \n",
       "1            10      0  JJ-aSuM4pCFPdkfoZ34q0Q     2013-09-24  \n",
       "2             1      0  uUzsFQn_6cXDh6rPNGbIFA     2017-03-02  \n",
       "3             6      0  mBneaEEH5EMyxaVyqS-72A     2015-03-13  \n",
       "4             3      0  W5mJGs-dcDWRGEhAzUYtoA     2016-09-08  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All types of reviews - 10K dataset\n",
    "# reviews_df = pd.read_json(\"../dataset/review_10k.json\", lines=True)\n",
    "\n",
    "# Just restaurant reviews - 10K dataset\n",
    "reviews_df = pd.read_json(\"../dataset/restaurant_reviews_10k.json\", lines=True)\n",
    "\n",
    "# All types of reviews\n",
    "# reviews_df = pd.read_json(\"../../../final_project/full_dataset/review.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--6MefnULPED_I942VcFNA</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>This is one of my top 3 places to get BBQ pork...</td>\n",
       "      <td>2</td>\n",
       "      <td>FEg8v92qx3kK4Hu4TF28Fg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--6MefnULPED_I942VcFNA</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-31</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>This restaurant is famous for their BBQ dishes...</td>\n",
       "      <td>0</td>\n",
       "      <td>HPtjvIrhzAUkKsiVkeT4MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--6MefnULPED_I942VcFNA</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-10-23</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Roasted pork is one of my favorite things... A...</td>\n",
       "      <td>1</td>\n",
       "      <td>MpvqV7lQcl15rflTBEUhXA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--6MefnULPED_I942VcFNA</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-30</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>I walked by the restaurant more than 5 years a...</td>\n",
       "      <td>1</td>\n",
       "      <td>x-Gbs8sVid3yhJIoHD6Gfw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--6MefnULPED_I942VcFNA</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-02-07</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>I came here to order a roast duck over rice to...</td>\n",
       "      <td>0</td>\n",
       "      <td>7Dykd1HolQx8mKPYhYDYSg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool       date  funny  stars  \\\n",
       "0  --6MefnULPED_I942VcFNA     0 2017-08-17      0      4   \n",
       "1  --6MefnULPED_I942VcFNA     0 2017-05-31      0      3   \n",
       "2  --6MefnULPED_I942VcFNA     0 2016-10-23      0      2   \n",
       "3  --6MefnULPED_I942VcFNA     0 2017-07-30      0      2   \n",
       "4  --6MefnULPED_I942VcFNA     0 2017-02-07      1      2   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  This is one of my top 3 places to get BBQ pork...       2   \n",
       "1  This restaurant is famous for their BBQ dishes...       0   \n",
       "2  Roasted pork is one of my favorite things... A...       1   \n",
       "3  I walked by the restaurant more than 5 years a...       1   \n",
       "4  I came here to order a roast duck over rice to...       0   \n",
       "\n",
       "                  user_id  \n",
       "0  FEg8v92qx3kK4Hu4TF28Fg  \n",
       "1  HPtjvIrhzAUkKsiVkeT4MA  \n",
       "2  MpvqV7lQcl15rflTBEUhXA  \n",
       "3  x-Gbs8sVid3yhJIoHD6Gfw  \n",
       "4  7Dykd1HolQx8mKPYhYDYSg  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = pd.read_json(\"../dataset/business_10k.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>attributes</th>\n",
       "      <th>business_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>city</th>\n",
       "      <th>hours</th>\n",
       "      <th>is_open</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>stars</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4855 E Warner Rd, Ste B9</td>\n",
       "      <td>{'AcceptsInsurance': True, 'ByAppointmentOnly'...</td>\n",
       "      <td>FYWN1wneV18bWNgQjJ2GNg</td>\n",
       "      <td>[Dentists, General Dentistry, Health &amp; Medical...</td>\n",
       "      <td>Ahwatukee</td>\n",
       "      <td>{'Friday': '7:30-17:00', 'Tuesday': '7:30-17:0...</td>\n",
       "      <td>1</td>\n",
       "      <td>33.330690</td>\n",
       "      <td>-111.978599</td>\n",
       "      <td>Dental by Design</td>\n",
       "      <td></td>\n",
       "      <td>85044</td>\n",
       "      <td>22</td>\n",
       "      <td>4.0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3101 Washington Rd</td>\n",
       "      <td>{'BusinessParking': {'garage': False, 'street'...</td>\n",
       "      <td>He-G7vWjzVUysIKrfNbPUQ</td>\n",
       "      <td>[Hair Stylists, Hair Salons, Men's Hair Salons...</td>\n",
       "      <td>McMurray</td>\n",
       "      <td>{'Monday': '9:00-20:00', 'Tuesday': '9:00-20:0...</td>\n",
       "      <td>1</td>\n",
       "      <td>40.291685</td>\n",
       "      <td>-80.104900</td>\n",
       "      <td>Stephen Szabo Salon</td>\n",
       "      <td></td>\n",
       "      <td>15317</td>\n",
       "      <td>11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6025 N 27th Ave, Ste 1</td>\n",
       "      <td>{}</td>\n",
       "      <td>KQPW8lFf1y5BT2MxiSZ3QA</td>\n",
       "      <td>[Departments of Motor Vehicles, Public Service...</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "      <td>33.524903</td>\n",
       "      <td>-112.115310</td>\n",
       "      <td>Western Motor Vehicle</td>\n",
       "      <td></td>\n",
       "      <td>85017</td>\n",
       "      <td>18</td>\n",
       "      <td>1.5</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5000 Arizona Mills Cr, Ste 435</td>\n",
       "      <td>{'BusinessAcceptsCreditCards': True, 'Restaura...</td>\n",
       "      <td>8DShNS-LuFqpEWIp0HxijA</td>\n",
       "      <td>[Sporting Goods, Shopping]</td>\n",
       "      <td>Tempe</td>\n",
       "      <td>{'Monday': '10:00-21:00', 'Tuesday': '10:00-21...</td>\n",
       "      <td>0</td>\n",
       "      <td>33.383147</td>\n",
       "      <td>-111.964725</td>\n",
       "      <td>Sports Authority</td>\n",
       "      <td></td>\n",
       "      <td>85282</td>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>581 Howe Ave</td>\n",
       "      <td>{'Alcohol': 'full_bar', 'HasTV': True, 'NoiseL...</td>\n",
       "      <td>PfOCPjBrlQAnz__NXj9h_w</td>\n",
       "      <td>[American (New), Nightlife, Bars, Sandwiches, ...</td>\n",
       "      <td>Cuyahoga Falls</td>\n",
       "      <td>{'Monday': '11:00-1:00', 'Tuesday': '11:00-1:0...</td>\n",
       "      <td>1</td>\n",
       "      <td>41.119535</td>\n",
       "      <td>-81.475690</td>\n",
       "      <td>Brick House Tavern + Tap</td>\n",
       "      <td></td>\n",
       "      <td>44221</td>\n",
       "      <td>116</td>\n",
       "      <td>3.5</td>\n",
       "      <td>OH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          address  \\\n",
       "0        4855 E Warner Rd, Ste B9   \n",
       "1              3101 Washington Rd   \n",
       "2          6025 N 27th Ave, Ste 1   \n",
       "3  5000 Arizona Mills Cr, Ste 435   \n",
       "4                    581 Howe Ave   \n",
       "\n",
       "                                          attributes             business_id  \\\n",
       "0  {'AcceptsInsurance': True, 'ByAppointmentOnly'...  FYWN1wneV18bWNgQjJ2GNg   \n",
       "1  {'BusinessParking': {'garage': False, 'street'...  He-G7vWjzVUysIKrfNbPUQ   \n",
       "2                                                 {}  KQPW8lFf1y5BT2MxiSZ3QA   \n",
       "3  {'BusinessAcceptsCreditCards': True, 'Restaura...  8DShNS-LuFqpEWIp0HxijA   \n",
       "4  {'Alcohol': 'full_bar', 'HasTV': True, 'NoiseL...  PfOCPjBrlQAnz__NXj9h_w   \n",
       "\n",
       "                                          categories            city  \\\n",
       "0  [Dentists, General Dentistry, Health & Medical...       Ahwatukee   \n",
       "1  [Hair Stylists, Hair Salons, Men's Hair Salons...        McMurray   \n",
       "2  [Departments of Motor Vehicles, Public Service...         Phoenix   \n",
       "3                         [Sporting Goods, Shopping]           Tempe   \n",
       "4  [American (New), Nightlife, Bars, Sandwiches, ...  Cuyahoga Falls   \n",
       "\n",
       "                                               hours  is_open   latitude  \\\n",
       "0  {'Friday': '7:30-17:00', 'Tuesday': '7:30-17:0...        1  33.330690   \n",
       "1  {'Monday': '9:00-20:00', 'Tuesday': '9:00-20:0...        1  40.291685   \n",
       "2                                                 {}        1  33.524903   \n",
       "3  {'Monday': '10:00-21:00', 'Tuesday': '10:00-21...        0  33.383147   \n",
       "4  {'Monday': '11:00-1:00', 'Tuesday': '11:00-1:0...        1  41.119535   \n",
       "\n",
       "    longitude                      name neighborhood postal_code  \\\n",
       "0 -111.978599          Dental by Design                    85044   \n",
       "1  -80.104900       Stephen Szabo Salon                    15317   \n",
       "2 -112.115310     Western Motor Vehicle                    85017   \n",
       "3 -111.964725          Sports Authority                    85282   \n",
       "4  -81.475690  Brick House Tavern + Tap                    44221   \n",
       "\n",
       "   review_count  stars state  \n",
       "0            22    4.0    AZ  \n",
       "1            11    3.0    PA  \n",
       "2            18    1.5    AZ  \n",
       "3             9    3.0    AZ  \n",
       "4           116    3.5    OH  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df = pd.read_json(\"../dataset/checkin_10k.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7KPBkxAOEtb3QeIL9PEErg</td>\n",
       "      <td>{'Thursday': {'21:00': 4, '1:00': 1, '4:00': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kREVIrSBbtqBhIYkTccQUg</td>\n",
       "      <td>{'Monday': {'13:00': 1}, 'Thursday': {'20:00':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tJRDll5yqpZwehenzE2cSg</td>\n",
       "      <td>{'Monday': {'12:00': 1, '1:00': 1}, 'Friday': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r1p7RAMzCV_6NPF0dNoR3g</td>\n",
       "      <td>{'Thursday': {'23:00': 1}, 'Saturday': {'21:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mDdqgfrvROGAumcQdZ3HIg</td>\n",
       "      <td>{'Monday': {'12:00': 1, '21:00': 1}, 'Wednesda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                               time\n",
       "0  7KPBkxAOEtb3QeIL9PEErg  {'Thursday': {'21:00': 4, '1:00': 1, '4:00': 1...\n",
       "1  kREVIrSBbtqBhIYkTccQUg  {'Monday': {'13:00': 1}, 'Thursday': {'20:00':...\n",
       "2  tJRDll5yqpZwehenzE2cSg  {'Monday': {'12:00': 1, '1:00': 1}, 'Friday': ...\n",
       "3  r1p7RAMzCV_6NPF0dNoR3g  {'Thursday': {'23:00': 1}, 'Saturday': {'21:00...\n",
       "4  mDdqgfrvROGAumcQdZ3HIg  {'Monday': {'12:00': 1, '21:00': 1}, 'Wednesda..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_df = pd.read_json(\"../dataset/photos_10k.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>caption</th>\n",
       "      <th>label</th>\n",
       "      <th>photo_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OnAzbTDn79W6CFZIriqLrA</td>\n",
       "      <td></td>\n",
       "      <td>inside</td>\n",
       "      <td>soK1szeyan202jnsGhUDmA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OnAzbTDn79W6CFZIriqLrA</td>\n",
       "      <td></td>\n",
       "      <td>inside</td>\n",
       "      <td>dU7AyRB_fHOZkflodEyN5A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OnAzbTDn79W6CFZIriqLrA</td>\n",
       "      <td></td>\n",
       "      <td>outside</td>\n",
       "      <td>6T1qlbBdKkXA1cDNqMjg2g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OnAzbTDn79W6CFZIriqLrA</td>\n",
       "      <td>Bakery area</td>\n",
       "      <td>inside</td>\n",
       "      <td>lHhMNhCA7rAZmi-MMfF3ZA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XaeCGHZzsMwvFcHYq3q9sA</td>\n",
       "      <td></td>\n",
       "      <td>food</td>\n",
       "      <td>oHSCeyoK9oLIGaCZq-wRJw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id      caption    label                photo_id\n",
       "0  OnAzbTDn79W6CFZIriqLrA                inside  soK1szeyan202jnsGhUDmA\n",
       "1  OnAzbTDn79W6CFZIriqLrA                inside  dU7AyRB_fHOZkflodEyN5A\n",
       "2  OnAzbTDn79W6CFZIriqLrA               outside  6T1qlbBdKkXA1cDNqMjg2g\n",
       "3  OnAzbTDn79W6CFZIriqLrA  Bakery area   inside  lHhMNhCA7rAZmi-MMfF3ZA\n",
       "4  XaeCGHZzsMwvFcHYq3q9sA                  food  oHSCeyoK9oLIGaCZq-wRJw"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_df = pd.read_json(\"../dataset/tip_10k.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>likes</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tJRDll5yqpZwehenzE2cSg</td>\n",
       "      <td>2012-07-15</td>\n",
       "      <td>0</td>\n",
       "      <td>Get here early enough to have dinner.</td>\n",
       "      <td>zcTZk7OG8ovAmh_fenH21g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jH19V2I9fIslnNhDzPmdkA</td>\n",
       "      <td>2015-08-12</td>\n",
       "      <td>0</td>\n",
       "      <td>Great breakfast large portions and friendly wa...</td>\n",
       "      <td>ZcLKXikTHYOnYt5VYRO5sg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dAa0hB2yrnHzVmsCkN4YvQ</td>\n",
       "      <td>2014-06-20</td>\n",
       "      <td>0</td>\n",
       "      <td>Nice place. Great staff.  A fixture in the tow...</td>\n",
       "      <td>oaYhjqBbh18ZhU0bpyzSuw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dAa0hB2yrnHzVmsCkN4YvQ</td>\n",
       "      <td>2016-10-12</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy hour 5-7 Monday - Friday</td>\n",
       "      <td>ulQ8Nyj7jCUR8M83SUMoRQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ESzO3Av0b1_TzKOiqzbQYQ</td>\n",
       "      <td>2017-01-28</td>\n",
       "      <td>0</td>\n",
       "      <td>Parking is a premium, keep circling, you will ...</td>\n",
       "      <td>ulQ8Nyj7jCUR8M83SUMoRQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id       date  likes  \\\n",
       "0  tJRDll5yqpZwehenzE2cSg 2012-07-15      0   \n",
       "1  jH19V2I9fIslnNhDzPmdkA 2015-08-12      0   \n",
       "2  dAa0hB2yrnHzVmsCkN4YvQ 2014-06-20      0   \n",
       "3  dAa0hB2yrnHzVmsCkN4YvQ 2016-10-12      0   \n",
       "4  ESzO3Av0b1_TzKOiqzbQYQ 2017-01-28      0   \n",
       "\n",
       "                                                text                 user_id  \n",
       "0              Get here early enough to have dinner.  zcTZk7OG8ovAmh_fenH21g  \n",
       "1  Great breakfast large portions and friendly wa...  ZcLKXikTHYOnYt5VYRO5sg  \n",
       "2  Nice place. Great staff.  A fixture in the tow...  oaYhjqBbh18ZhU0bpyzSuw  \n",
       "3                     Happy hour 5-7 Monday - Friday  ulQ8Nyj7jCUR8M83SUMoRQ  \n",
       "4  Parking is a premium, keep circling, you will ...  ulQ8Nyj7jCUR8M83SUMoRQ  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Count Vectorizer\n",
      "(10000, 24872)\n"
     ]
    }
   ],
   "source": [
    "n_features = 100000\n",
    "\n",
    "text = reviews_df[\"text\"]\n",
    "\n",
    "print(\"Fitting Count Vectorizer\")\n",
    "# vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "#                                 max_features=n_features,\n",
    "#                                 stop_words='english')\n",
    "# word_vector = vectorizer.fit_transform(text)\n",
    "\n",
    "# No setting of hyper-parameters\n",
    "vectorizer = CountVectorizer()\n",
    "word_vector = vectorizer.fit_transform(text)\n",
    "\n",
    "print(np.shape(word_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At ces trade show and looking for lunch. I show up at 2:03 and the host jokingly says we are closed. We laughed. But he meant it. Last year my burger ordered medium came out almost raw. I am never going back\n",
      "1\n",
      "  (0, 17650)\t1\n",
      "  (0, 3376)\t1\n",
      "  (0, 13684)\t1\n",
      "  (0, 12582)\t1\n",
      "  (0, 4549)\t1\n",
      "  (0, 19037)\t1\n",
      "  (0, 11962)\t1\n",
      "  (0, 22483)\t1\n",
      "  (0, 3953)\t1\n",
      "  (0, 10897)\t1\n",
      "  (0, 13729)\t1\n",
      "  (0, 24528)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 12556)\t1\n",
      "  (0, 13164)\t1\n",
      "  (0, 15363)\t1\n",
      "  (0, 13056)\t1\n",
      "  (0, 19747)\t2\n",
      "  (0, 1101)\t1\n",
      "  (0, 10472)\t1\n",
      "  (0, 1133)\t1\n",
      "  (0, 3582)\t1\n",
      "  (0, 15453)\t1\n",
      "  (0, 14751)\t1\n",
      "  (0, 2016)\t1\n",
      "  (0, 1762)\t2\n",
      "  (0, 9793)\t1\n",
      "  (0, 23190)\t1\n",
      "  (0, 23929)\t2\n",
      "  (0, 1555)\t1\n",
      "  (0, 1239)\t2\n",
      "  (0, 8885)\t1\n",
      "  (0, 3440)\t1\n",
      "  (0, 22022)\t1\n",
      "  (0, 11748)\t1\n",
      "  (0, 14510)\t1\n"
     ]
    }
   ],
   "source": [
    "#Print example text, stars, and embeddings\n",
    "\n",
    "print(reviews_df[\"text\"][102])\n",
    "print(reviews_df[\"stars\"][102])\n",
    "print(word_vector[102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB Training and Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_user_reviews = reviews_df[\"text\"][0:6000]\n",
    "# x_dev_user_reviews = reviews_df[\"text\"][6001:8000]\n",
    "# x_test_user_reviews = reviews_df[\"text\"][8001:10000]\n",
    "\n",
    "\n",
    "# x_train_user_reviews = word_vector[0:6000]\n",
    "# x_dev_user_reviews = word_vector[6001:8000]\n",
    "x_train_user_reviews = word_vector[0:8000]\n",
    "x_test_user_reviews = word_vector[8001:10000]\n",
    "\n",
    "# print(\"x_train_user_reviews\", x_train_user_reviews)\n",
    "# print(\"shape x_train_user_reviews\", np.shape(x_train_user_reviews))\n",
    "\n",
    "\n",
    "\n",
    "# y_train_user_stars = reviews_df[\"stars\"][0:6000]\n",
    "# y_dev_user_stars = reviews_df[\"stars\"][6001:8000]\n",
    "y_train_user_stars = reviews_df[\"stars\"][0:8000]\n",
    "y_test_user_stars = reviews_df[\"stars\"][8001:10000]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_file = x_train_user_reviews\n",
    "label_file = y_train_user_stars\n",
    "training_data = x_train_user_reviews\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> 133             self.config.input_dim = self.training_data.shape[2]\n",
    "#     134             self.config.step_size = self.training_data.shape[1]\n",
    "#     135             self.config.label_dim = self.training_label.shape[1]\n",
    "\n",
    "# # print(training_data.shape[2])\n",
    "# print(training_data.shape[1])\n",
    "# print(np.shape(training_data))\n",
    "# print(len(training_data))\n",
    "# print(\"hi\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 55.93%\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(x_train_user_reviews, y_train_user_stars)\n",
    "\n",
    "y_pred = nb.predict(x_test_user_reviews)\n",
    "\n",
    "acc = accuracy_score(y_pred, y_test_user_stars)\n",
    "print(\"Accuracy on test set: {:.02%}\".format(acc))\n",
    "# pred_proba = nb.predict_proba(y_pred)\n",
    "# log_loss_metric = log_loss(y_test_user_stars, pred_proba)\n",
    "# print(\"Log-loss on test set: {:.02%}\".format(log_loss_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Print example prediction\n",
    "\n",
    "print(y_pred[400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Smallest GloVe file\n",
    "# gloveFile = \"../../glove/glove.6B.50d.txt\"\n",
    "\n",
    "# # Smaller GloVe file\n",
    "# gloveFile = \"../../glove/glove.6B.300d.txt\"\n",
    "\n",
    "# # Primary GloVe file\n",
    "# # gloveFile = \"../../glove/glove.42B.300d.txt\"\n",
    "\n",
    "# import numpy as np\n",
    "# def loadGloveModel(gloveFile):\n",
    "#     print(\"Loading Glove Model\")\n",
    "#     f = open(gloveFile,'r')\n",
    "#     model = {}\n",
    "#     for line in f:\n",
    "#         splitLine = line.split()\n",
    "#         word = splitLine[0]\n",
    "#         embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "#         model[word] = embedding\n",
    "#     print(\"Done.\",len(model),\" words loaded!\")\n",
    "#     return model\n",
    "\n",
    "# loadGloveModel(gloveFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Smallest GloVe file\n",
    "# gloveFile = \"../../glove/glove.6B.50d.txt\"\n",
    "\n",
    "# # # Smaller GloVe file\n",
    "# # gloveFile = \"../../glove/glove.6B.300d.txt\"\n",
    "\n",
    "# # # Primary GloVe file\n",
    "# # gloveFile = \"../../glove/glove.42B.300d.txt\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # def load_glove(self, glove_folder):\n",
    "# # def load_glove(self):\n",
    "# def load_glove():\n",
    "# #     self.console('Loading GloVe embeddings...')\n",
    "#     print('Loading GloVe embeddings...')\n",
    "#     glove = {}\n",
    "#     count = 0\n",
    "# #     with open(os.path.join(glove_folder, 'glove.6B.' + str(self._parms['embedding_dim']) + 'd.txt'), 'r') as f:\n",
    "#     with open(gloveFile, 'r') as f:\n",
    "#         while True:\n",
    "#             line = f.readline()\n",
    "#             if not line:\n",
    "#                 break\n",
    "#             line = line.split(' ')\n",
    "#             word = line[0]\n",
    "#             vector = np.asarray(line[1:], dtype='float32')\n",
    "#             glove[word] = vector\n",
    "# #     self._glove = glove\n",
    "# #     self.console('%d embeddings loaded.' % len(self._glove))\n",
    "\n",
    "#     print('%d embeddings loaded.' % len(glove))\n",
    "    \n",
    "#     return glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = datetime.datetime.now()\n",
    "# print(\"Start time: \", start_time)\n",
    "\n",
    "# test_glove = load_glove()\n",
    "\n",
    "# end_time = datetime.datetime.now()\n",
    "# print(\"End time: \", end_time)\n",
    "\n",
    "# time_taken = end_time - start_time\n",
    "# print(\"Time taken: \", time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(test_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN with Attention (new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import os\n",
    "\n",
    "# # from tensorflow.models.rnn import rnn, rnn_cell\n",
    "\n",
    "# from tensorflow.python.ops import rnn, rnn_cell\n",
    "\n",
    "# rnn_cell = tf.nn.rnn_cell\n",
    "\n",
    "\n",
    "# #rnn= tf.nn.rnn\n",
    "# rnn= tf.nn.dynamic_rnn\n",
    "\n",
    "\n",
    "# # train_file = x_train_user_reviews\n",
    "# # label_file = y_train_user_stars\n",
    "\n",
    "\n",
    "\n",
    "# import time\n",
    "# import numpy as np\n",
    "# import csv\n",
    "# import random\n",
    "# import collections\n",
    "# # import util\n",
    "# from random import shuffle\n",
    "# # from util import xavier_weight_init\n",
    "# import sys\n",
    "\n",
    "# class Config(object):\n",
    "#       \"\"\"Holds model hyperparams and data information.\n",
    "#       The config class is used to store various hyperparameters and dataset\n",
    "#       information parameters. Model objects are passed a Config() object at\n",
    "#       instantiation.\n",
    "#       \"\"\"\n",
    "#       batch_size =32\n",
    "#       batches_per_epoch =  15\n",
    "#       step_size= 128 # number of words in a review\n",
    "#       input_dim= 128 # this is the word vector size\n",
    "#       hidden_dim = 100 # number of nerons per hidden layer\n",
    "#       label_dim = 5 # we have a total of classes (like or not like)\n",
    "#       max_epochs = 500\n",
    "#       early_stopping = 3\n",
    "#       dropout =1\n",
    "#       learning_rate = 0.001\n",
    "#       forget_bias = 1.0\n",
    "#       #model = 'RNN' #'BiRNN'\n",
    "#       model = 'BiRNN'\n",
    "#       cell_type = 'LSTM'\n",
    "#       #cell_type = 'GRU'\n",
    "#       stack = 1\n",
    "#       use_peepholes = False\n",
    "#       cell_clip = 1.0\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# #       train_file = \"\"\n",
    "# #       label_file = \"\"\n",
    "        \n",
    "#       train_file = x_train_user_reviews\n",
    "#       label_file = y_train_user_stars\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#       run_type = \"regression\"\n",
    "#       multi_learn = False\n",
    "        \n",
    "        \n",
    "        \n",
    "#       train_data_dir = \"Data/train\"\n",
    "#       val_data_dir = \"Data/val\"\n",
    "        \n",
    "        \n",
    "#       attention=True\n",
    "    \n",
    "    \n",
    "    \n",
    "#       test_data_dir = \"Data/test\"\n",
    "        \n",
    "        \n",
    "        \n",
    "#       # train_num_reviews = 1\n",
    "#       val_num_reviews = 1\n",
    "#       marker_list = []\n",
    "#       cur_marker = 0\n",
    "#       epoch_per_val=4\n",
    "#       init='norm'\n",
    "#       weight_dir='default'\n",
    "#       grad_clip_threshold=5\n",
    "#       residual=False\n",
    "\n",
    "\n",
    "\n",
    "# class Models(object):\n",
    "\n",
    "# #     def read_markers(self, data_dir):\n",
    "# #         for f in os.listdir(data_dir):\n",
    "# #             if f[0:8] == 'compress':\n",
    "# #                 self.config.marker_list.append(f)\n",
    "# #     def read_train_file(self, data_dir):\n",
    "#     def read_train_file(self):\n",
    "#             '''\n",
    "#             Read the data and label file.\n",
    "#             assumed file name conventions:\n",
    "#                 -file starts with x indicates data file, starts with y indicates label file\n",
    "#                 -file name x_(# words in a review)_(size of the word vector)_(#of reviews in the file)_(corresponding label marker).data\n",
    "#                 -file name y_(type of label)_(bucket or regression)_(# words in a review)_(size of the word vector)_(#of reviews in the file)_(corresponding label marker).data\n",
    "#             input: data file directory\n",
    "#             output:\n",
    "#                 it outputs a 3 hyper-dimensional structrue as data and a 2 hyper-dimensional structrue as label:\n",
    "#                 data : [number of reviews [number of words in the review x dimension of word vector]]\n",
    "#                 label: [number of reviews, [one hot vector if classification, number if regression]]\n",
    "#             '''\n",
    "\n",
    "# #             loaded=np.load(os.path.join(data_dir, self.config.marker_list[int(self.config.cur_marker)]))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "# #             self.training_data = loaded['training_data']\n",
    "# #             self.training_label = loaded['training_label']\n",
    "            \n",
    "            \n",
    "#             self.training_data = train_file\n",
    "\n",
    "#             self.training_label = label_file\n",
    "            \n",
    "            \n",
    "#             self.config.input_dim = self.training_data.shape[2]\n",
    "#             self.config.step_size = self.training_data.shape[1]\n",
    "#             self.config.label_dim = self.training_label.shape[1]\n",
    "#             return\n",
    "\n",
    "#     def read_val_file(self, data_dir):\n",
    "\n",
    "#         loaded=np.load(os.path.join(data_dir, 'compress_val.npz'))\n",
    "#         self.val_data = loaded['training_data']\n",
    "#         self.val_label = loaded['training_label']\n",
    "\n",
    "#         return\n",
    "\n",
    "\n",
    "#     def print_model_params(self):\n",
    "#         print('*'*99)\n",
    "#         print( 'Run Type:', str(self.config.run_type))\n",
    "#         print( 'Model:', self.config.model)\n",
    "\n",
    "#         print( 'Cell type:', self.config.cell_type)\n",
    "#         print( 'Hidden Units:', str(self.config.hidden_dim))\n",
    "\n",
    "#         print( \"\\n\")\n",
    "#         print( 'Learning rate:', str(self.config.learning_rate))\n",
    "#         print( 'init:', str(self.config.init))\n",
    "#         print( 'Dropout:', str(self.config.dropout))\n",
    "#         print( 'graident threshold', str(self.config.grad_clip_threshold))\n",
    "#         print( \"\\n\")\n",
    "\n",
    "#         print( 'attention:', str(self.config.attention))\n",
    "#         print( 'residual:', str(self.config.residual))\n",
    "#         print( 'Stack:', str(self.config.stack))\n",
    "#         print( 'step size:', self.config.step_size)\n",
    "#         print( 'input dim:', self.config.input_dim)\n",
    "#         print( 'batch isze', self.config.batch_size)\n",
    "#         print( \"\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print( 'review per training file', self.config.train_num_reviews)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         print( 'marker list', self.config.marker_list)\n",
    "#         print( 'Forget Bias:', str(self.config.forget_bias))\n",
    "#         print( 'Peephole:', str(self.config.use_peepholes))\n",
    "#         print( '*'*99)\n",
    "\n",
    "#     def init_variables(self):\n",
    "#             '''\n",
    "#             initialize model parameters, note LSTM and BiRNN requires twice the hidden dimenssion due their design\n",
    "#             '''\n",
    "#             weight_size=self.config.hidden_dim\n",
    "#             if self.config.model=='BiRNN':\n",
    "#                 weight_size_out=2*weight_size\n",
    "#                 attention_weight = 2*self.config.hidden_dim\n",
    "#             else:\n",
    "#                 weight_size_out=weight_size\n",
    "#                 attention_weight = self.config.hidden_dim\n",
    "\n",
    "#             if self.config.attention:\n",
    "#                 weight_size_out = self.config.step_size\n",
    "#             elif self.config.model!='BiRNN':\n",
    "#                 weight_size_out = weight_size\n",
    "\n",
    "#             xavier_initializer = xavier_weight_init()\n",
    "#             # Define weights and bias\n",
    "#             with tf.variable_scope(str('test')):\n",
    "#                 if self.config.init=='norm':\n",
    "#                       weights_hidden = tf.Variable(tf.random_normal([self.config.input_dim, weight_size])) # Hidden layer weights\n",
    "#                       weights_out = tf.Variable(tf.random_normal([weight_size_out, self.config.label_dim]))\n",
    "#                       biases_hidden = tf.Variable(tf.random_normal([weight_size]))\n",
    "#                       biases_out = tf.Variable(tf.random_normal([self.config.label_dim]))\n",
    "#                       wegiths_attention=tf.Variable(tf.random_normal([attention_weight]))\n",
    "#                 elif self.config.init=='xaiver':\n",
    "#                       weights_hidden = tf.Variable(xavier_initializer((self.config.input_dim, weight_size)))\n",
    "#                       weights_out = tf.Variable(xavier_initializer((weight_size_out, self.config.label_dim)))\n",
    "#                       biases_hidden =tf.Variable(xavier_initializer((weight_size,)))\n",
    "#                       biases_out = tf.Variable(xavier_initializer((self.config.label_dim,)))\n",
    "#                       wegiths_attention =tf.Variable(xavier_initializer((attention_weight,)))\n",
    "\n",
    "#                 self.weights = {\n",
    "#                     'hidden': weights_hidden,\n",
    "#                     'out1': weights_out\n",
    "#                 }\n",
    "#                 self.biases = {\n",
    "#                    'hidden': biases_hidden,\n",
    "#                     'out1': biases_out\n",
    "#                 }\n",
    "#                 for i in range(self.config.step_size):\n",
    "#                     self.weights[i]=wegiths_attention#tf.Variable(tf.random_normal([weight_size_out]))\n",
    "#                     self.biases[i]=tf.Variable(tf.random_normal([self.config.batch_size]))\n",
    "\n",
    "#     def BiRNN(self, scope):\n",
    "#             '''\n",
    "#             bidirection rnn model\n",
    "#             Note: bidirectional model is most useful when tacking RNNs, in single stack case it just averaging two outputs\n",
    "#             input: information needed to construct a model. F_bias is only relevant when cell type is LSTM\n",
    "#             output:\n",
    "#                 linear combination of the rnn results and output weights\n",
    "#             '''\n",
    "#             # input shape: (batch_size, step_size, input_dim)\n",
    "#             # we need to permute step_size and batch_size(change the position of step and batch size)\n",
    "#             data = tf.transpose(self.input_data, [1, 0, 2])\n",
    "\n",
    "#             # Reshape to prepare input to hidden activation\n",
    "#             # (step_size*batch_size, n_input), flattens the batch and step\n",
    "#             #after the above transformation, data is now (step_size*batch_size, input_dim)\n",
    "#             data = tf.reshape(data, [-1, self.config.input_dim])\n",
    "\n",
    "#             # Define lstm cells with tensorflow\n",
    "#             with tf.variable_scope(str(scope)):\n",
    "#                   # Linear activation\n",
    "#                   data = tf.matmul(data, self.weights['hidden']) + self.biases['hidden']\n",
    "#                   data = tf.nn.dropout(data, self.config.dropout)\n",
    "#                   # Define a cell\n",
    "#                   if self.config.cell_type == 'GRU':\n",
    "#                       lstm_fw_cell = rnn_cell.GRUCell(self.config.hidden_dim)\n",
    "#                       lstm_bw_cell = rnn_cell.GRUCell(self.config.hidden_dim)\n",
    "#                   else:\n",
    "#                       lstm_fw_cell = rnn_cell.LSTMCell(self.config.hidden_dim, forget_bias=self.config.forget_bias,\n",
    "#                                                        use_peepholes=self.config.use_peepholes, cell_clip=self.config.cell_clip, state_is_tuple=True)\n",
    "#                       lstm_bw_cell = rnn_cell.LSTMCell(self.config.hidden_dim, forget_bias=self.config.forget_bias,\n",
    "#                                                        use_peepholes=self.config.use_peepholes, cell_clip=self.config.cell_clip, state_is_tuple=True)\n",
    "\n",
    "#                   self.init_state_bw = lstm_bw_cell.zero_state(self.config.batch_size, dtype=tf.float32)\n",
    "#                   self.init_state_fw = lstm_fw_cell.zero_state(self.config.batch_size, dtype=tf.float32)\n",
    "\n",
    "#                   # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "#                   data = tf.split(0, self.config.step_size, data) # step_size * (batch_size, hidden_dim)\n",
    "\n",
    "#                   if self.config.stack == 2:\n",
    "#                       print('running stack 2.......')\n",
    "#                       outputs1, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, data,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN1\")\n",
    "#                       outputs, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs1,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN2\")\n",
    "#                   elif self.config.stack == 3:\n",
    "#                       print('running stack 3.......')\n",
    "#                       outputs1, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, data,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN1\")\n",
    "#                       outputs2, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs1,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN2\")\n",
    "\n",
    "#                       outputs, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs2,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN3\")\n",
    "#                   elif self.config.stack == 4:\n",
    "#                       print('running stack 4.......')\n",
    "#                       outputs1, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, data,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN1\")\n",
    "#                       outputs2, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs1,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN2\")\n",
    "#                       outputs3, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs2,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN3\")\n",
    "#                       outputs, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs3,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN4\")\n",
    "#                   elif self.config.stack == 5:\n",
    "#                       print('running stack 5.......')\n",
    "#                       outputs1, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, data,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN1\")\n",
    "#                       outputs2, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs1,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN2\")\n",
    "#                       outputs3, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs2,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN3\")\n",
    "#                       outputs4, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs3,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN4\")\n",
    "#                       outputs, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs4,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN5\")\n",
    "#                   elif self.config.stack == 6:\n",
    "#                       print('running stack 6.......')\n",
    "#                       outputs1, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, data,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN1\")\n",
    "#                       outputs2, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs1,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN2\")\n",
    "#                       outputs3, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs2,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN3\")\n",
    "#                       outputs4, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs3,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN4\")\n",
    "#                       outputs5, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs4,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN5\")\n",
    "#                       outputs, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs5,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN6\")\n",
    "#                   elif self.config.stack == 7:\n",
    "#                       print('running stack 7.......')\n",
    "#                       outputs1, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, data,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN1\")\n",
    "#                       outputs2, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs1,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN2\")\n",
    "#                       outputs3, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs2,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN3\")\n",
    "#                       outputs4, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs3,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN4\")\n",
    "#                       outputs5, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs4,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN5\")\n",
    "#                       outputs6, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs5,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN6\")\n",
    "#                       outputs, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, outputs6,\n",
    "#                                                               initial_state_fw=self.init_state_fw,\n",
    "#                                                               initial_state_bw=self.init_state_bw, scope=\"RNN7\")\n",
    "#                   else:\n",
    "#                       print('running single stack Bi-directional RNN.......')\n",
    "#                       outputs, output_state_fw,output_state_bw  = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, data,\n",
    "#                                                                     initial_state_fw=self.init_state_fw,\n",
    "#                                                                     initial_state_bw=self.init_state_bw, scope=\"RNN1\")\n",
    "\n",
    "#                   if self.config.attention:\n",
    "#                         pred = self.compute_output(outputs, data)\n",
    "#                   else:\n",
    "#                         pred = self.compute_output(outputs[-1], data)\n",
    "#                   return pred\n",
    "\n",
    "\n",
    "#     def RNN(self, scope):\n",
    "#             '''\n",
    "#             standard rnn model\n",
    "#             input: information needed to construct a model. F_bias is only relevant when cell type is LSTM\n",
    "#             output:\n",
    "#                 linear combination of the rnn results and output weights\n",
    "#             '''\n",
    "#             # input shape: (batch_size, step_size, input_dim)\n",
    "#             # we need to permute step_size and batch_size(change the position of step and batch size)\n",
    "#             data = tf.transpose(self.input_data, [1, 0, 2])\n",
    "#             # Reshape to prepare input to hidden activation\n",
    "#             # (step_size*batch_size, n_input), flattens the batch and step\n",
    "#             #after the above transformation, data is now (step_size*batch_size, input_dim)\n",
    "#             data = tf.reshape(data, [-1, self.config.input_dim])\n",
    "\n",
    "#             with tf.variable_scope(str(scope)):\n",
    "#                   data = tf.nn.dropout(tf.matmul(data, self.weights['hidden']) + self.biases['hidden'], self.config.dropout)\n",
    "\n",
    "#                   # Define a lstm cell with tensorflow\n",
    "#                   if self.config.cell_type == 'GRU':\n",
    "#                       lstm_cell = rnn_cell.GRUCell(self.config.hidden_dim)\n",
    "#                   else:\n",
    "#                       lstm_cell = rnn_cell.LSTMCell(self.config.hidden_dim, forget_bias=self.config.forget_bias, state_is_tuple=True)\n",
    "#                   self.init_state = lstm_cell.zero_state(self.config.batch_size, dtype=tf.float32)\n",
    "#                   # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "#                   data = tf.split(0, self.config.step_size, data) # step_size * (batch_size, hidden_dim)\n",
    "\n",
    "#                   if self.config.stack == 2:\n",
    "#                       print('running stack 2.......')\n",
    "#                       output1, states = tf.nn.rnn(lstm_cell, data, initial_state=self.init_state, scope=\"RNN1\")\n",
    "#                       outputs, states = tf.nn.rnn(lstm_cell, output1, initial_state=self.init_state, scope=\"RNN2\")\n",
    "#                   elif self.config.stack == 3:\n",
    "#                       print('running stack 3.......')\n",
    "#                       output1, states = tf.nn.rnn(lstm_cell, data, initial_state=self.init_state, scope=\"RNN1\")\n",
    "#                       output2, states = tf.nn.rnn(lstm_cell, output1, initial_state=self.init_state, scope=\"RNN2\")\n",
    "#                       outputs, states = tf.nn.rnn(lstm_cell, output2, initial_state=self.init_state, scope=\"RNN3\")\n",
    "#                   elif self.config.stack == 4:\n",
    "#                       print('running stack 4.......')\n",
    "#                       output1, states = tf.nn.rnn(lstm_cell, data, initial_state=self.init_state, scope=\"RNN1\")\n",
    "#                       output2, states = tf.nn.rnn(lstm_cell, output1, initial_state=self.init_state, scope=\"RNN2\")\n",
    "#                       output3, states = tf.nn.rnn(lstm_cell, output2, initial_state=self.init_state, scope=\"RNN3\")\n",
    "#                       outputs, states = tf.nn.rnn(lstm_cell, output3, initial_state=self.init_state, scope=\"RNN4\")\n",
    "#                   else:\n",
    "#                       print('running single stack RNN.......')\n",
    "#                       outputs, states = tf.nn.rnn(lstm_cell, data, initial_state=self.init_state, scope=\"RNN1\")\n",
    "\n",
    "#                   # Get lstm cell output\n",
    "#                   outputs, states = tf.nn.rnn(lstm_cell, data, initial_state=self.init_state)\n",
    "\n",
    "#                   # we really just interested in the last state's output\n",
    "#                   # return [tf.matmul(outputs[-1], self.weights['out1']) + self.biases['out1']]\n",
    "#                   if self.config.attention:\n",
    "#                         pred=self.compute_output(outputs)\n",
    "#                   else:\n",
    "#                         pred =self.compute_output(outputs[-1])\n",
    "#                   return pred\n",
    "\n",
    "#     def compute_output(self, outputs, data):\n",
    "#             if not self.config.attention:\n",
    "#                 print('running none attention mode.......')\n",
    "#                 # Linear activation\n",
    "#                 # for basic rnn prediction we really just interested in the last state's output, we need to average them in this case\n",
    "#                 return [tf.nn.dropout(tf.matmul(outputs, self.weights['out1']) + self.biases['out1'], self.config.dropout)]\n",
    "#             else:\n",
    "#                 print('running attention mode.......')\n",
    "#                 # print total_outputs.get_shape()\n",
    "#                 # print outputs[-1].get_shape()\n",
    "#                 # we now need to do apply the attention model, the output of each layer comes out from outputs[0], total layer = step_size\n",
    "#                 # I will first iterate through each layer and multiply the output to its weights\n",
    "#                 # I will follow the example below, which essentially produces a matrix vector product\n",
    "#                 # x = tf.constant(5.0, shape=[5, 6])\n",
    "#                 # w = tf.constant([0.0, 1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "#                 # xw = tf.mul(x, w)\n",
    "#                 # max_in_rows = tf.reduce_max(xw, 1), i need to ues reduce sum here\n",
    "#                 #\n",
    "#                 # sess = tf.Session()\n",
    "#                 # print sess.run(xw)\n",
    "#                 # # ==> [[0.0, 5.0, 10.0, 15.0, 20.0, 25.0],\n",
    "#                 # #      [0.0, 5.0, 10.0, 15.0, 20.0, 25.0],\n",
    "#                 # #      [0.0, 5.0, 10.0, 15.0, 20.0, 25.0],\n",
    "#                 # #      [0.0, 5.0, 10.0, 15.0, 20.0, 25.0],\n",
    "#                 # #      [0.0, 5.0, 10.0, 15.0, 20.0, 25.0]]\n",
    "#                 #\n",
    "#                 # print sess.run(max_in_rows)\n",
    "#                 # # ==> [25.0, 25.0, 25.0, 25.0, 25.0]\n",
    "#                 # print self.weights[1].get_shape() #(256,)\n",
    "#                 # print outputs[27].get_shape() #(?,256)\n",
    "#                 # attention_list = [tf.reduce_sum(tf.mul(outputs[i], weights[i]),1)+bias[i] for i in range(len(outputs))]\n",
    "#                 if self.config.residual:\n",
    "#                     print('running residual mode.......')\n",
    "#                     data = tf.transpose(self.input_data, [1, 0, 2])\n",
    "#                     for i in range(self.config.step_size):\n",
    "#                         data1 = tf.concat(1, [data[i], data[i]])\n",
    "#                         outputs[i]+=data1\n",
    "#                 else:\n",
    "#                     print('running non-residual mode.......')\n",
    "#                 attention_list = [tf.reduce_sum(tf.mul(outputs[i], self.weights[i]),1)+self.biases[i] for i in range(self.config.step_size)]\n",
    "#                 #after obtaining the attention list I need to make a vector out of it\n",
    "#                 attention_vec = tf.transpose(tf.pack(attention_list))\n",
    "#                 #attention_vec = tf.add(attention_vec,data)\n",
    "#                 # print self.weights['out1'].get_shape()\n",
    "#                 pred=[tf.nn.dropout(tf.matmul(attention_vec, self.weights['out1']) + self.biases['out1'], self.config.dropout)]\n",
    "#                 return pred\n",
    "\n",
    "#     def add_placeholders(self):\n",
    "#             '''\n",
    "#             feeding information to the input placeholders\n",
    "#             this function is call as the init process, data are feed in by tensor flow graph\n",
    "#             '''\n",
    "#             # define graph input place holders\n",
    "#             self.input_data = tf.placeholder(\"float\", [None, self.config.step_size, self.config.input_dim])\n",
    "#             self.input_label = tf.placeholder(\"float\", [None, self.config.label_dim])\n",
    "\n",
    "#     def get_feed_dict(self, data, label):\n",
    "#         if (self.config.model == 'BiRNN'):\n",
    "#             feed_dict = {self.input_data: data,\n",
    "#                          self.input_label: label}\n",
    "#         else:\n",
    "#             feed_dict = {self.input_data: data,\n",
    "#                          self.input_label: label}\n",
    "#         return feed_dict\n",
    "\n",
    "#     def run_model(self, scope=None, debug=False):\n",
    "#             '''\n",
    "#             this is the core function that launches the model, it initializes the weights and call the model specified in the config\n",
    "#             after model execution it records the test and training loss.\n",
    "#             input: model, training data, label, test data/label, and all other paramters needed to run the model\n",
    "#             output:\n",
    "#                 the best learning rate found through cross vaildation.\n",
    "#             '''\n",
    "#             self.print_model_params()\n",
    "#             #making predictions, this actives the rnn model\n",
    "#             if (self.config.model ==\"BiRNN\"): pred = self.BiRNN(scope)\n",
    "#             elif (self.config.model==\"RNN\"): pred = self.RNN(scope)\n",
    "\n",
    "#              # Define loss and optimizer\n",
    "#             label1 = tf.split(1, self.config.label_dim, self.input_label)\n",
    "\n",
    "#             if self.config.run_type=='regression':\n",
    "#                 cost = tf.sqrt(tf.reduce_mean(tf.square(tf.sub(pred[0], self.input_label))))\n",
    "\n",
    "#             if self.config.run_type=='classification':\n",
    "#                 cost = tf.reduce_mean(\n",
    "#                     tf.nn.softmax_cross_entropy_with_logits(pred[0], self.input_label))\n",
    "\n",
    "\n",
    "#             optimizer = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "#             #opt_func=tf.train.AdamOptimizer(learning_rate=self.config.learning_rate)\n",
    "#             #tvars=tf.trainable_variables()\n",
    "\n",
    "#             #clip the graident\n",
    "#             # tf.gradients(ys, xs, grad_ys=None, name='gradients', colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None)\n",
    "#             # Constructs symbolic partial derivatives of sum of ys w.r.t. x in xs.\n",
    "#             #grads, _=tf.clip_by_global_norm(tf.gradients(cost, tvars), self.config.grad_clip_threshold)\n",
    "#             #optimizer=opt_func.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "#             #compute accuracy for classification\n",
    "#             class_one_hot_prediction = tf.argmax(self.input_label, 1)\n",
    "#             classification_prediction=tf.argmax(tf.nn.softmax(pred[0]),1)\n",
    "#             classification_acc =tf.reduce_sum(tf.cast(tf.equal(classification_prediction, class_one_hot_prediction), 'int32'))\n",
    "\n",
    "#              # Initializing the variables\n",
    "#             init = tf.global_variables_initializer()\n",
    "#             saver = tf.train.Saver()\n",
    "\n",
    "#             def ValidationError(_type):\n",
    "#                 #-------------------------validation starts here-------------------------------------------\n",
    "#                     val_loss=[]\n",
    "#                     val_epoch = 0\n",
    "#                     if _type == 'val':\n",
    "#                           print('running validation loss')\n",
    "#                           self.read_val_file(self.config.val_data_dir)\n",
    "#                     elif _type == 'test':\n",
    "#                           print('running test loss')\n",
    "#                           self.read_val_file(self.config.test_data_dir)\n",
    "\n",
    "#                     train_dropout=self.config.dropout\n",
    "#                     self.config.dropout = train_dropout\n",
    "#                     val_i=1\n",
    "#                     val_last_index = 0\n",
    "#                     while val_i*self.config.batch_size <= len(self.val_data):\n",
    "#                         samples=[i for i in range(val_last_index, val_i*self.config.batch_size)]\n",
    "#                         val_last_index = val_i*self.config.batch_size\n",
    "#                         val_i+=1\n",
    "#                         sample = np.array(samples)\n",
    "#                         input_training_data=self.val_data[sample, :]\n",
    "#                         input_training_label=self.val_label[sample, :]\n",
    "#                         feed_dict = self.get_feed_dict(input_training_data, input_training_label)\n",
    "#                         if self.config.run_type == 'classification':\n",
    "#                             loss, match = sess.run([cost, classification_acc], feed_dict)\n",
    "#                             acc= 1.0*match/len(input_training_label)\n",
    "#                         elif self.config.run_type == 'regression':\n",
    "#                             acc = sess.run(cost, feed_dict)\n",
    "#                         val_loss.append(acc)\n",
    "#                     self.config.dropout=train_dropout\n",
    "#                     return 1.0*sum(val_loss)/(len(val_loss))\n",
    "\n",
    "#             def SaveWeights():\n",
    "#                   # if not os.path.exists(\"./weights\"):\n",
    "#                   if not os.path.exists(\"./\"+self.config.weight_dir):\n",
    "#                         os.makedirs(\"./\"+self.config.weight_dir)\n",
    "#                   path=saver.save(sess, './'+self.config.weight_dir+'/', global_step=None, latest_filename=None, meta_graph_suffix='meta', write_meta_graph=True, write_state=True)\n",
    "\n",
    "#             saver = tf.train.Saver()\n",
    "#             #Launch the graph\n",
    "#             with tf.Session() as sess:\n",
    "#                 #saver.restore(sess, './'+self.config.weight_dir+'/')\n",
    "#                 #print 'weights restored...'\n",
    "#                 #test_accuracy = ValidationError('test')\n",
    "#                 #print 'test accuracy', test_accuracy\n",
    "#                 #return test_accuracy\n",
    "\n",
    "#                 sess.run(init)\n",
    "#                 best_val_epoch = 0\n",
    "#                 if self.config.run_type=='classification':\n",
    "#                     best_val_accuracy= float('-inf')\n",
    "#                 if self.config.run_type=='regression':\n",
    "#                     best_val_accuracy= float('inf')\n",
    "#                 #-------------------------training starts here-------------------------------------------\n",
    "#                 # I have batches per epoch and epoch per validation check out which is my max_epoch\n",
    "#                 # I will read one file per time and taking batches out of the file, once the file is exhausted I will move\n",
    "#                 # on to the next file without interupting the epoch run\n",
    "#                 # note, number batchs per epoch * batch size must be less than the numver of reviews in a file\n",
    "#                 index = 1\n",
    "#                 last_index = 0\n",
    "#                 total_epoch = 0\n",
    "#                 val_epoch = 0\n",
    "#                 val_loss = []\n",
    "#                 for epoch in xrange(self.config.max_epochs):\n",
    "#                     total_epoch +=1\n",
    "#                     val_epoch +=1\n",
    "#                     train_accuarcy = []\n",
    "#                     test_accuracy = 0\n",
    "#                     train_loss = []\n",
    "#                     counter = 0\n",
    "#                     # Training\n",
    "#                     total_traing_data = self.training_label.shape[0]\n",
    "#                     while counter  < self.config.batches_per_epoch:\n",
    "#                         current_index=index*self.config.batch_size\n",
    "#                         if current_index >= total_traing_data:\n",
    "#                             samples=[i for i in range(total_traing_data-self.config.batch_size, total_traing_data)]\n",
    "#                             sample = np.array(samples)\n",
    "#                             #samples=np.random.randint(total_traing_data, size=self.config.batch_size)\n",
    "#                             input_training_data=self.training_data[sample, :]\n",
    "#                             input_training_label=self.training_label[sample, :]\n",
    "#                             index = 1\n",
    "#                             last_index=0\n",
    "#                             self.config.cur_marker+=1\n",
    "#                             if self.config.cur_marker == len(self.config.marker_list): self.config.cur_marker = 0\n",
    "# #                             self.read_train_file(self.config.train_data_dir)\n",
    "#                             self.read_train_file()\n",
    "#                         else:\n",
    "#                             samples=[i for i in range(last_index, current_index)]\n",
    "#                             last_index = current_index\n",
    "#                             index +=1\n",
    "#                             sample = np.array(samples)\n",
    "#                             input_training_data=self.training_data[sample, :]\n",
    "#                             input_training_label=self.training_label[sample, :]\n",
    "\n",
    "#                         feed_dict = self.get_feed_dict(input_training_data, input_training_label)\n",
    "#                         sess.run(optimizer, feed_dict)\n",
    "#                         if self.config.run_type == 'classification':\n",
    "#                             loss, match = sess.run([cost, classification_acc], feed_dict)\n",
    "#                             acc= 1.0*match/len(input_training_label)\n",
    "#                         elif self.config.run_type == 'regression':\n",
    "#                             acc = sess.run(cost, feed_dict)\n",
    "#                             loss = acc\n",
    "#                         train_accuarcy.append(acc)\n",
    "#                         train_loss.append(loss)\n",
    "#                         counter += 1\n",
    "\n",
    "#                     epoch_loss=sum(train_loss)/counter\n",
    "#                     epoch_acc=sum(train_accuarcy)/counter\n",
    "#                     print(\"Epoch \" + str(epoch) + \", Loss= \" + \"{:.6f}\".format(epoch_loss) + \", Accuracy= \" + \"{:.6f}\".format(epoch_acc))\n",
    "#                     sys.stdout.flush()\n",
    "#                     if val_epoch == self.config.epoch_per_val:\n",
    "#                         val_epoch = 0\n",
    "#                         val_accuracy = ValidationError('val')\n",
    "#                         if self.config.run_type=='classification':\n",
    "#                             if best_val_accuracy<val_accuracy:\n",
    "#                                 best_val_epoch=total_epoch\n",
    "#                                 best_val_accuracy= val_accuracy\n",
    "#                                 SaveWeights()\n",
    "#                         if self.config.run_type == 'regression':\n",
    "#                             if best_val_accuracy>val_accuracy:\n",
    "#                                 best_val_epoch=total_epoch\n",
    "#                                 best_val_accuracy= val_accuracy\n",
    "#                                 SaveWeights()\n",
    "\n",
    "#                         print('*'*30)\n",
    "#                         print(str(self.config.run_type)+' validation accuracy at epoch %d: %f'%(total_epoch, val_accuracy))\n",
    "#                         print('best validation accuracy so far at epoch %d: %f'%(total_epoch, best_val_accuracy))\n",
    "#                         print('*'*30)\n",
    "#                 print(\"Optimization Finished!\")\n",
    "\n",
    "#                 saver.restore(sess, './'+self.config.weight_dir+'/')\n",
    "#                 print('weights restored...')\n",
    "#                 test_accuracy = ValidationError('test')\n",
    "#                 print('test accuracy', test_accuracy)\n",
    "#                 return test_accuracy\n",
    "\n",
    "\n",
    "#     def __init__(self, config):\n",
    "#       self.config = config\n",
    "# #       if len(self.config.marker_list) == 0: self.read_markers(self.config.train_data_dir)\n",
    "# #       self.config_cur_marker=self.config.marker_list[0]\n",
    "# #       self.read_train_file(self.config.train_data_dir)\n",
    "#       self.read_train_file()\n",
    "#       self.add_placeholders()\n",
    "#       self.init_variables()\n",
    "#       self.val_data=[]\n",
    "\n",
    "# def run_regression(config=None, stack=1, attention=False, res=False):\n",
    "#       for i in range(stack):\n",
    "#             ts = int(time.time())\n",
    "#             if config is None:\n",
    "#                 config = Config()\n",
    "#             config.run_type='regression'\n",
    "#             config.train_data_dir='Data/train/regression'\n",
    "#             config.val_data_dir='Data/val/regression'\n",
    "#             config.test_data_dir='Data/test/regression'\n",
    "\n",
    "#             config.cell_type='LSTM'\n",
    "#             #config.cell_type='GRU'\n",
    "#             config.model=\"BiRNN\"\n",
    "#             #config.model=\"RNN\"\n",
    "#             config.learning_rate=0.001\n",
    "#             config.batch_size=16\n",
    "#             config.batches_per_epoch=80\n",
    "#             config.max_epochs=40\n",
    "#             config.dropout=1\n",
    "#             config.hidden_dim=300\n",
    "#             config.epoch_per_val=5\n",
    "#             config.stack=i+1\n",
    "#             config.attention=attention\n",
    "#             #config.init='norm'\n",
    "#             config.init='xaiver'\n",
    "#             config.grad_clip_threshold = 10000\n",
    "#             config.residual=res\n",
    "\n",
    "#             config.weight_dir=\"regression_\"+str(config.model)+\"_\"+str(config.cell_type)+\"_\"+\"stack\"+str(config.stack)+\"_\"+str(ts)\n",
    "#             if not os.path.exists(\"./\"+config.weight_dir):\n",
    "#               os.makedirs(\"./\"+config.weight_dir)\n",
    "#             f=open(\"./\"+config.weight_dir+'/run_file.txt', 'a')\n",
    "#             sys.stdout = f\n",
    "#             model = Models(config)\n",
    "#             loss_val = model.run_model(scope=str(i))\n",
    "\n",
    "# def run_2classification(config=None,stack=1, attention=False, res=False):\n",
    "#     for i in range(5,8):\n",
    "#             ts = int(time.time())\n",
    "#             if config is None:\n",
    "#               config = Config()\n",
    "#             config.run_type='classification'\n",
    "#             config.train_data_dir='Data/train/2_classification'\n",
    "#             config.val_data_dir='Data/val/2_classification'\n",
    "#             config.test_data_dir='Data/test/2_classification'\n",
    "#             config.cell_type='LSTM'\n",
    "#             #config.cell_type='GRU'\n",
    "#             config.model=\"BiRNN\"\n",
    "#             #config.model=\"RNN\"\n",
    "#             config.learning_rate=0.001\n",
    "#             config.batch_size=16\n",
    "#             config.batches_per_epoch=80\n",
    "#             config.max_epochs=40\n",
    "#             config.dropout=1\n",
    "#             config.hidden_dim=300\n",
    "#             config.epoch_per_val=5\n",
    "#             config.stack=i+1\n",
    "#             config.attention=attention\n",
    "#             #config.init='norm'\n",
    "#             config.init='xaiver'\n",
    "#             config.grad_clip_threshold = 10000\n",
    "#             config.residual=res\n",
    "\n",
    "#             config.weight_dir=\"attention_2_classification_\"+str(config.model)+\"_\"+str(config.cell_type)+\"_\"+\"stack\"+str(config.stack)+\"_\"+str(ts)\n",
    "#             if not os.path.exists(\"./\"+config.weight_dir):\n",
    "#               os.makedirs(\"./\"+config.weight_dir)\n",
    "#             f=open(\"./\"+config.weight_dir+'/run_file.txt', 'a')\n",
    "#             sys.stdout = f\n",
    "#             model = Models(config)\n",
    "#             loss_val = model.run_model(scope=str(i))\n",
    "\n",
    "# #def run_3classification(config=None,stack=1, attention=False):\n",
    "# #      for i in range(stack):\n",
    "# #            ts = int(time.time())\n",
    "# #            ts = int(time.time())\n",
    "# #            if config is None:\n",
    "# #              config = Config()\n",
    "# #\n",
    "# #            config.run_type='classification'\n",
    "# #            config.train_data_dir='Data/train/3_classification'\n",
    "# #            config.val_data_dir='Data/val/3_classification'\n",
    "# #            config.test_data_dir='Data/test/3_classification'\n",
    "# #            config.weight_dir=\"3_classification_\"+str(ts)\n",
    "# #            config.cell_type='LSTM'\n",
    "# #            #config.cell_type='GRU'\n",
    "# #            config.model=\"BiRNN\"\n",
    "# #            #config.model=\"RNN\"\n",
    "# #            config.learning_rate=0.001\n",
    "# #            config.batch_size=128\n",
    "# #            config.batches_per_epoch=5\n",
    "# #            config.max_epochs=30\n",
    "# #            config.dropout=0.8\n",
    "# #            config.hidden_dim=300\n",
    "# #            config.epoch_per_val=5\n",
    "# #            config.stack=i+1\n",
    "# #            config.attention=attention\n",
    "# #            #config.init='norm'\n",
    "# #            config.init='xaiver'\n",
    "# #            config.grad_clip_threshold = 10000\n",
    "# #\n",
    "# #\n",
    "# #            config.weight_dir=\"3_classification_\"+str(config.model)+\"_\"+str(config.cell_type)+\"_\"+\"stack\"+str(config.stack)+\"_\"+str(ts)\n",
    "# #            if not os.path.exists(\"./\"+config.weight_dir):\n",
    "# #              os.makedirs(\"./\"+config.weight_dir)\n",
    "# #            f=open(\"./\"+config.weight_dir+'/run_file.txt', 'a')\n",
    "# #            sys.stdout = f\n",
    "# #            model = Models(config)\n",
    "# #            loss_val = model.run_model(scope=str(i))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     random.seed(31415)\n",
    "#     print(sys.argv[1])\n",
    "#     if sys.argv[1] == '2_classification':\n",
    "#         run_2classification(stack=1, attention=False)\n",
    "#     elif sys.argv[1] == '3_classification':\n",
    "#         run_3classification(stack=4, attention=True)\n",
    "#     elif sys.argv[1] == 'regression':\n",
    "#         run_regression(stack=1, attention=False)\n",
    "#     elif sys.argv[1]=='stack_regression':\n",
    "#         run_regression(stack=7, attention=True, res=True)\n",
    "#     elif sys.argv[1]=='stack_classification':\n",
    "#         run_2classification(stack=7, attention=True, res=True)\n",
    "#     else:\n",
    "#         print('you must select a task to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from itertools import izip\n",
    "# import operator\n",
    "# import csv\n",
    "# import numpy as np\n",
    "# #this is a hack file, assumed a lot of things, such as naming conventions and implied squence\n",
    "# wordfile=[]\n",
    "# attention_file =[]\n",
    "# correctness_file =[]\n",
    "# class_file=[]\n",
    "# pos_word_weight_dic={}\n",
    "# neg_word_weight_dic={}\n",
    "# pos_freq={}\n",
    "# neg_freq={}\n",
    "# #cause the way the files are named, not only i grab the right one, they are also aling, such as word_0 aligns with attention_0 and correct_0 and class_0\n",
    "# files = [f for f in os.listdir('.') if os.path.isfile(f)]\n",
    "# for f in files:\n",
    "#     if f.startswith('word'):\n",
    "#         wordfile.append(f)\n",
    "#     if f.startswith('attention'):\n",
    "#         attention_file.append(f)\n",
    "#     if f.startswith('correct'):\n",
    "#         correctness_file.append(f)\n",
    "#     if f.startswith('y'):\n",
    "#         class_file.append(f)\n",
    "# print wordfile\n",
    "# print attention_file\n",
    "# print correctness_file\n",
    "# print class_file\n",
    "# for i, wf in enumerate(wordfile):\n",
    "#     print i, wf\n",
    "#     with open(wf) as word_f, open(attention_file[i]) as attention_f, open(correctness_file[i]) as correct_f, open(class_file[i]) as class_f:\n",
    "#         correctness = [line.rstrip('\\n') for line in correct_f]\n",
    "#         word=[line.rstrip('\\n') for line in word_f]\n",
    "#         attention=[line.rstrip('\\n') for line in attention_f]\n",
    "#         classes=[line.rstrip('\\n') for line in class_f]\n",
    "#         print len(attention)\n",
    "#         for i in correctness:\n",
    "#             word_count = 0\n",
    "#             while word_count < 250:\n",
    "#                 #if i == \"True\":\n",
    "#                     x = word[word_count]\n",
    "#                     y = attention[word_count]\n",
    "#                     if classes[word_count]==\"10\":\n",
    "#                         if x in neg_word_weight_dic:\n",
    "#                                neg_word_weight_dic[x]-=float(y)\n",
    "#                                neg_freq[x]+=1\n",
    "#                         else:\n",
    "#                                neg_freq[x]=1\n",
    "#                                neg_word_weight_dic[x]=-1.0*float(y)\n",
    "#                     if classes[word_count]==\"01\":\n",
    "#                         if x in pos_word_weight_dic:\n",
    "#                                pos_word_weight_dic[x]+=float(y)\n",
    "#                                pos_freq[x]+=1\n",
    "#                         else:\n",
    "#                                pos_freq[x]=1\n",
    "#                                pos_word_weight_dic[x]=float(y)\n",
    "#                     word_count +=1\n",
    "#                 #print word_weight_dic\n",
    "#                 #print freq\n",
    "# freq_t=0.0\n",
    "# for word in neg_freq:\n",
    "#     freq_t+=neg_freq[word]\n",
    "# for word in pos_freq:\n",
    "#     freq_t+=pos_freq[word]\n",
    "# for word in pos_word_weight_dic:\n",
    "#     freq_c = pos_freq[word]\n",
    "#     if word in neg_word_weight_dic:\n",
    "#         pos_word_weight_dic[word] += neg_word_weight_dic[word]\n",
    "#         freq_c+=neg_freq[word]\n",
    "#     #pos_word_weight_dic[word] /=np.abs(freq_c*(1+np.log(freq_c/freq_t)))\n",
    "#     #pos_word_weight_dic[word] /=np.abs(freq_c*np.log(freq_c/freq_t))\n",
    "#     pos_word_weight_dic[word] /=freq_c\n",
    "# #for word in neg_word_weight_dic:\n",
    "# #    neg_word_weight_dic[word] /=(1+100*np.log(neg_freq[word]))\n",
    "\n",
    "# pos_sorted_word = sorted(pos_word_weight_dic.items(), key=operator.itemgetter(1), reverse=True)\n",
    "# #neg_sorted_word = sorted(neg_word_weight_dic.items(), key=operator.itemgetter(1), reverse=True)\n",
    "# #print pos_sorted_word\n",
    "# #print neg_sorted_word\n",
    "\n",
    "# w = csv.writer(open(\"output.csv\", \"w\"))\n",
    "# for (key, val) in pos_sorted_word:\n",
    "#     w.writerow([key, val])\n",
    "\n",
    "# #w = csv.writer(open(\"neg_output.csv\", \"w\"))\n",
    "# #for (key, val) in neg_sorted_word:\n",
    "# #    w.writerow([key, val])\n",
    "\n",
    "# print len(pos_freq)\n",
    "# print len(pos_word_weight_dic)\n",
    "\n",
    "# #print len(neg_freq)\n",
    "# #print len(neg_word_weight_dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate: LSTM Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras\n",
    "# !pip install pandas_ml\n",
    "\n",
    "# !pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, Dropout, Dense, LSTM\n",
    "from keras.callbacks import CSVLogger, History, ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, accuracy_score, precision_recall_fscore_support\n",
    "from pandas_ml import ConfusionMatrix\n",
    "%matplotlib inline\n",
    "\n",
    "class YelpLSTM(object):\n",
    "    def __init__(self, parms):\n",
    "        self._parms = parms\n",
    "#         self._tokenizer = Tokenizer(nb_words=self._parms['vocabulary_size'])\n",
    "        self._tokenizer = Tokenizer(num_words=self._parms['vocabulary_size'])\n",
    "\n",
    "\n",
    "        self._reviews = None\n",
    "        self._balanced = None\n",
    "        self._glove = None\n",
    "        self._embedding_matrix = None\n",
    "        self._model = None\n",
    "        self._verbose = True\n",
    "        self._predicted_classes = None\n",
    "        self._predicted_proba = None\n",
    "        self._eval_actual = None\n",
    "        self._eval_predicted_proba = None\n",
    "        self._eval_predicted_classes = None\n",
    "        self._logs = None\n",
    "        self._tpr = None\n",
    "        self._fpr = None\n",
    "        self._thresholds = None\n",
    "        self._auc = None\n",
    "        self._target_range = None\n",
    "        \n",
    "    def console(self, message):\n",
    "        if self._verbose:\n",
    "            print(message)\n",
    "            \n",
    "    def update_parms(self, parms):\n",
    "        if parms['vocabulary_size'] != self._parms['vocabulary_size']:\n",
    "            self._tokenizer = Tokenizer(nb_words=parms['vocabulary_size'])\n",
    "        self._parms = parms\n",
    "        \n",
    "#     def load_reviews(self, reviews):\n",
    "    def load_reviews(self, reviews_path):\n",
    "        self.console('Loading reviews...')\n",
    "#         self._reviews = pd.read_csv(reviews)\n",
    "\n",
    "\n",
    "#         print(\"reviews test before load\", reviews[0:10])\n",
    "\n",
    "#         self._reviews = reviews\n",
    "\n",
    "\n",
    "\n",
    "#         self._reviews = pd.read_json(reviews_path, lines=True)\n",
    "        self._reviews = pd.read_json(reviews_path, lines=True)\n",
    "\n",
    "#         self._reviews = pd.read_json(\"../dataset/restaurant_reviews_10k.json\", lines=True)\n",
    "\n",
    "#         self._reviews = pd.read_json(\"../dataset/restaurant_reviews_10k.json\", lines=True)\n",
    "#         self._reviews = reviews_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.console('%d reviews loaded.' % len(self._reviews))\n",
    "#         self.console('%d reviews loaded.' % np.shape(self._reviews))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.console(self._reviews[0:10])\n",
    "        \n",
    "        \n",
    "#     def load_glove(self, glove_folder):\n",
    "    def load_glove(self, gloveFile):\n",
    "        self.console('Loading GloVe embeddings...')\n",
    "        glove = {}\n",
    "        count = 0\n",
    "#         with open(os.path.join(glove_folder, 'glove.6B.' + str(self._parms['embedding_dim']) + 'd.txt'), 'r') as f:\n",
    "\n",
    "        with open(gloveFile, 'r') as f:\n",
    "            while True:\n",
    "                line = f.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "                line = line.split(' ')\n",
    "                word = line[0]\n",
    "                vector = np.asarray(line[1:], dtype='float32')\n",
    "                glove[word] = vector\n",
    "        self._glove = glove\n",
    "        self.console('%d embeddings loaded.' % len(self._glove))\n",
    "        \n",
    "    @property\n",
    "    def training(self):\n",
    "        return self._X_train, self._y_train\n",
    "    \n",
    "    @property\n",
    "    def test(self):\n",
    "        return self._X_test, self._y_test\n",
    "    \n",
    "    @property\n",
    "    def best_model(self):\n",
    "        return self._model\n",
    "    \n",
    "    @property\n",
    "    def predicted_classes(self):\n",
    "        return self._predicted_classes\n",
    "    \n",
    "    @property\n",
    "    def predicted_proba(self):\n",
    "        return self._predicted_proba\n",
    "    \n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        return self._tokenizer\n",
    "    \n",
    "    @property\n",
    "    def logs(self):\n",
    "        return self._logs\n",
    "        \n",
    "    @property\n",
    "    def confusion_matrix(self):\n",
    "        return self._cm\n",
    "    \n",
    "    @property\n",
    "    def prfs(self):\n",
    "        return self._prfs\n",
    "    \n",
    "    @property\n",
    "    def fpr(self):\n",
    "        return self._fpr\n",
    "    \n",
    "    @property\n",
    "    def tpr(self):\n",
    "        return self._tpr\n",
    "    \n",
    "    @property\n",
    "    def thresholds(self):\n",
    "        return self._thresholds\n",
    "    \n",
    "    @property\n",
    "    def auc(self):\n",
    "        return self._auc\n",
    "        \n",
    "    def _balance_dataset(self):\n",
    "        categories = []\n",
    "        samples = []\n",
    "                \n",
    "        self._target_range = range(2)\n",
    "        if self._parms['target']['feature'] == 'stars':\n",
    "            prefix = ''\n",
    "            self._target_range = range(1,6)\n",
    "        else:\n",
    "            prefix = 'is_'\n",
    "            self._reviews['is_' + self._parms['target']['feature']] = self._reviews[self._parms['target']['feature']].apply(lambda v: v > self._parms['target']['threshold']).astype(int)\n",
    "            \n",
    "        for i in self._target_range:\n",
    "            categories.append(self._reviews[self._reviews[prefix + self._parms['target']['feature']] == i])\n",
    "            \n",
    "        \n",
    "        \n",
    "#         sizes = map(lambda s: len(s), categories)\n",
    "        \n",
    "        sizes = list(map(lambda s: len(s), categories))\n",
    "        \n",
    "        \n",
    "#         print(sizes)\n",
    "        \n",
    "        \n",
    "        nb_samples = min(self._parms['samples'], np.min(sizes))\n",
    "#         nb_samples = np.min(sizes)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         nb_samples = nb_samples.astype(np.int32)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.console('Using %s samples per category' % str(nb_samples))\n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.console('Type for nb_samples' % str(type(nb_samples)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for category in categories:\n",
    "            samples.append(category.sample(n=nb_samples, random_state=32))\n",
    "        self._balanced = pd.concat(samples)\n",
    "\n",
    "    def _build_datasets(self):\n",
    "        self._tokenizer.fit_on_texts(self._balanced.text.values)\n",
    "        \n",
    "        sequences = self._tokenizer.texts_to_sequences(self._balanced.text)\n",
    "        padded_seq = pad_sequences(sequences, maxlen=self._parms['seq_size'])\n",
    "\n",
    "        if self._parms['target']['feature'] == 'stars':\n",
    "            target = to_categorical(self._balanced[self._parms['target']['feature']])\n",
    "        else:\n",
    "            target = self._balanced['is_' + self._parms['target']['feature']].values\n",
    "\n",
    "        self._X_train, self._X_test, self._y_train, self._y_test = train_test_split(padded_seq, target, test_size=0.2, random_state=42)\n",
    "\n",
    "    def _build_embeddings(self):\n",
    "        tokenized_words = map(lambda t: t[0], sorted(self._tokenizer.word_index.items(), key=lambda t: t[1])[:self._parms['vocabulary_size']])\n",
    "\n",
    "        embedding_matrix = np.zeros((self._parms['vocabulary_size'], self._parms['embedding_dim']))\n",
    "        for idx, word in enumerate(tokenized_words):\n",
    "            try:\n",
    "                embedding_matrix[idx] = self._glove[word]\n",
    "            except:\n",
    "                pass\n",
    "        self._embedding_matrix = embedding_matrix\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Embedding(input_dim=self._parms['vocabulary_size'],\n",
    "                            output_dim=self._parms['embedding_dim'],\n",
    "                            input_length=self._parms['seq_size'],\n",
    "                            weights=[self._embedding_matrix],\n",
    "                            trainable=False))\n",
    "\n",
    "        model.add(LSTM(self._parms['memory_neurons']))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        activation = 'sigmoid'\n",
    "        loss = 'binary_crossentropy'\n",
    "        outputs = 1\n",
    "        if len(self._y_train.shape) > 1:\n",
    "            activation = 'softmax'\n",
    "            loss = 'categorical_crossentropy'\n",
    "            outputs = self._y_train.shape[1]\n",
    "\n",
    "        model.add(Dense(outputs, activation=activation))\n",
    "        model.compile(loss=loss, optimizer='nadam', metrics=['accuracy'])\n",
    "        self._model = model\n",
    "        self.console(self._model.summary())\n",
    "\n",
    "    def fit(self, model_name, folder='./', verbose=True):\n",
    "        self._verbose = verbose\n",
    "        \n",
    "        assert self._reviews is not None, 'Reviews file was not loaded'\n",
    "        assert len(self._reviews) > 0, 'Reviews file is empty'\n",
    "        assert self._glove is not None, 'GloVe file was not loaded'\n",
    "        assert len(self._glove) > 0, 'GloVe file is empty'\n",
    "        \n",
    "        self.console('Balancing dataset...')\n",
    "        self._balance_dataset()\n",
    "        self.console('Building training and test datasets...')\n",
    "        self._build_datasets()\n",
    "        self.console('Building word embeddings from GloVe...')\n",
    "        self._build_embeddings()\n",
    "        self.console('Building model...')\n",
    "        self._build_model()\n",
    "        self.console('Fitting model...')\n",
    "        \n",
    "        parms_desc = model_name + '_%ddim_%dvoc_%dseq' % (self._parms['embedding_dim'],\n",
    "                                                          self._parms['vocabulary_size'],\n",
    "                                                          self._parms['seq_size'])\n",
    "\n",
    "        hist = History()\n",
    "        \n",
    "        \n",
    "#         logger = CSVLogger(os.path.join(folder, parms_desc) + '_training_logs.csv')\n",
    "        logger = CSVLogger('_training_logs.csv')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         checks = ModelCheckpoint(os.path.join(folder, parms_desc) + '_model-{epoch:02d}_{val_acc:.2f}',\n",
    "        checks = ModelCheckpoint('_model-{epoch:02d}_{val_acc:.2f}',\n",
    "                                 monitor='val_acc',\n",
    "                                 verbose=int(self._verbose),\n",
    "                                 save_best_only=True,\n",
    "                                 mode='max')\n",
    "        early_stopping = EarlyStopping(monitor='val_acc', patience=2)\n",
    "\n",
    "        self._model.fit(self._X_train,\n",
    "                        self._y_train,\n",
    "                        nb_epoch=self._parms['nb_epochs'],\n",
    "#                         epoch=self._parms['epochs'],\n",
    "                        batch_size=self._parms['batch_size'],\n",
    "                        validation_data=(self._X_test, self._y_test),\n",
    "                        callbacks=[checks, hist, logger, early_stopping])\n",
    "        \n",
    "#         self._logs = pd.read_csv(os.path.join(folder, parms_desc) + '_training_logs.csv')\n",
    "        self._logs = pd.read_csv('_training_logs.csv')\n",
    "        best_epoch = self._logs['val_acc'].argmax()\n",
    "        best_val_acc = '{:.2f}'.format(self._logs['val_acc'].iloc[best_epoch])\n",
    "#         best_model = (os.path.join(folder, parms_desc) + '_model-%02d_%s') % (best_epoch, best_val_acc)\n",
    "        best_model = ('_model-%02d_%s') % (best_epoch, best_val_acc)\n",
    "        \n",
    "#         with open(os.path.join(folder, parms_desc + '_tokenizer'), 'wb') as tok:\n",
    "        with open('_tokenizer', 'wb') as tok:\n",
    "            pickle.dump(self._tokenizer, tok)\n",
    "        \n",
    "        self.console('Calculating predictions for the best model...')\n",
    "        self._model = load_model(best_model)\n",
    "        self._predicted_proba = self.predict_proba()\n",
    "        if len(self._y_train.shape) > 1:\n",
    "            self._predicted_classes = np.argmax(self._predicted_proba, axis=1)\n",
    "        else:\n",
    "            self._predicted_classes = (self._predicted_proba > 0.5).astype(int)\n",
    "        self.console('Calculating metrics for the best model...')\n",
    "        self.evaluate()\n",
    "        self.console('Finished!')\n",
    "        \n",
    "        return self._model\n",
    "\n",
    "    def load(self, tokenizer, model):\n",
    "        error_msg = ''\n",
    "        try:\n",
    "            self._model = load_model(model)\n",
    "        except:\n",
    "            error_msg = 'Error loading model!'\n",
    "            \n",
    "        try:\n",
    "            with open(tokenizer, 'rb') as tok:\n",
    "                self._tokenizer = pickle.load(tok)\n",
    "        except:\n",
    "            error_msg = 'Error loading tokenizer!'\n",
    "            \n",
    "        return (error_msg == ''), error_msg\n",
    "    \n",
    "    def make_prediction(self, sentence):\n",
    "        sequence = self._tokenizer.texts_to_sequences([sentence])\n",
    "        padded_seq = pad_sequences(sequence, maxlen=self._parms['seq_size'])\n",
    "        return self.predict_classes(padded_seq)[0]\n",
    "    \n",
    "    def predict_classes(self, X=None, threshold=0.5):\n",
    "        if len(self._y_train.shape) > 1:\n",
    "            predictions = np.argmax(self.predict_proba(X), axis=1)\n",
    "        else:\n",
    "            predictions = (self.predict_proba(X) > threshold).astype(int)\n",
    "        return predictions\n",
    "        \n",
    "    def predict_proba(self, X=None):\n",
    "        if X is None:\n",
    "            X = self._X_test\n",
    "        predictions = self._model.predict_proba(X)\n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, actual=None, predicted_proba=None, threshold=0.5):\n",
    "        if actual is None:\n",
    "            eval_actual = self._y_test[:]\n",
    "        else:\n",
    "            eval_actual = actual[:]\n",
    "            \n",
    "        if predicted_proba is None:\n",
    "            eval_predicted_proba = self._predicted_proba[:]\n",
    "        else:\n",
    "            eval_predicted_proba = predicted_proba[:]\n",
    "            \n",
    "        if len(eval_actual.shape) == 1:\n",
    "            binary = True\n",
    "            eval_predicted_classes = (eval_predicted_proba > threshold).astype(int).ravel()\n",
    "            eval_predicted_proba = eval_predicted_proba.ravel()\n",
    "        else:\n",
    "            binary = False\n",
    "            eval_predicted_classes = eval_predicted_proba.argmax(axis=1)\n",
    "            eval_actual = eval_actual.argmax(axis=1)\n",
    "        \n",
    "        self._eval_actual = eval_actual\n",
    "        self._eval_predicted_proba = eval_predicted_proba\n",
    "        self._eval_predicted_classes = eval_predicted_classes\n",
    "    \n",
    "        self._cm = ConfusionMatrix(self._eval_actual, self._eval_predicted_classes)\n",
    "\n",
    "        prfs = precision_recall_fscore_support(y_true=self._eval_actual, y_pred=self._eval_predicted_classes)\n",
    "        prfs = pd.DataFrame.from_dict(dict(zip(['precision', 'recall', 'fscore', 'support'], prfs)))\n",
    "        \n",
    "#         prfs.set_index([self._target_range], inplace=True)\n",
    "        \n",
    "        self._prfs = prfs\n",
    "        \n",
    "        if binary:\n",
    "            self._fpr, self._tpr, self._thresholds = roc_curve(self._eval_actual, self._eval_predicted_proba)\n",
    "            self._auc = auc(self._fpr, self._tpr)\n",
    "        else:\n",
    "            self._fpr, self._tpr, self._thresholds, self._auc = None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews = './dataset/english_reviews_sample.csv'\n",
    "\n",
    "\n",
    "# reviews_df = pd.read_json(\"../dataset/restaurant_reviews_10k.json\", lines=True)\n",
    "\n",
    "# reviews_path = \"../dataset/restaurant_reviews_10k.json\"\n",
    "\n",
    "\n",
    "# print(reviews_df[0:10])\n",
    "# print(reviews_df.head())\n",
    "\n",
    "# reviews_df.head()\n",
    "# print(np.shape(reviews_df))\n",
    "\n",
    "# glove_folder = './glove.6B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "parms = {'embedding_dim': 100,\n",
    "         'vocabulary_size': 10000,\n",
    "         'seq_size': 400,\n",
    "         'nb_epochs': 30,\n",
    "#          'epochs': 30,\n",
    "         'batch_size': 128,\n",
    "         'memory_neurons': 100,\n",
    "         'target': {'feature': 'stars', 'threshold': None},\n",
    "         'samples': 62500}\n",
    "\n",
    "lstm = YelpLSTM(parms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reviews...\n",
      "10000 reviews loaded.\n"
     ]
    }
   ],
   "source": [
    "# test = lstm.load_reviews(reviews_df)\n",
    "# print(test)\n",
    "# reviews_path = \"../dataset/review_10k.json\"\n",
    "\n",
    "reviews_path = \"../dataset/restaurant_reviews_10k.json\"\n",
    "lstm.load_reviews(reviews_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  2018-08-02 17:25:49.827405\n",
      "Loading GloVe embeddings...\n",
      "400000 embeddings loaded.\n",
      "End time:  2018-08-02 17:26:21.092899\n",
      "Time taken:  0:00:31.265494\n"
     ]
    }
   ],
   "source": [
    "# # Smallest GloVe file\n",
    "# gloveFile = \"../../glove/glove.6B.50d.txt\"\n",
    "\n",
    "# # Smaller GloVe file\n",
    "gloveFile = \"../../glove/glove.6B.300d.txt\"\n",
    "\n",
    "# # Primary GloVe file\n",
    "# gloveFile = \"../../glove/glove.42B.300d.txt\n",
    "\n",
    "\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "print(\"Start time: \", start_time)\n",
    "\n",
    "lstm.load_glove(gloveFile)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"End time: \", end_time)\n",
    "\n",
    "time_taken = end_time - start_time\n",
    "print(\"Time taken: \", time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing dataset...\n",
      "Using 965 samples per category\n",
      "Building training and test datasets...\n",
      "Building word embeddings from GloVe...\n",
      "Building model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 400, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 1,081,006\n",
      "Trainable params: 81,006\n",
      "Non-trainable params: 1,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexanderherring/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:298: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3860 samples, validate on 965 samples\n",
      "Epoch 1/30\n",
      "3860/3860 [==============================] - 28s 7ms/step - loss: 1.6676 - acc: 0.1972 - val_loss: 1.6487 - val_acc: 0.1782\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.17824, saving model to _model-01_0.18\n",
      "Epoch 2/30\n",
      "3860/3860 [==============================] - 26s 7ms/step - loss: 1.6286 - acc: 0.1940 - val_loss: 1.6245 - val_acc: 0.2218\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.17824 to 0.22176, saving model to _model-02_0.22\n",
      "Epoch 3/30\n",
      "3860/3860 [==============================] - 26s 7ms/step - loss: 1.6262 - acc: 0.1943 - val_loss: 1.6294 - val_acc: 0.2062\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.22176\n",
      "Epoch 4/30\n",
      "3860/3860 [==============================] - 26s 7ms/step - loss: 1.6214 - acc: 0.1982 - val_loss: 1.6177 - val_acc: 0.1782\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.22176\n",
      "Calculating predictions for the best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexanderherring/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:302: FutureWarning: 'argmax' is deprecated. Use 'idxmax' instead. The behavior of 'argmax' will be corrected to return the positional maximum in the future. Use 'series.values.argmax' to get the position of the maximum now.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = '_model-01_0.22', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-6b84c978057f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model = lstm.fit(model_name='stars_100neurons', folder='./models/stars')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'stars_100neurons'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-96-f789df347dda>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model_name, folder, verbose)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calculating predictions for the best model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicted_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '_model-01_0.22', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# model = lstm.fit(model_name='stars_100neurons', folder='./models/stars')\n",
    "model = lstm.fit(model_name='stars_100neurons', folder='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
